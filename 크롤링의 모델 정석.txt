0. 크롤링의 모델 정석
** impotr 할것 (대부분)
from bs4 import BeautifulSoup             **BeautifulSoup의 파써( html.parser )
pip install html5lib  lxml                      **파써( html5lib/ lxml )
import requests,                		        ** 불러오기  ( requests, urlopen  )          
from urllib.request import urlopen           결국 urllib와 urlopen   
import json

** select / select_all   find_all / find
   아이디 #   class .    .클래스명:first-child  .클래스명:last-child   .클래스명:nth-child(3) 
   뒤에 아무것도 없으면 모두 선택한거임   
   div 하위에서만 사용하는 .클래스명 > 요소

** 그밖에
print(html.read())  --> 가져온 html이라는 문서 읽기 프린트
print(content.text)


** 프로세스
1. 웹페이지 정보 위치 확인(F12)
2. HTML 소스 불러오기                		url= "https://ai dev.tistory.com/1"
							html =urlopen(url)         
									        	또는   response = requests.get(url) 
									        	이후   html =response.content  
3. 데이터에서 정보 가공 후 추출     	bs_obj= BeautifulSoup (html, "html.parser")
							title =bs_obj.select_one ("div > h1")
							print(html.read())         또는 print(content.text)
4. 추출정보를 csv 또는 db 등에 저장,가공,시각화


0) 여러 선택들 :테이블 태그 크롤링
columns = []
column =table.select_one ('tr').select('td')
for col in column:
	columns.append(col.text)
print(columns)

이런 선택도 가능
rows = table.select tr:not (:first child)')
1)
import requests
from urllib.request import urlopen
from bs4 import BeautifulSoup

url = 'https://ai-dev.tistory.com/1'

response = requests.get(url)
# response = urlopen(url)

soup = BeautifulSoup(response.content, 'html.parser')
title = soup.select_one('header > h1 > a')
print(title.text)
# print(title.get('href'))

2)
url = 'https://ai-dev.tistory.com/2'
response = requests.get(url)

soup = BeautifulSoup(response.content, 'html.parser')
table = soup.select_one('div.tt_article_useless_p_margin table')
rows = table.select('tr') # 'tr:not(:first-child)' - 첫번째 줄을 제외한 나머지 줄
table_list = []
for row in rows:
    row_list = []
    for td in row.select('td'):
        row_list.append(td.text)
    table_list.append(row_list)
table_list

3)
items = soup.select_one('div.tt_article_useless_p_margin > ul')
ul_list = []
for item in items.select('li'):
    ul_list.append(item.text)
print(ul_list)

4) 페이지 이동하며 크롤링
contents = []
for i in range(1, 5):
    url = f'https://ai-dev.tistory.com/{i}' 
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        try:
            title = soup.select_one('div.hgroup > h1').text
            date = soup.select_one('span.date').text
            contents.append([title, date])
print(contents)

===네이버에서 검색 ===
def naver_searching(search_word):
    url = f'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={search_word}'
    response = requests.get(url)
    if response.status_code == 200:
        search_word_related = []
        soup = BeautifulSoup(response.content, 'html.parser')
        items = soup.select_one('ul.lst_related_srch')
        for item in items.select('li>a'):
            search_word_related.append({ item.text.strip():  
                        'https://search.naver.com/search.naver' + item.get('href')})
 
        return search_word_related
    else:
        print(response.status_code)

--> 시도해보기
       naver_searching('삼성전자')


