with open('./data/ml/stevejobs.txt', 'r', encoding='utf8') as f:
    rows = f.readlines()
    lines = [row for row in rows]
    text = ' '.join(lines)
print(text[:200])


import re

compile = re.compile("[^ a-zA-Z0-9\.]+")
text = compile.sub('',text).lower()
print(text[:200])


import nltk
nltk.download('punkt')


sentences = nltk.sent_tokenize(text = text)
print(sentences[:3])


# nltk.word_tokenize(text)
# 단어 토큰화
word_token = []
for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    word_token.extend(words)
print(word_token[:10])


word_token2 = []
for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    word_token2.append(words)
print(word_token2[:3])


def tokenize_text(text):
    sentences = nltk.sent_tokenize(text)
    word_tokens = [nltk.word_tokenize(sentense) for sentense in sentences]
    return word_tokens
word_tokens = tokenize_text(text)
print(word_tokens[:2])


nltk.word_tokenize('로렘 입숨(lorem ipsum; 줄여서 립숨, lipsum)은 출판이나 그래픽 디자인 분야에서 폰트, 타이포그래피, 레이아웃 같은 그래픽 요소나 시각적 연출을 보여줄 때 사용하는 표준 채우기 텍스트로, 최종 결과물에 들어가는 실제적인 문장 내용이 채워지기 전에 시각 디자인 프로젝트 모형의 채움 글로도 이용된다. 이런 용도로 사용할 때 로렘 입숨을 그리킹(greeking)이라고도 부르며, 때로 로렘 입숨은 공간만 차지하는 무언가를 지칭하는 용어로도 사용된다.')


nltk.download('stopwords')


stopwords = nltk.corpus.stopwords.words('english')
print('영어 stop words 개수: ',len(stopwords))
print(stopwords[:10])


stopwords.append('.')


all_tokens = []
for sentence in word_tokens:
    filtered_words = []
    for word in sentence:
        if word not in stopwords:
            filtered_words.append(word)
    all_tokens.append(filtered_words)
print(all_tokens[:2])


print(word_tokens[:1])


from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()

print(stemmer.stem('happier'))
print(stemmer.stem('happiest'))
# print(stemmer.stem('amused'))


from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')


lemma = WordNetLemmatizer()

print(lemma.lemmatize('amusing','r')) 
print(lemma.lemmatize('amuses','r')) 
print(lemma.lemmatize('amused','r'))


print(stemmer.stem('happier'), stemmer.stem('happiest'))
print(lemma.lemmatize('happier','a'))
print(lemma.lemmatize('happiest','a'))


for i, sentence in enumerate(all_tokens):
    for j, word in enumerate(sentence):
        all_tokens[i][j] = stemmer.stem(word)
print(all_tokens[:2])


print(all_tokens[:5])


from sklearn.datasets import fetch_20newsgroups
import pandas as pd


train_news= fetch_20newsgroups(subset='train', 
                               remove=('headers', 'footers', 'quotes'), 
                               random_state=156)
X_train = train_news.data
y_train = train_news.target
print(X_train[0])


import pandas as pd

data = pd.Series(X_train)
print(data.shape)
data.head(5)


from sklearn.feature_extraction.text import CountVectorizer

cnt_vect = CountVectorizer()
cnt_vect.fit(X_train)

X_train_cnt_vect = cnt_vect.transform(X_train)
X_train_cnt_vect


print(X_train_cnt_vect)


print(X_train_cnt_vect.toarray())


string = ''.join(X_train)


compile = re.compile("[^ a-zA-Z]+")
text = compile.sub('',string).lower()


words = nltk.word_tokenize(text)


word_series = pd.Series(words)
print(len(word_series.unique()))


text


X_train_cnt_vect


cnt_vect.get_feature_names_out()[23663]


X_train_cnt_vect.toarray()[0]


cnt_vect.inverse_transform(X_train_cnt_vect.toarray()[[0]])


from sklearn.preprocessing import OneHotEncoder

oh_enc = OneHotEncoder()

oh_enc.fit_transform(data)


print(X_train[0])


train_news.target_names


train_news.target_names[y_train[0]]


from sklearn.datasets import load_iris

iris = load_iris()
iris_y = iris.target

iris.target_names


iris.target_names[0]


cnt_vect = CountVectorizer(stop_words='english')
cnt_vect.fit(X_train)

X_train_cnt_vect = cnt_vect.transform(X_train)
X_train_cnt_vect


cnt_vect = CountVectorizer(stop_words=stopwords)
cnt_vect.fit(X_train)

X_train_cnt_vect = cnt_vect.transform(X_train)
X_train_cnt_vect


cnt_vect = CountVectorizer(ngram_range=(1, 2), max_features = 3000)
cnt_vect.fit(X_train)

X_train_cnt_vect = cnt_vect.transform(X_train)
X_train_cnt_vect


cnt_vect.get_feature_names_out()[100:200]


cnt_vect = CountVectorizer(max_df = 0.85, min_df = 2)
cnt_vect.fit(X_train)

X_train_cnt_vect = cnt_vect.transform(X_train)
X_train_cnt_vect


X_train_cnt_vect.toarray().max()


X_train_cnt_vect.toarray()[X_train_cnt_vect.toarray() > 0].min()


from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(max_df = 0.85, 
                           stop_words='english',
                           max_features=3000,
                           ngram_range=(1,2),
                            token_pattern='[a-zA-Z]+')
tfidf_vect.fit(X_train)

X_train_tfidf_vect = tfidf_vect.transform(X_train)
X_train_tfidf_vect


df = pd.DataFrame(X_train_tfidf_vect.toarray(), columns = tfidf_vect.get_feature_names_out())


df


with open('data/ml/소나기.txt', 'r', encoding='utf8') as f:
    text = f.read()
print(text[:200])



import re
compile = re.compile("[^ ㄱ-ㅣ가-힣\.]+")
text = compile.sub('',text)
print(text[:200])


from konlpy.tag import Okt

def okt_tokenizer(text):
    okt = Okt()
    words = okt.morphs(text)
    return words

okt_tokenizer(text)


test_text = '나는 정말로 파이썬을 좋아한다. 아니 머신러닝을 더 좋아한다.'
print('normalize :', okt.normalize(test_text))
print('morphs :', okt.morphs(test_text))
print('nouns :', okt.nouns(test_text))
print('phrases :', okt.phrases(test_text))
print('pos :', okt.pos(test_text))



