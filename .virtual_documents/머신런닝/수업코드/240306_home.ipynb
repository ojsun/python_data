import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score


import re
from konlpy.tag import Okt


# 실습: 감성분석
df=pd.read_csv('./data/악플 데이터.csv')


# 전처리(한글): 클렌징, 토큰화, 불용어제거

def cleaning(text):
    p=re.compile('[^ a-zA-Zㄱ-ㅣ가-힣]+')
    result=p.sub('',text).lower()
    return result
df['content']=df['content'].apply(cleaning)

okt=Okt()
def okt_tokenizer(text):
    words=okt.pos(text, stem=True)
    filtered_words=[]
    for word, pos in words:
        if pos not in ['Josa']:
            filtered_words.append(word)
    return filtered_words

with open('./data/stopword.txt','r',encoding='utf-8') as f:
    word = f.read()
    stopwords=word.split('\n')


# 데이터셋 나누기- target, peatures, 학습, 테스트 sets

X_df= df['content']
y_df= df['target']

X_train, X_test, y_train, y_test= train_test_split(X_df, y_df, test_size=0.2, random_state=0)
print(X_train.shape,X_test.shape,y_test.shape)


# 벡터화, 적용 1. CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
cnt_vct=CountVectorizer(stop_words=stopwords, tokenizer=okt_tokenizer)
X_train_cnt=cnt_vct.fit_transform(X_train)
X_test_cnt=cnt_vct.transform(X_test)


# 모델 학습
from sklearn.linear_model import LogisticRegression
lr_clf=LogisticRegression(solver='liblinear', random_state=0)
lr_clf.fit(X_train_cnt, y_train)
pred=lr_clf.predict(X_test_cnt)
print('정확도 accuracy:', accuracy_score(y_test, pred))
print('정확도 f1 :',f1_score(y_test, pred))


# 벡터화, 적용 2. fidfVectorizer


from sklearn.feature_extraction.text import TfidfVectorizer
tf_vct=TfidfVectorizer(stop_words=stopwords, tokenizer=okt_tokenizer)
X_train_tf=tf_vct.fit_transform(X_train)
X_test_tf=tf_vct.transform(X_test)


from sklearn.linear_model import LogisticRegression
lr_clf=LogisticRegression(solver='liblinear', random_state=0)
lr_clf.fit(X_train_tf, y_train)
pred=lr_clf.predict(X_test_tf)
print('정확도 accuracy:', accuracy_score(y_test, pred))
print('정확도 f1 :',f1_score(y_test, pred))


test_text = '욕나온다. 쓰레기'
predict = tf_vct.transform([test_text])
lr_clf.predict(predict)


#실습2    # 독립변수들이 원핫형식(희소행렬)


df=pd.read_csv('./data/unsmile_data.csv')  


y = df.iloc[:, 1:]
y_label = pd.DataFrame({'target': y.columns})


from sklearn.preprocessing import OneHotEncoder
oh_enc = OneHotEncoder()
oh_enc.fit(y_label)


y_oh = y[oh_enc.categories_[0]]
y_oh.drop([5876, 11942], inplace=True)
oh_enc.inverse_transform(y_oh)


data = df[['문장']]
data.drop([5876, 11942], inplace=True)
data[['정답']] = oh_enc.inverse_transform(y_oh)
data











# 감정사전 함수 사용
def setiment_analyzer(text):
    import pandas as pd
    from konlpy.tag import Kkma
    from nltk.util import ngrams
    
    senti_words = pd.read_csv('./data/polarity.csv')
    kkma = Kkma()

    ngram1 = kkma.pos(text, join = True)
    ngram2 = list(ngrams(ngram1, n=2))
    new_ngram2 = []
    for n in ngram2:
        new_ngram2.append(';'.join(n))
    ngram3 = list(ngrams(ngram1, n=3))
    new_ngram3 = []
    for n in ngram3:
        new_ngram3.append(';'.join(n))

    words = ngram1 + new_ngram2 + new_ngram3

    result_df = senti_words[senti_words['ngram'].isin(words)]

    neg_df = result_df[result_df['max.value'] == 'NEG']
    pos_df = result_df[result_df['max.value'] == 'POS']
    neg_value = (neg_df['NEG'] / neg_df['freq']).sum()
    pos_value = (pos_df['NEG'] / pos_df['freq']).sum()
    neg_length = neg_df.shape[0]
    pos_length = pos_df.shape[0]

    if pos_length == 0:
        final_value = (pos_value ) - (neg_value / neg_length)
    elif neg_length == 0:
        final_value = (pos_value / pos_length) - (neg_value)
    else:
        final_value = (pos_value / pos_length) - (neg_value / neg_length)


    if final_value >= 0:
        print('긍정문장입니다.')
    else:
        print('부정문장입니다.')
    return final_value


text = df['content'].sample(1).iloc[0]
print(text)
setiment_analyzer(text)




































