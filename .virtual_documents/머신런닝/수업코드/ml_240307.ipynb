import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score








# 협업 필터링: 유저 기반 필터링 
#cosine_similarity를 이용한 유사도 측정

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
user_based = np.array([[5,1,4,4],[1,1,4,4],[4,2,4,4],[1,4,4,2]])
result = cosine_similarity(user_based)
result


pd.DataFrame(result, columns=['김','이','박','최'], index=['김','이','박','최'])


# result_df = pd.DataFrame(result)
# usernames = ['김', '이', '박', '최']
# result_df.columns = usernames
# result_df.index = usernames
# result_df


# KNN(최근접이웃)을 이용한 유사도 찾기
from sklearn.neighbors import NearestNeighbors

k = 2      # 본인 제외 1명 찾기
knn = NearestNeighbors(n_neighbors= k)
knn.fit(user_based)
top_k_distances, top_k_users = knn.kneighbors(user_based)
print(top_k_distances)
print(top_k_users)








## 컨텐츠 기반 필터링-실습: TMDB 5000 영화 데이터셋트
movies =pd.read_csv('../data/tmdb_5000_movies.csv')
print(movies.shape)
movies.head(2)


movies.columns   # 필요한 columns : 'genres', 'keywords'


movies_df = movies[['id','title', 'genres', 'vote_average', 'vote_count', 'popularity', 'keywords', 'overview']]
movies_df.head()


genres=movies_df['genres'].iloc[0]
eval(genres)   # 문자열 내 있는 코드를 실행하라는 함수


from ast import literal_eval         # literal_eval 리스트 등의 집합적 코드만 실행하여 안전을 기함/ eval은 모두 다 됨
movies_df['genres'] = movies_df['genres'].apply(literal_eval)
movies_df['keywords'] = movies_df['keywords'].apply(literal_eval)


movies_df[['genres','keywords']]


movies_df['genres'] = movies_df['genres'].apply(lambda x : [ y['name'] for y in x])  # list로 가져오기
movies_df['keywords'] = movies_df['keywords'].apply(lambda x : [ y['name'] for y in x])  # list로 가져오기


movies_df[['genres', 'keywords']].head()


# 내용이 많으니 단어들을 한데 묶어서 벡터화 시킴
from sklearn.feature_extraction.text import CountVectorizer

movies_df['genres_literal'] = movies_df['genres'].apply(lambda x : (' ').join(x))
count_vect = CountVectorizer(ngram_range=(1,2))
genre_mat = count_vect.fit_transform(movies_df['genres_literal'])
print(genre_mat.shape)


# 키워드는 허접해서 안씀
movies_df['keywords_literal'] = movies_df['keywords'].apply(lambda x : (' ').join(x))
count_vect = CountVectorizer(ngram_range=(1,2))
keywords_mat = count_vect.fit_transform(movies_df['keywords_literal'])
print(keywords_mat.shape)


genres_df=pd.DataFrame(genre_mat.toarray(), columns=count_vect.get_feature_names_out())


# 필요한 컬럼만 살리기
test_df=movies_df[['vote_average','vote_count','popularity']]


pd.concat([test_df, genres_df], axis=1)


# 장르 유사도에 따른 영화 추천
genre_sim = cosine_similarity(genre_mat, genre_mat)
print(genre_sim.shape)
print(genre_sim)


# 유사도가 큰 순서로 정렬후 인덱스 반환
genre_sim_sorted_ind = genre_sim.argsort()[:, ::-1]  # 무조건 1개 밖에 추천 못함// [::-1]은 내림차순 [::1]은 오름차순
print(genre_sim_sorted_ind)


def find_sim_movie(df, sorted_ind, title_name, top_n=1):
    title_movie = df[df['title'] == title_name]
    title_index = title_movie.index.values
    similar_indexes = sorted_ind[title_index, :(top_n)]
    similar_indexes = similar_indexes.reshape(-1)
    return df.iloc[similar_indexes]
    
similar_movies = find_sim_movie(movies_df, genre_sim_sorted_ind, 'The Godfather',5)
similar_movies


np.sort(genre_sim[2731])[::-1][:10]  #장르만으로 추천할때는 모두 같은 장르만 나오므로 추천시스템 문제가 있음 


# 같은 장르 + 평점 높은 작품 먼저 추천 - 이것도 투표수 적을땐 객관성 없음
movies_df[['title','vote_average','vote_count']].sort_values('vote_average', ascending=False)[:10]


## 장르 유사도에 따른 영화 추천
C = movies_df['vote_average'].mean()       # 전체 영화에 대한 평균평점
m = movies_df['vote_count'].quantile(0.5)  # 평점을 부여하기 위한 최소횟수
print('C:',round(C,3), ', m:',round(m,3))


# 장르 유사도 함수(가중치 계산)
percentile = 0.6
m = movies_df['vote_count'].quantile(percentile) # 평점을 부여하기 위한 최소횟수
C = movies_df['vote_average'].mean()             # 전체 영화에 대한 평균평점
def weighted_vote_average(record):
    v = record['vote_count']                     # 개별 영화에 평점을 투표한 수
    R = record['vote_average']                   # 개별 영화에 대한 평균 평점
    return ( (v/(v+m)) * R ) + ( (m/(m+v)) * C ) 
movies_df['weighted_vote'] = movies_df.apply(weighted_vote_average, axis=1)   # 칼럼 추가
movies_df['weighted_vote'] 


# 가중치 변수로 추천영화 정렬
movies_df[['title','vote_average','weighted_vote']].sort_values('weighted_vote', ascending = False)[:10]


# 투표수로 정렬/ 투표수를 감안하지 않아서 점수는 높지만 문제 있음
movies_df[['title','vote_average','weighted_vote']].sort_values('vote_average', ascending = False)[:10]


## 아이템 기반 최근접이웃(KNN) 협업필터링 실습

movies = pd.read_csv('../data/movies.csv')
ratings = pd.read_csv('../data/ratings.csv')
print(movies.shape)
print(ratings.shape)


movies


ratings          # timestamp: 현재 취향을 참고하기 위해 최근순으로 적용하면 좋음


# 참고자료-ratings 이해하고 안씀
pd.read_csv('../data/UserRatings1.csv').head()


ratings = ratings[['userId', 'movieId', 'rating']]
ratings_matrix = ratings.pivot_table(values='rating', index='userId', columns='movieId')
ratings_matrix.head()   # movie 데이터는 9744 이므로 맞춰야 함


rating_movies = pd.merge(ratings, movies, on='movieId')
ratings_matrix = rating_movies.pivot_table(values='rating', index='userId', columns='title')
ratings_matrix = ratings_matrix.fillna(0)
ratings_matrix.head(3)


ratings_matrix.shape  # 610명, 9719편 영화


# 안 본 영화들의 0으로 되어있는 칸에 예측점수 채워넣기 --목표


# 영화(아이템) 간 유사도 산출  : 평점만 가지고 유사도 산출
from sklearn.metrics.pairwise import cosine_similarity
ratings_matrix_T = ratings_matrix.transpose()
item_sim = cosine_similarity(ratings_matrix_T)
item_sim_df = pd.DataFrame(data=item_sim, index=ratings_matrix.columns, columns=ratings_matrix.columns)
print(item_sim_df.shape)
item_sim_df.head()


# 가장 가까운(유사도) 영화 뽑아보기
item_sim_df["Godfather, The (1972)"].sort_values(ascending=False)[:6]


## 최근접 이웃 협업 필터링 실습
# 개인 예측 평점 함수(가중치 계산)
def predict_rating(ratings_arr, item_sim_arr ):
    ratings_pred = ratings_arr.dot(item_sim_arr)/ np.array([np.abs(item_sim_arr).sum(axis=1)]) # 계산을 위해 행렬로 바꿈
    return ratings_pred
ratings_pred = predict_rating(ratings_matrix.values , item_sim_df.values)


ratings_pred


# 유저별 평점 예측 - 빈칸채우기 --> 실제 유저가 평가한 점수들과도 오차가 큼/ 그래도 의미가 있음
ratings_pred_matrix = pd.DataFrame(data=ratings_pred, index= ratings_matrix.index, columns = ratings_matrix.columns)
ratings_pred_matrix.head(3)


# 평가해보기
from sklearn.metrics import mean_squared_error
def get_mse(pred, actual):                         # actual 실제데이터/ 비어있는 상태
    pred = pred[actual.nonzero()].flatten()        # 그래서 제로가 아닌 것들만 가져옴   
    actual = actual[actual.nonzero()].flatten()     
    return np.sqrt(mean_squared_error(pred, actual))  # 이미 루트값이므로 
print('아이템 기반 모든 인접 이웃 RMSE: ', get_mse(ratings_pred, ratings_matrix.values ))
# mse에 루트를 시키면 +-3.14 정도  


actual=ratings_matrix.values[ratings_matrix.values.nonzero()].flatten()  # 실제 데이터


rating_pred=ratings_pred[ratings_matrix.values.nonzero()].flatten()   # 예측 데이터


def predict_rating_topsim(ratings_arr, item_sim_arr, n=20):  # top 20개만 
   
    pred = np.zeros(ratings_arr.shape)            # 유저-아이템 행렬만큼 0행렬 생성 
    for col in range(ratings_arr.shape[1]):       # 아이템 개수 만큼 수행
        top_n_items = [np.argsort(item_sim_arr[:, col])[:-n-1:-1]]  
        for row in range(ratings_arr.shape[0]):       # 모든 유저에 대한
            pred[row, col] = item_sim_arr[col, :][top_n_items].dot(ratings_arr[row, :][top_n_items].T) 
            pred[row, col] /= np.sum(np.abs(item_sim_arr[col, :][top_n_items]))  # /= 왼쪽 변수에서 오른쪽 값을 나누고 결과를 왼쪽변수에 할당
    return pred


ratings_pred = predict_rating_topsim(ratings_matrix.values , item_sim_df.values, n=10)
print('아이템 기반 인접 TOP-10 이웃 RMSE: ', get_mse(ratings_pred, ratings_matrix.values ))



# 유저기반의 경우에는 행렬을 바꾸고 아이템대신 유저로 바꾸면 어덜까?? 찾아볼것


# 계산된 예측 평점 데이터는 DataFrame으로 재생성
ratings_pred_matrix = pd.DataFrame(data=ratings_pred, index= ratings_matrix.index,columns = ratings_matrix.columns)
ratings_pred_matrix


user_rating_id = ratings_matrix.loc[1, :]
user_rating_id[ user_rating_id > 0].sort_values(ascending=False)[:10]


def get_unseen_movies(ratings_matrix, userId):  # 안 본 리스트
    user_rating = ratings_matrix.loc[userId,:]
    already_seen = user_rating[ user_rating > 0].index.tolist()
    movies_list = ratings_matrix.columns.tolist()
    unseen_list = [ movie for movie in movies_list if movie not in already_seen]
    return unseen_list


def recomm_movie_by_userid(pred_df, userId, unseen_list, top_n=10):
    recomm_movies = pred_df.loc[userId, unseen_list].sort_values(ascending=False)[:top_n]
    return recomm_movies
unseen_list = get_unseen_movies(ratings_matrix, 9)
recomm_movies = recomm_movie_by_userid(ratings_pred_matrix, 9, unseen_list, top_n=10)
recomm_movies = pd.DataFrame(data=recomm_movies.values,index=recomm_movies.index,columns=['pred_score'])
recomm_movies


ratings_matrix['Godfather: Part II, The (1974)']





## 잠재요인 협업 필터  : 차원축소의 잠재요인 (가중치), 경사하강법) ==> 모델기반 협업(알고리즘적 예측)
# 잠재요인 협업필더링 실습 : 오차계산
def get_rmse(R, P, Q, non_zeros): # P, Q 예측행렬/ R 실제값
    error = 0
    # 두개의 분해된 행렬 P와 Q.T의 내적 곱(np.dot)으로 예측 R 행렬 생성
    full_pred_matrix = np.dot(P, Q.T) 
    # 실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출
    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]
    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]
    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]
    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]
    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)
    rmse = np.sqrt(mse)
    return rmse


# 확률적 경사하강법 학습 함수
def matrix_factorization(R, K, steps=100, learning_rate=0.01, r_lambda = 0.01):
    num_users, num_items = R.shape
    # P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 랜덤한 값으로 입력합니다. 
    np.random.seed(1)
    P = np.random.normal(scale=1./ K, size=(num_users, K)) 
    Q = np.random.normal(scale=1./ K, size=(num_items, K))  # 나중에 전치시킬 예정
    break_count = 0
    # R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트 객체에 저장. 
    non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ]  # 빈칸에 0넣기/ 0아닌 곳은 남기기
    # 이전 코드에 이어서 작성
    # SGD기법(차원축소)으로 P와 Q 매트릭스를 계속 업데이트.   
    for step in range(steps):
        for i, j, r in non_zeros:
            # 실제 값(r)과 예측 값의 차이인 오류 값 구함
            eij = r - np.dot(P[i, :], Q[j, :].T)
            # Regularization을 반영한 SGD 업데이트 공식 적용
            P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])  # P[i,:] 가중치  *** 중요
            Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])  # Q[j,:]  가중치  ** 중요 
            # // 둘다 가동하면 속도문제, 결과 문제있음: 번갈아가면서 실행하는 ASL기법(SGD와 비교) 있음
        rmse = get_rmse(R, P, Q, non_zeros)
        if (step % 10) == 0 :
            print("### iteration step : ", step," rmse : ", rmse)
    return P, Q


P, Q = matrix_factorization(ratings_matrix.values, K=50, steps=100, learning_rate=0.01, r_lambda = 0.01)
pred_matrix = np.dot(P, Q.T)


# 예측 평점 출력
ratings_pred_matrix = pd.DataFrame(data=pred_matrix, index= ratings_matrix.index,columns = ratings_matrix.columns)
ratings_pred_matrix.head(3)


# 영화 추천
# 9번 사용자가 관람하지 않는 영화명 추출
unseen_list = get_unseen_movies(ratings_matrix, 9)
# 아이템 기반의 인접 이웃 협업 필터링으로 영화 추천
recomm_movies = recomm_movie_by_userid(ratings_pred_matrix, 9, unseen_list, top_n=10)
# 평점 데이타를 DataFrame으로 생성. 
recomm_movies = pd.DataFrame(data=recomm_movies.values, index=recomm_movies.index, columns=['pred_score'])
recomm_movies





## 실습 /추천시스템-유사성에 의거한 추천 유머


joke=pd.read_csv('../data/JokeText.csv')
joke


# 컨텐츠기반 추천시스템: 
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer


from nltk.corpus import stopwords
stopwords = stopwords.words('english')


# 벡터화
from sklearn.feature_extraction.text import CountVectorizer
cnt_vct = CountVectorizer(stop_words=stopwords,ngram_range=(1,2))
data=cnt_vct.fit_transform(joke['JokeText'])  

data_sim=cosine_similarity(data)
data_sim_sorted_index= np.argsort(data_sim, axis=1)[:,::-1]



data_sim_sorted_index


joke['JokeText']


df=pd.read_csv('../data/UserRatings1.csv')


df2=pd.read_csv('../data/UserRatings2.csv')


ratings=pd.concat([df,df2], axis=1)
ratings=ratings.fillna(0)
ratings.drop('JokeId', axis=1, inplace=True)


ratings


# 전치하기
ratings_matrix_T = ratings_matrix.transpose()
item_sim = cosine_similarity(ratings_matrix_T)





ratings.T.values


def predict_rating_topsim(ratings_arr, item_sim_arr, n=20):  # top 20개만 
   
    pred = np.zeros(ratings_arr.shape)            # 유저-아이템 행렬만큼 0행렬 생성 
    for col in range(ratings_arr.shape[1]):       # 아이템 개수 만큼 수행
        top_n_items = [np.argsort(item_sim_arr[:, col])[:-n-1:-1]]  
        for row in range(ratings_arr.shape[0]):       # 모든 유저에 대한
            pred[row, col] = item_sim_arr[col, :][top_n_items].dot(ratings_arr[row, :][top_n_items].T) 
            pred[row, col] /= np.sum(np.abs(item_sim_arr[col, :][top_n_items]))  # /= 왼쪽 변수에서 오른쪽 값을 나누고 결과를 왼쪽변수에 할당
    return pred

predict_rating_topsim(ratings.T.values, item_sim, n=10)   # 다시 한번 보기 **


P, Q = matrix_factorization(ratings.T.values[:1000], K=50, steps=100, learning_rate=0.01, r_lambda = 0.01)
pred_matrix = np.dot(P, Q.T)                    # NaN 값이 있다고 에러


def re_matrix_factorization(R, K, P, Q, steps=100, learning_rate=0.01, r_lambda = 0.01):
    # 만약 데이터프레임이 들어오면
    if type(R) == 'pandas.core.frame.DataFrame':
        R = R.values

    num_users, num_items = R.shape
    # P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 랜덤한 값으로 입력합니다. 
    np.random.seed(1)
    
    break_count = 0
    # R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트 객체에 저장. 
    non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ]
    # 이전 코드에 이어서 작성
    # SGD기법으로 P와 Q 매트릭스를 계속 업데이트. 
    for step in range(steps):
        for i, j, r in non_zeros:
            # 실제 값과 예측 값의 차이인 오류 값 구함
            eij = r - np.dot(P[i, :], Q[j, :].T)
            # Regularization을 반영한 SGD 업데이트 공식 적용
            P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])
            Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])
        
        
        if (step % 10) == 0 :
            rmse = get_rmse(R, P, Q, non_zeros)
            print("### iteration step : ", step," rmse : ", rmse)
    return P, Q


P, Q = re_matrix_factorization(ratings.T.values[:1000], K=50, P = P, Q =  Q, 
                               steps=100, learning_rate=0.01, r_lambda = 0.01)
pred_matrix = np.dot(P, Q.T)     


ratings.T.head(5)
























