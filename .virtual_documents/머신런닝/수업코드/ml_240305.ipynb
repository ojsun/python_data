import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')


from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score


import nltk
import re
from nltk import sent_tokenize, word_tokenize
from konlpy.tag import Okt, Hannanum, Komoran, Kkma
from nltk.stem import LancasterStemmer
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer





with open('../data/소나기.txt', 'r', encoding='utf8') as f:
    text = f.read()
print(text[:200])


import re
compile = re.compile("[^ ㄱ-ㅣ가-힣\.]+")
text = compile.sub('',text)
print(text[:200])


# 문장 토큰화
import nltk
sentences = nltk.sent_tokenize(text)
print(sentences[:3])


okt = Okt()
okt.morphs(text)


# 단어 토큰화
from konlpy.tag import Okt
okt = Okt()
words = []
for sentence in sentences:
    word = okt.morphs(sentence)
    words.append(word)
print(words[:5])


nltk.word_tokenize(text)   # 형태소, 조사 제거 안되므로 konlpy


okt = Okt()
def okt_tokenizer(text):
    tokens_ko = okt.morphs(text, stem=True)
    return tokens_ko
word_tokens = okt_tokenizer(text)
print(word_tokens[:20])


# 단어 토큰화
from konlpy.tag import Okt
okt = Okt()
words = []
for sentence in sentences:
    word = okt.morphs(sentence)
    words.append(word)
print(words[:2])


test_text = '나는 정말로 파이썬을 좋아한다. 아니 머신러닝을 더 좋아한다.'
print('normalize :', okt.normalize(test_text)) # 문장으로 추출
print('morphs :', okt.morphs(test_text))       # 구문 분석
print('nouns :', okt.nouns(test_text))         # 명사만
print('phrases :', okt.phrases(test_text))     # 구문
print('pos :', okt.pos(test_text))             # 품사와 함께 값고 함께 





# ---------------------------------- 수업시간에 ------------


print('morphs :', okt.morphs(text,stem=True)) 


print('morphs :', okt.pos(test_text)) 


# 불용어 제거
with open('../data/stopword.txt','r',encoding='utf-8') as f:
    word = f.read()
stopwords = word.split('\n')
print(stopwords[:10])


def Stopwords(words, Stopwords = None):
    filtered_words = []
    for word in words:
        if word not in stopwords: 
            filtered_words.append(word)
    return filtered_words
filtered_words = Stopwords(word_tokens, stopwords)
print(filtered_words[:20])


hah=Hannanum()
han_stopwords=[]
for word in stopwords:
    han_stopwords.expend(han.morphs(word))


filtered_words = []
for word in word_tokens:
    if word not in stopwords:
        filtered_words.append(word)
print(filtered_words[:20])


df= pd.read_csv('../data/petition.csv')
data=df.head()


from sklearn.feature_extraction.text import CountVectorizer
cnt_vect = CountVectorizer(tokenizer=okt_tokenizer, token_pattern='[ㄱ-| 가-힣]+',
                           stop_words=stopwords)
cnt_vect.fit(data)
words_cnt_vect = cnt_vect.transform(data)
words_cnt_vect


# 너무 길어서 골라 쓸 예정 : 결국  5줄
result=cnt_vect.fit_transform(data['content'])
pd.DataFrame(result.toarray(), columns=cnt_vect.get_feature_names_out())


data.values


cnt_vect = CountVectorizer(tokenizer=okt_tokenizer, token_pattern='[ㄱ-| 가-힣]+',
                           stop_words=stopwords)
cnt_vect.fit(data)
words_cnt_vect = cnt_vect.transform(data)


result=cnt_vect.fit_transform(data['content'])
pd.DataFrame(result.toarray(), columns=cnt_vect.get_feature_names_out())





df= pd.read_csv('../data/petition.csv')
data=df.head()
data


df.info()


df['category'].unique()


df['category'].value_counts()  # 문화.예술


df[df['category'].isin(['문화/예술/체육/언론', '미래', '경제민주화'])]


df.info()


from sklearn.preprocessing import LabelEncoder
df_data = df[['content', 'category']]
lb_enc = LabelEncoder()
df_data['category'] = lb_enc.fit_transform(df_data['category'])
df_data.info()


df_data=df_data.sample(3000)


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test =train_test_split(df_data['content'], df_data['category'], test_size = 0.2, random_state = 0)
print(f'학습 데이터 수:{len(X_train)}, 평가 데이터 수:{len(X_test)}')


import re
def cleaning(text):
    p = re.compile("[^ ㄱ-ㅣ가-힣\.]+")
    result = p.sub('',text)
    return result


cleaning(0.1)


X_train = X_train.astype(str)


X_train_clean=X_train.apply(cleaning)
X_test_clean=X_test.apply(cleaning)





from konlpy.tag import Okt
okt = Okt()
words = []
def okt_token(text):
    resuit=okt.morphs(text, stem=True)
    return result


cnt_vect=CountVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)


# 시간때문에 줄임
cnt_vect.fit(X_train_clean)   
X_train_cnt=cnt_vect.transform(X_train_clean)
X_test_cnt=cnt_vect.transform(X_test_clean)


pd.DataFrame(X_train_cnt_vct.toarray(), columns=cnt_vect.get_feature_names_out())


print(X_train_cnt_vct)


from sklearn.linear_model import LogisticRegression
lr_clf=LogisticRegression()
lr_clf.fit(X_train_cnt_vct, y_train)
pred=lr_clf.predict(X_test_cnt_vct)
print('예측 정확도 : ', accuracy_score(y_test, pred))


# pipeline : 같은 모양만 맞추면 돌아감// 다른 file 적용시 import 등 필요없음


# pipeline을 안쓴 상태로 예측하는 서비스

text= '다.'
text_trans=cnt_vect.transform([text])
result=lr_clf.predict(text_trans)
final_res=lb_enc.inverse_transform(result)
print(final_res[0])


# 파이프라인을 이용하여 예측하는 서비스
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
pipeline=Pipeline([('vect', TkidfVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)),
                   ('lr_clf', LogisticRegression())])
# params={'vect__ngram_range': [(1,1),(1,2)],
#         'vect__max_df': [100,200,300],
#         'lr_clf__C': [1,5,7]}
Tkidf_vect=



text= '우리 아이 유치원 '
result= pipe.predict([text])

classes_ =['문화/예술/체육/언론', '미래', '경제민주화']
classes_[result[0]]


tfidf_vct=TfidfVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)
tfidf_vct.fit(X_train)
X_train_tfidf=tfidf_vct.transform(X_train)
X_test_tfidf=tfidf_vct.transform(X_test)
lr_clf=LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_tfidf, y_train)






