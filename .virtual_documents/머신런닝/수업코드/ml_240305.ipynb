import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')


from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score


import nltk
import re
from nltk import sent_tokenize, word_tokenize
from konlpy.tag import Okt, Hannanum, Komoran, Kkma
from nltk.stem import LancasterStemmer
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer





with open('../data/소나기.txt', 'r', encoding='utf8') as f:
    text = f.read()
print(text[:200])


import re
compile = re.compile("[^ ㄱ-ㅣ가-힣\.]+")
text = compile.sub('',text)
print(text[:200])


# 문장 토큰화
import nltk
sentences = nltk.sent_tokenize(text)
print(sentences[:3])


okt = Okt()
okt.morphs(text)


# 단어 토큰화
from konlpy.tag import Okt
okt = Okt()
words = []
for sentence in sentences:
    word = okt.morphs(sentence)
    words.append(word)
print(words[:5])


nltk.word_tokenize(text)   # 형태소, 조사 제거 안되므로 konlpy


okt = Okt()
def okt_tokenizer(text):
    tokens_ko = okt.morphs(text, stem=True)
    return tokens_ko
word_tokens = okt_tokenizer(text)
print(word_tokens[:20])


# 단어 토큰화
from konlpy.tag import Okt
okt = Okt()
words = []
for sentence in sentences:
    word = okt.morphs(sentence)
    words.append(word)
print(words[:2])


test_text = '나는 정말로 파이썬을 좋아한다. 아니 머신러닝을 더 좋아한다.'
print('normalize :', okt.normalize(test_text)) # 문장으로 추출
print('morphs :', okt.morphs(test_text))       # 구문 분석
print('nouns :', okt.nouns(test_text))         # 명사만
print('phrases :', okt.phrases(test_text))     # 구문
print('pos :', okt.pos(test_text))             # 품사와 함께 값고 함께 





# ---------------------------------- 수업시간에 ------------


print('morphs :', okt.morphs(text,stem=True)) 


print('morphs :', okt.pos(test_text)) 


# 불용어 제거
with open('../data/stopword.txt','r',encoding='utf-8') as f:
    word = f.read()
stopwords = word.split('\n')
print(stopwords[:10])


def Stopwords(words, Stopwords = None):
    filtered_words = []
    for word in words:
        if word not in stopwords: 
            filtered_words.append(word)
    return filtered_words
filtered_words = Stopwords(word_tokens, stopwords)
print(filtered_words[:20])


hah=Hannanum()
han_stopwords=[]
for word in stopwords:
    han_stopwords.expend(han.morphs(word))


filtered_words = []
for word in word_tokens:
    if word not in stopwords:
        filtered_words.append(word)
print(filtered_words[:20])


df= pd.read_csv('../data/petition.csv')
data=df.head()


from sklearn.feature_extraction.text import CountVectorizer
cnt_vect = CountVectorizer(tokenizer=okt_tokenizer, token_pattern='[ㄱ-| 가-힣]+',
                           stop_words=stopwords)
cnt_vect.fit(data)
words_cnt_vect = cnt_vect.transform(data)
words_cnt_vect


# 너무 길어서 골라 쓸 예정 : 결국  5줄
result=cnt_vect.fit_transform(data['content'])
pd.DataFrame(result.toarray(), columns=cnt_vect.get_feature_names_out())


data.values


cnt_vect = CountVectorizer(tokenizer=okt_tokenizer, token_pattern='[ㄱ-| 가-힣]+',
                           stop_words=stopwords)
cnt_vect.fit(data)
words_cnt_vect = cnt_vect.transform(data)


result=cnt_vect.fit_transform(data['content'])
pd.DataFrame(result.toarray(), columns=cnt_vect.get_feature_names_out())





df= pd.read_csv('../data/petition.csv')
data=df.head()
data


df.info()


df['category'].unique()


df['category'].value_counts()  # 문화.예술


df[df['category'].isin(['문화/예술/체육/언론', '미래', '경제민주화'])]


df.info()


from sklearn.preprocessing import LabelEncoder
df_data = df[['content', 'category']]
lb_enc = LabelEncoder()
df_data['category'] = lb_enc.fit_transform(df_data['category'])
df_data.info()


df_data=df_data.sample(3000)


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test =train_test_split(df_data['content'], df_data['category'], test_size = 0.2, random_state = 0)
print(f'학습 데이터 수:{len(X_train)}, 평가 데이터 수:{len(X_test)}')


import re
def cleaning(text):
    p = re.compile("[^ ㄱ-ㅣ가-힣\.]+")
    result = p.sub('',text)
    return result


cleaning(0.1)


X_train = X_train.astype(str)


X_train_clean=X_train.apply(cleaning)
X_test_clean=X_test.apply(cleaning)





from konlpy.tag import Okt
okt = Okt()
words = []
def okt_token(text):
    resuit=okt.morphs(text, stem=True)
    return result


cnt_vect=CountVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)


# 시간때문에 줄임
cnt_vect.fit(X_train_clean)   
X_train_cnt=cnt_vect.transform(X_train_clean)
X_test_cnt=cnt_vect.transform(X_test_clean)


pd.DataFrame(X_train_cnt_vct.toarray(), columns=cnt_vect.get_feature_names_out())


print(X_train_cnt_vct)


from sklearn.linear_model import LogisticRegression
lr_clf=LogisticRegression()
lr_clf.fit(X_train_cnt_vct, y_train)
pred=lr_clf.predict(X_test_cnt_vct)
print('예측 정확도 : ', accuracy_score(y_test, pred))


# pipeline : 같은 모양만 맞추면 돌아감// 다른 file 적용시 import 등 필요없음


# pipeline을 안쓴 상태로 예측하는 서비스 **

text= '다.'
text_trans=cnt_vect.transform([text])
result=lr_clf.predict(text_trans)
final_res=lb_enc.inverse_transform(result)
print(final_res[0])


# 파이프라인을 이용하여 예측하는 서비스  **
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
pipeline=Pipeline([('vect', TkidfVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)),
                   ('lr_clf', LogisticRegression())])
# params={'vect__ngram_range': [(1,1),(1,2)],
#         'vect__max_df': [100,200,300],
#         'lr_clf__C': [1,5,7]}
Tkidf_vect=



text= '우리 아이 유치원 '
result= pipe.predict([text])

classes_ =['문화/예술/체육/언론', '미래', '경제민주화']
classes_[result[0]]


tfidf_vct=TfidfVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)
tfidf_vct.fit(X_train)
X_train_tfidf=tfidf_vct.transform(X_train)
X_test_tfidf=tfidf_vct.transform(X_test)
lr_clf=LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_tfidf, y_train)





df=pd.read_csv('../data/네이버 스마트스토어 리뷰.csv')


df.score.value_counts()


review_df = pd.read_csv('../data/labeledTrainData.tsv', header=0, sep="\t", quoting=3)
review_df.head(3)


review_df['sentiment'].value_counts()


def cleaning(text):
    p=re.compile('[^ a=zA-Z]+')
    return p.sub('',text)

review_df['review'] =review_df['review'].apply(cleaning)


import nltk
from nltk.stem import LancasterStemmer
stemmer=LancasterStemmer()

words=nltk.word_tokenize(text)
print(word)
#stopwords=nltk.corpus.stopwords.words('english')

nltk.download('stopwords')
filtered_words=[]
# def **
for word in words:
    word= word.lower()
    word=stemmer.stem(word)
    if word not in stopwords:
        filtered_words.append(word)




X_train, X_test, y_train, y_test=train_test_split(review_df['review'],review_df['sentiment'], test_size=0.2, random_state=0 ) 


# 벡처화
from sklearn.feature_extraction.text import CountVectorizer
cnt_vct=CountVectorizer()
cnt_vct.fit(X_train)   # y값 없음!!
X_train_cnt_vct=cnt_vct.transform(X_train)
X_test_cnt_vct=cnt_vct.transform(X_test)

# 3.모델로 분류
from sklearn.linear_model import LogisticRegression
lr_clf=LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_cnt_vct, y_train)
pred=lr_clf.predict(X_test_cnt_vct)
print('예측 정확도 : ', accuracy_score(y_test, pred))


# 영화 평가 분석
review_df = pd.read_csv('../data/labeledTrainData.tsv', header=0, sep="\t", quoting=3)
review_df.head(3)


import re
review_df['review'] = review_df['review'].str.replace('<br />',' ')
review_df['review'] = review_df['review'].apply( lambda x : re.sub("[^a-zA-Z]", " ", x) )
review_df.head(3)


from sklearn.model_selection import train_test_split
class_df = review_df['sentiment']
feature_df = review_df.drop(['id','sentiment'], axis=1)
X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.3, random_state=156)
print(X_train.shape, X_test.shape)


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score
pipeline = Pipeline([('cnt_vect', CountVectorizer(stop_words='english', 
                                                  ngram_range=(1,2) )),('lr_clf', LogisticRegression(solver='liblinear', C=10))])


pipeline.fit(X_train['review'], y_train)
pred = pipeline.predict(X_test['review'])
pred_probs = pipeline.predict_proba(X_test['review'])[:,1]
print(f'예측 정확도는 {accuracy_score(y_test ,pred):.4f}')
print(f'ROC-AUC는 {roc_auc_score(y_test, pred_probs):.4f}')


from sklearn.feature_extraction.text import TfidfVectorizer
pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2) )),('lr_clf', LogisticRegression(solver='liblinear', C=10))])
pipeline.fit(X_train['review'], y_train)
pred = pipeline.predict(X_test['review'])
pred_probs = pipeline.predict_proba(X_test['review'])[:,1]
print(f'예측 정확도는 {accuracy_score(y_test ,pred):.4f}')
print(f'ROC-AUC는 {roc_auc_score(y_test, pred_probs):.4f}')


## 2번째 감성분석 사례: 한글 데이터


df=pd.read_csv('../data/네이버 스마트스토어 리뷰.csv')
df.head()


import re
def cleaning(text):
    p = re.compile("[^ ㄱ-ㅣ가-힣\.]+")
    result = p.sub('',text)
    return result


df['content']=df['content'].apply(cleaning)


X_train, X_test, y_train, y_test=train_test_split(df['content'],df['score'], test_size=0.2, random_state=0 ) 


okt = Okt()

def okt_tokenizer(text):
    word = okt.morphs(text, stemm=True )
    return words


tfidf_vct=TfidfVectorizer(tokenizer=okt_tokenizer, stop_words=stopwords, max_features=5000)
tfidf_vct.fit(X_train)
X_train_tf=tfidf_vct.transform(X_train)
X_test_tf=tfidf_vct.transform(X_test)


lr_clf=LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_tf, y_train)
pred=lr_clf.predict(X_test_tf)
print('예측 정확도 : ', accuracy_score(y_test, pred))


# LDA 토핑모델링



from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 
    'comp.windows.x', 'talk.politics.mideast', 
    'soc.religion.christian', 'sci.electronics', 'sci.med' ]
news_df= fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=cats, random_state=0)


count_vect = CountVectorizer(max_df=0.95,max_features=1000, min_df=2,stop_words='english', token_pattern = '[a-zA-Z]+',ngram_range=(1,2))
feat_vect = count_vect.fit_transform(news_df.data)
print('CountVectorizer Shape:', feat_vect.shape)



lda = LatentDirichletAllocation(n_components=8, random_state=0)
lda.fit(feat_vect)
print(lda.components_.shape)
print(lda.components_)


def display_topics(model, feature_names, no_top_words):
    for topic_index, topic in enumerate(model.components_):
        print('Topic #',topic_index)
        topic_word_indexes = topic.argsort()[::-1]
        top_indexes=topic_word_indexes[:no_top_words]
        feature_concat = ' '.join([feature_names[i] for i in top_indexes])
        print(feature_concat)
feature_names = count_vect.get_feature_names_out()
display_topics(lda, feature_names, 15)


df = pd.read_csv('../data/petition.csv')
cats = ['정치개혁', '인권/성평등', '안전/환경', '교통/건축/국토', '육아/교육']
df_cats = df[df['category'].isin(cats)]
df_samples = df_cats.sample(frac=0.05, random_state = 0)
df_samples['category'].value_counts()


import re
df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))
from konlpy.tag import Okt
okt = Okt()
def okt_tokenizer(text):
    tokens_ko = okt.morphs(text, stem = True)
    return tokens_ko
with open('../data/stopword.txt','r',encoding='utf-8') as f:
    word = f.read()
stopwords = word.split('\n')


from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(max_df = 0.9,max_features = 1000,min_df = 2, ngram_range = (1, 2),tokenizer = okt_tokenizer,stop_words = stopwords)
df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])
df_tfidf_vect



from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 5, random_state = 0)
lda.fit(df_tfidf_vect)
print(lda.components_.shape)
print(lda.components_)


feature_names = tfidf_vect.get_feature_names_out()
display_topics(lda, feature_names, 10)


# 텍스트 축소(요약)
df = pd.read_csv('../data/petition.csv')
# df_cat=df[df['category']=='육아/교육']


# df_cat['content']= df_cat['content'].apply(cleaning)


# 하나의 주제에서 주요 주제 뽑기
cats = ['육아/교육']
df_cats = df[df['category'].isin(cats)]
df_samples = df_cats.sample(frac=0.5, random_state = 0)
df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))


tfidf_vect = TfidfVectorizer(max_df = 0.9,max_features = 1000,min_df = 2, ngram_range = (1, 2),tokenizer = okt_tokenizer,stop_words = stopwords)
df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])   # 중요 **


from sklearn.de composition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 5, random_state = 0)    # 중요 **
lda.fit(df_tfidf_vect)
feature_names = tfidf_vect.get_feature_names_out()
display_topics(lda, feature_names, 15)


lda.components_.argsort()[::-1][:5]


lda=pd.DataFrame(lda.components_, columns=tfidf_vect.get_feature_names_out())


# KMrans 군집


df = pd.read_csv('../data/petition.csv')
cats = ['안전/환경', '교통/건축/국토', '육아/교육', '일자리']
df_cats = df[df['category'].isin(cats)]
df_samples = df_cats.sample(frac=0.1, random_state = 0)
df_samples.shape





from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(max_df = 0.85,
                            min_df = 2, 
                            ngram_range = (1, 2),
                            tokenizer = okt_tokenizer,
                            stop_words = stopwords)
df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])
df_tfidf_vect



from sklearn.cluster import KMeans
km_cluster = KMeans(n_clusters = 4, max_iter = 10000, random_state = 0)
km_cluster.fit(df_tfidf_vect)
cluster_label = km_cluster.labels_
clust_df = df_samples[['content', 'category']]
clust_df['cluster'] = cluster_label
clust_df.head(3)


clust_df.groupby(['category', 'cluster'])['content'].count()


clust_df[clust_df['cluster'] == 1]


def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):
    cluster_details = {}
    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]
    for cluster_num in range(clusters_num):
        cluster_details[cluster_num] = {}
        cluster_details[cluster_num]['cluster'] = cluster_num
        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]
        top_features = [ feature_names[ind] for ind in top_feature_indexes ]
        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()
        cluster_details[cluster_num]['top_features'] = top_features
        cluster_details[cluster_num]['top_features_value'] = top_feature_values
    return cluster_details



def print_cluster_details(cluster_details):
    for cluster_num, cluster_detail in cluster_details.items():
        print('####### Cluster {0}'.format(cluster_num))
        print('Top features:', cluster_detail['top_features'])
        print('=============================')
clust_centers = km_cluster.cluster_centers_
feature_names = tfidf_vect.get_feature_names_out()
cluster_details = get_cluster_details(cluster_model=km_cluster, 
cluster_data=clust_df,
                    feature_names=feature_names, 
                    clusters_num=4, 
                    top_n_features=10 )
print_cluster_details(cluster_details)





# 은,는,이 조사가 날리기 !


# 방법1
test_text=clust_df.iloc[0,0]
result=okt.pos(test_text, stem=True)

filtered_words=[]
for word, type_ in result:
    if type_ !='Josa':
        filtered_words.append(word)
print(filtered_words)


# 방법2
filtered_words=[]
for word, type_ in result:
    if (type_ =='Noun' or 'Verb'):
        filtered_words.append(word)
print(filtered_words)


# 조사 지우는 함수
def ok_not_josa(text):
    filtered_words=[]
    for word, type_ in result:
        if type_ !='Josa':
            filtered_words.append(word)
   return filtered_words              # 문자로 받아서( text), 리스트(filtered_words)롷 반환


lb_enc = LabelEncoder()
lb_enc.fit(df_samples['category'])
df_samples['category']=lb_enc.transform(df_samples['category'])
df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣] +'))
                                                    


from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(max_df = 0.85,
                            min_df = 2, 
                            ngram_range = (1, 2),
                            tokenizer = okt_tokenizer,
                            stop_words = stopwords)
df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])
df_tfidf_vect





















