








import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')


df=pd.read_csv('https://raw.githubusercontent.com/kim-juwan/data_folder/main/data/flight_price.csv')
df.head()


df.info()








# 여기에 코드를 작성하세요.
df.isna().sum()








# 여기에 코드를 작성하세요.
sns.histplot(data=df, x='price', bins=10)








# 여기에 코드를 작성하세요.
sns.barplot(data=df, x='class', y='price')








df['class'].value_counts()


from scipy import stats

V_1=df[df['class'] =='Economy']['price']
V_2=df[df['class'] =='Business']['price']
sns.histplot(V_1, kde=True)
sns.histplot(V_2, kde=True)


print(stats.ttest_1samp(V_1, df['price'].mean()))
print(stats.ttest_1samp(V_2, df['price'].mean()))

# p-value가 0.05보다 작으므로 변수 V_1, V_2는 모평균을 따르지 않는다.








# 여기에 코드를 작성하세요
sns.jointplot(data=df, x='duration', y='price')








# 여기에 코드를 작성하세요.
print(stats.pearsonr(df['duration'], df['price']))
print(stats.spearmanr(df['duration'], df['price']))
print(stats.kendalltau(df['duration'], df['price']))

## 모든 검정에서 pvalue=0.0 이므로 비행시간과 항공권가격은 상관계가 있다. 





# 여기에 코드를 작성하세요.
print(df['flight'].unique())
print(df['flight'].value_counts())





# 여기에 코드를 작성하세요.
df_del=df.drop(['flight' , 'arrival_time'], axis=1)
df_del.columns


df.info()





# 여기에 코드를 작성하세요.
df_preset=pd.get_dummies(df_del)
print(df_preset.shape)
df_preset.info()


df_preset.head()





from sklearn.model_selection import train_test_split
X=df_preset.drop('price', axis=1)
y=df_preset['price']
X_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.25, random_state=42)
print(X_train.shape, X_test.shape,y_train.shape, y_test.shape)





from sklearn.linear_model import LinearRegression
lr_reg=LinearRegression()
lr_reg.fit(X_train,y_train)





from sklearn.metrics import mean_squared_error
y_pred=lr_reg.predict(X_test)
lr_mse=mean_squared_error(y_test,y_pred)
lr_mse


lr_rmse=np.sqrt(lr_mse)
lr_rmse


## 실제값과 에측값의 차이
def get_top_error_data(y_test, pred, n_top=10):
    result_df=pd.DataFrame(y_test.values, columns=['real_count'])
    result_df['pred_count']=np.round(pred)
    result_df['diff']=np.abs(result_df['real_count'] - result_df['pred_count'])
    print(result_df.sort_values('diff', ascending=False)[:n_top])
get_top_error_data(y_test, y_pred, n_top=10)





from sklearn.ensemble import GradientBoostingRegressor
gbm=GradientBoostingRegressor(max_depth=3, min_samples_split=5, learning_rate=0.01, random_state=0)
gbm.fit(X_train, y_train)





y_pred=gbm.predict(X_test)
gbm_mse=mean_squared_error(y_test,y_pred)
gbm_mse


gbm_rmse=np.sqrt(gbm_mse)
np.round(gbm_rmse,2)


## 실제값과 에측값의 차이
def get_top_error_data(y_test, pred, n_top=10):
    result_df=pd.DataFrame(y_test.values, columns=['real_count'])
    result_df['pred_count']=np.round(pred)
    result_df['diff']=np.abs(result_df['real_count'] - result_df['pred_count'])
    print(result_df.sort_values('diff', ascending=False)[:n_top])
get_top_error_data(y_test, y_pred, n_top=10)








df.info()


df.describe()


df.corr(numeric_only=True)


# price 이상치 제거 (약 150 row)
IQR = df['price'].quantile(0.75) - df['price'].quantile(0.25)
high_border = df['price'].quantile(0.75) + IQR * 1.5
low_border = df['price'].quantile(0.25) - IQR * 1.5
print(high_border, low_border)


df_2=df[(df['price'] < high_border) & (df['price'] > low_border)]
df_2.describe()


# 인코딩
df_2_enc= pd.get_dummies(df_2)


df_2_enc


# 스케일링
X=df_2_enc.drop('price', axis=1)
y=df_2_enc['price']
X_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.25, random_state=42)
print(X_train.shape, X_test.shape,y_train.shape, y_test.shape)


from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)


def get_model_predict(model, X_train, X_test,  y_train, y_test):
    model.fit(X_train,y_train)
    pred=model.predict(X_test)
    model_mse=mean_squared_error(y_test,pred)
    print('RMSE값', np.sqrt(model_mse))


from sklearn.linear_model import Ridge
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor

#rf_reg=RandomForestRegressor(random_state=0, n_estimators=100)
lr_reg= LinearRegression()
rdge=Ridge(alpha=10)
#gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)
xgb_reg = XGBRegressor(random_state=0, n_estimators=1000)
models = [lr_reg, rdge, gb_reg]
for model in models: 
    get_model_predict(model, X_train, X_test,  y_train, y_test)


# 1번 LinearRegression 2번 Ridge 3번 XGB
# 1번 선형회귀는 버림
# 2번 Ridge는 전처리(스케일링, 이상치제거) 전 데이터로 찾은 적합한 alpha로 실습함 -- 오차 줄음
# 3번 


# 적합한 Ridge의 alpha 값 찾기
from sklearn.model_selection import cross_val_score
alphas=[0,0.1,1,10,100]
for alpha in alphas:
    ridge=Ridge(alpha=alpha)
    neg_mse_scores=cross_val_score(ridge, X, y, scoring='neg_mean_squared_error',cv=5)
    avg_rmse= np.mean(np.sqrt(-1*neg_mse_scores))
    print(f'alpha {alpha} 일 때 5fold의 평균 RMSE: {avg_rmse:.3f}')


# 한가지 더 해봄 - RandomForest

rf_reg=RandomForestRegressor(random_state=0, n_estimators=100)
neg_mse_scores=cross_val_score(rf_reg,X,y, scoring='neg_mean_squared_error',cv=5)
rmse_scores=np.sqrt(-1 * neg_mse_scores)
avg_rmse= np.mean(rmse_scores)

print('5fold NMSE scores: ', np.round(neg_mse_scores, 2))
print('5fold  RMSE scores: ', np.round(rmse_scores, 2))
print(f'5fold 평균 RMSE: {avg_rmse:3f}')






