import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score


import nltk
import re
from nltk import sent_tokenize, word_tokenize
from konlpy.tag import Okt, Hannanum, Kkma, Komoran
from nltk.stem import LancasterStemmer
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation


# 유사도 분석(코사인): 논문표절 , 추천시스템


def cos_similarity(v1, v2):
    dot_product = np.dot(v1, v2)
    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))
    similarity = dot_product / l2_norm 
    return similarity


doc_list = ['if you take the blue pill, the story ends' ,
            'if you take the red pill, you stay in Wonderland',
            'if you take the red pill']
tfidf_vect_simple = TfidfVectorizer(ngram_range=(1,2))        # ngram_range=(1,2)일때 유사도 더 올라가는지 볼것
feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)  
print(feature_vect_simple.shape)


feature_vect_array = feature_vect_simple.toarray()
vect1 = np.array(feature_vect_array[0]).reshape(-1,)
vect2 = np.array(feature_vect_array[1]).reshape(-1,)
similarity_simple = cos_similarity(vect1, vect2 )
print('문장1,문장2 코사인유사도:{0:.3f}'.format(similarity_simple))


vect1 = np.array(feature_vect_array[0]).reshape(-1,)
vect3 = np.array(feature_vect_array[2]).reshape(-1,)
similarity_simple = cos_similarity(vect1, vect3 )
print('문장1,문장3 코사인유사도:{0:.3f}'.format(similarity_simple))
vect2 = np.array(feature_vect_array[1]).reshape(-1,)
vect3 = np.array(feature_vect_array[2]).reshape(-1,)
similarity_simple = cos_similarity(vect2, vect3 )
print('문장2,문장3 코사인유사도:{0:.3f}'.format(similarity_simple))





from sklearn.metrics.pairwise import cosine_similarity
similarity_simple_pair = cosine_similarity(feature_vect_simple)  # 한개 데이터만 넣어도 작동
print(similarity_simple_pair)
print('shape:',similarity_simple_pair.shape)


pd.DataFrame(feature_vect_simple.toarray(), columns=tfidf_vect_simple.get_feature_names_out())


## 유사도_실습


df=pd.read_csv('./data/petition.csv')


import re
def cleaning(text):
    p = re.compile('[^ ㄱ-ㅣ가-힣]+')
    result = p.sub('',text)
    return result


df_cat=df[df['category']=='육아/교육']


df_cat['content']= df_cat['content'].apply(cleaning)


from konlpy.tag import Okt
okt = Okt()
def okt_tokenizer(text):
    tokens_ko = okt.morphs(text, stem = True)
    return tokens_ko

with open('./data/stopword.txt','r',encoding='utf-8') as f:  # okt에 적합함
    word = f.read()
    stopwords = word.split('\n')


tfidf_vect = TfidfVectorizer(max_df = 0.85, min_df = 2, tokenizer=okt_tokenizer, stop_words=stopwords, max_features=1000)       
feature_vect = tfidf_vect.fit_transform(df_cat['content'])  
print(feature_vect.shape)


similarity_pair = cosine_similarity(feature_vect)  
print(similarity_pair)
print('shape:',similarity_pair.shape)


df=pd.DataFrame(feature_vect.toarray(), columns=tfidf_vect.get_feature_names_out())


similarity_pair=cosine_similarity(feature_vect)





## 텍스트분석 실습 1. 영화 리뷰


df=pd.read_csv('./data/네이버 영화 리뷰.csv')
df['content'].head(30)


# 문서분류 (몇 점- 분류, 회귀)
# 감성분류 (나누기: 긍정 10~7점 중립 6~4 부정 3~1 ), 감성어휘사전 활용?
# 문서군집화(kMean, meanshift, dbscan. 가우시안) + 토픽 모델링(lda)
# 유사도 분석(cosine)


# 문서분류 (몇 점- 분류, 회귀)
#1. 클렌징/토큰/불용어/어근추출


df['content']=df['content'].apply(cleaning)


okt.pos(df['content'].iloc[16], stem=True)


okt = Okt()
def okt_tokenizer(text):
    words=okt.pos(text, stem=True)
    filtered_words=[]
    for word, pos in words:
        if pos not in ['Josa', 'KoreanParticle']:
            filtered_words.append(word)
    return filtered_words


with open('./data/stopword.txt','r',encoding='utf-8') as f:
    word = f.read()
    stopwords = word.split('\n')


cnt_vect=CountVectorizer(max_df=0.9, min_df=2, tokenizer=okt_tokenizer, stop_words=stopwords, max_features=5000)
cnt_vect.fit(df['content'])
X=cnt_vect.transform(df['content'])
X


result_df=pd.DataFrame(X.toarray(), columns=cnt_vect.get_feature_names_out())
result_df


y= df['score']
y.value_counts()


X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=0 ) 


from sklearn.ensemble import RandomForestClassifier
lr_clf=LogisticRegression()
lr_clf.fit(X_train, y_train)

pred=lr_clf.predict(X_test)
accuracy_score(y_test, pred)


## 불균형 데이터이므로 10점으로 찍음 ㅋ 10점
test_text ='영화를 보다가 잠들었어요'
pred=cnt_vect.transform([test_text])
rf_clf.predict(pred)


#


tfidf_vct=TfidfVectorizer(tokenizer=okt_tokenizer, stop_words=stopwords, max_features=1000)
tfidf_vct.fit(X_train)
X_train_tf=tfidf_vct.transform(X_train)
X_test_tf=tfidf_vct.transform(X_test)


tfidf_vect=TfidfVectorizer(max_df=0.9, min_df=2, tokenizer=okt_tokenizer, stop_words=stopwords, max_features=5000)
tfidf_vect.fit(df['content'])
X_tf=cnt_vect.transform(df['content'])
X_train, X_test, y_train, y_test=train_test_split(X_tf,y, test_size=0.2, random_state=0 ) 


print(X_train.shape, X_test.shape,y_train.shape, y_test.shape)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score
lr_reg=LogisticRegression(solver='liblinear')
lr_reg.fit(X_train, y_train)
pred=lr_reg.predict(X_test)
print('예측 정확도 : ', np.sqrt(mean_squared_error(y_test, pred)))
print(r2_score(y_test, pred))


from sklearn.ensemble import RandomForestClassifier  #**
rf_clf=RandomForestClassifier()
lr_clf.fit(X_train_tf, y_train)
pred=rf_clf.predict(X_test_tf)
accuracy_score(y_test, pred)


lr_clf=LogisticRegression()
lr_clf.fit(X_train, y_train)

pred=lr_clf.predict(X_test)
accuracy_score(y_test, pred)


X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=0 ) 


from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
poly=PolynomialFeatures(degree=2)

X_train_poly=poly.fit_transform(X_train)
X_test_poly= poly.transform(X_test)

lr_reg=Ridge(alpha=1)
lr_reg.fit(X_train_poly, y_train)

pred=lr_reg.predict(X_test_poly)
print('예측 정확도 : ', np.sqrt(mean_squared_error(y_test, pred)))


# 감성분석(긍정, 중립, 부정)
good_df=df[df['score'] == 10].sample(frac=0.35)  #10점 많아 샘플링
bad_df=df[df['score'] <= 5]


good_df['score'] =1
bad_df['score'] = 0
df=pd.concat([good_df,bad_df], axis=0)


X=df['content']
y=df['score']
cnt_vect=CountVectorizer(max_df=0.9,tokenizer=okt_tokenizer, stop_words=stopwords, max_features=1000)
X_cnt=cnt_vect.fit_transform(X)

X_train, X_test, y_train, y_test=train_test_split(X_cnt,y, test_size=0.2, random_state=0 ) 


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score
lr_reg=LogisticRegression(solver='liblinear')
lr_reg.fit(X_train, y_train)
pred=lr_reg.predict(X_test)
print('예측 정확도 : ', np.sqrt(mean_squared_error(y_test, pred)))
print(r2_score(y_test, pred))





##내가 다시 해보자


#0.
df_review=pd.read_csv('./data/네이버 영화 리뷰.csv')
df_review['content']


#1.
import re
df['content'] = df['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))

from konlpy.tag import Okt
okt = Okt()
def okt_tokenizer(text):   # 자르기 + 조사 삭제
    words=okt.pos(text, stem=True)
    filtered_words=[]
    for word, pos in words:
        if pos not in ['Josa']:
            filtered_words.append(word)
    return filtered_words

with open('data/stopword.txt','r',encoding='utf-8') as f:
    word = f.read()
    stopwords = word.split('\n')


#2. 벡터화 count
X=df['content']
y=df['score']
cnt_vect=CountVectorizer(tokenizer=okt_tokenizer, stop_words=stopwords, max_features=1000)
cnt_vect.fit(X)
X_cnt=cnt_vect.transform(X)


X_train, X_test, y_train, y_test=train_test_split(X_cnt,y, test_size=0.2, random_state=0 ) 


#3.
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score

lr_reg=LogisticRegression(solver='liblinear')
lr_reg.fit(X_train, y_train)
pred=lr_reg.predict(X_test)
print('예측 정확도 : ', np.sqrt(mean_squared_error(y_test, pred)))
print(r2_score(y_test, pred))









