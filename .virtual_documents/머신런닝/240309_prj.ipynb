import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score


#한글 폰트 적용하기/ 보통  맑은 고딕체 사용
from matplotlib import font_manager, rc
font = 'C:/Windows/Fonts/Malgun.ttf'
font_name = font_manager.FontProperties(fname=font).get_name()
rc('font', family=font_name)


## 충북,충남,대전,세종 사망자 교통사고 (2012-2014)


df=pd.read_csv('오재선_주제제안_3/2012_2014_교통사망사고정보.csv',encoding='cp949')


df['발생지시도'].value_counts()


df=df[df['발생지시도'].isin(['세종','대전','충북','충남'])]


df=df.drop(['당사자종별_1당_대분류', '당사자종별_1당', '당사자종별_2당_대분류',
       '당사자종별_2당', '발생위치X_UTMK', '발생위치Y_UTMK', '경도', '위도','발생년','발생분','도로형태_대분류','사고유형_대분류','사고유형_중분류'], axis=1)


df['사상자수'] = df['사망자수'] + df['중상자수']+ df['사상자수'] + df['경상자수'] + df['부상신고자수']


df.drop(['사망자수','중상자수','경상자수','부상신고자수'], axis=1, inplace=True)
df


df['발생년월일시']=pd.to_datetime(df['발생년월일시'])
df['발생년월일시']


clm_12=pd.read_csv('오재선_주제제안_3/기후_충청권/2012_편집_OBS_ASOS_TIM_20240309201122.csv',encoding='cp949')
clm_13=pd.read_csv('오재선_주제제안_3/기후_충청권/2013_편집_OBS_ASOS_TIM_20240310114858.csv',encoding='cp949')
clm_14=pd.read_csv('오재선_주제제안_3/기후_충청권/2014_편집_OBS_ASOS_TIM_20240309200737.csv',encoding='cp949')


clm_all=pd.concat([clm_12, clm_13, clm_14],axis=0, ignore_index=True )


clm_all['일시']=pd.to_datetime(clm_all['일시'])
clm_all.columns = ['지점명', '일시', '기온','강수량','풍속','적설','시정','지면온도']


df['일시']=df['발생년월일시']
df=df.drop(['발생년월일시'],axis=1)


df['일시']=pd.to_datetime(df['일시'])


print(df.shape, clm_all.shape)


# merged_df = pd.concat([df, clm_all]).drop_duplicates().sort_values(by='일시')
# merged_df.info()





df_merge=pd.merge(df, clm_all, on='일시', how='left')
df_merge.shape


# 충청권으로 할 경우 구분해서 지정해줘야함!!!!
print(df['발생지시군구'].value_counts())
print(clm_all['지점명'].value_counts())


df_merge.duplicated(df).sum()


df_merge=df_merge.drop_duplicates(df) 
df_merge.shape


df_merge['사고유형'].value_counts()


df_merge['사고유형']=df_merge['사고유형'].replace(to_replace=['측면직각충돌','공작물충돌','정면충돌','주/정차차량 충돌'], value='충돌')
df_merge['사고유형']=df_merge['사고유형'].replace(to_replace=['진행중 추돌','주정차중 추돌'], value='추돌')
df_merge['사고유형']=df_merge['사고유형'].replace(to_replace=['횡단중','차도통행중','보도통행중','길가장자리구역통행중'], value='보행')
df_merge['사고유형']=df_merge['사고유형'].replace(to_replace=['도로이탈 추락','도로이탈 기타'], value='도로이탈')

df_merge['사고유형'].value_counts()


# 범주 구분해야?
df_merge['법규위반'].value_counts()


df_merge.drop('법규위반_대분류', axis=1, inplace=True)


df_merge['도로형태'].value_counts()


df_merge['도로형태']=df_merge['도로형태'].replace(to_replace=['횡단보도상','횡단보도부근'], value='횡단보도')
df_merge['도로형태']=df_merge['도로형태'].replace(to_replace=['교량위','터널안','고가도로위','지하도로내'], value='교량_터널_고가지하도로')


df_merge.isna().sum()


df_fill=df_merge.fillna({'강수량': 0, '적설': 0})


df_merge[df_merge['지점명'].isna()==True]


df_fill.isna().sum()


df_fill['시정'].value_counts()


df_fill.drop('시정', axis=1, inplace=True)


df_fill.isna().sum()


df_fill.info()


### 이제부턴 머신러닝 본격 전처리


## 발생지 기준을 뭘로 할지 결정/ 일시를 봄,여름,가을로 나눌것/ 독립변수 좀더 고민필요
df_ml=df_fill.drop(['발생지시군구','지점명','일시', '법규위반','사고유형'],axis=1)
df_ml


df_dum=pd.get_dummies(df_ml)
df_dum.shape


X=df_dum.drop('사상자수', axis=1)
y=df_dum['사상자수']
print(X.shape, y.shape)


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)
print(X_train.shape, y_train.shape, X_train.shape)


def get_model_cv_prediction(model, X, y):
    neg_mse_scores = cross_val_score(model, X, y,scoring="neg_mean_squared_error", cv = 5)
    rmse_scores = np.sqrt(-1 * neg_mse_scores)
    avg_rmse = np.mean(rmse_scores)
    print(' ',model.__class__.__name__ , ' ')
    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))


from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor
rf_reg=RandomForestRegressor(random_state=0, n_estimators=100)
sgd_reg=SGDRegressor(alpha=0.1)
dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)
gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)
xgb_reg = XGBRegressor(random_state=0, n_estimators=1000)
models = [rf_reg,sgd_reg,dt_reg, gb_reg, xgb_reg]
for model in models: 
    get_model_cv_prediction(model, X, y)


alphas=[0, 0.01, 0.1, 1, 10]
for alpha in alphas:
    sgd_reg=SGDRegressor(alpha=alpha)
    neg_mse_scores=cross_val_score(sgd_reg,X,y, scoring='neg_mean_squared_error',cv=5)
    avg_rmse= np.mean(np.sqrt(-1*neg_mse_scores))
    print(f'alpha {alpha} 일 때 5fold의 평균 RMSE: {avg_rmse:.3f}')





from sklearn.ensemble import RandomForestRegressor

rf_reg=RandomForestRegressor(random_state=0, n_estimators=100)
rf_reg.fit(X, y)
ftr_importances_values = rf_reg.feature_importances_
ftr_importances = pd.Series(ftr_importances_values,index=X.columns)
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]
plt.figure(figsize=(8,6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20 , y = ftr_top20.index)
plt.show()


df_dum['사상자수'].sort_values()


df_dum['사상자수'].value_counts()



