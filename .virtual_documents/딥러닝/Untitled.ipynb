import tensorflow as tf


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')


plt.rcParams['figure.figsize']=(7,4)  # figure 사이즈 변경
sns.set_palette('twilight')
pal_1=sns.color_palette('coolwarm', 10)
pal_2=sns.color_palette('deep',10)

from matplotlib import font_manager, rc
font = 'C:/Windows/Fonts/malgun.ttf'
font_name = font_manager.FontProperties(fname=font).get_name()
rc('font', family=font_name)


## 다층 신경망  : 
class DualLayer:     # 은닉층 1개
    def __init__(self, units = 8):  # 클래스 만들때 모든 은닉층, 가중치 등 정해 틀 만들고 시작
        self.units = units # 은닉층의 뉴런 개수
        self.w1 = None    # 입력 > 은닉 가중치
        self.b1 = None    # 입력 > 은닉 절편
        self.w2 = None    # 은닉 > 출력 가중치
        self.b2 = None    # 은닉 > 출력 절편
        self.a1 = None    # 은닉층의 활성화 출력
        self.losses = []
    def forpass(self, x):
        z1 = np.dot(x, self.w1) + self.b1
        self.a1 = self.activation(z1)            # 상속 받아 활성함수는 시그모이드
        z2 = np.dot(self.a1, self.w2) + self.b2  # np.dot 행렬
        return z2
    def backprop(self, x, err):
        m = len(x)
        # 은닉층 > 출력층 가중치, 절편 업데이트
        w2_grad = np.dot(self.a1.T, err) / m    # 평균
        b2_grad = np.sum(err) / m               # 평균
        # 은닉층 오차
        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)
        # 입력층 > 은닉층 가중치, 절편 업데이트        
        w1_grad = np.dot(x.T, err_to_hidden) / m
        b1_grad = np.sum(err_to_hidden, axis=0) / m
        return w1_grad, b1_grad, w2_grad, b2_grad
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a
    def predict(self, x):     # y 결과 값
        z = self.forpass(x)
        return z > 0
    def score(self, x, y):
        return np.mean(self.predict(x) == y.reshape(-1, 1))  
  
    def init_weights(self, n_features):        # 가중치 초기화  
        self.w1 = np.ones((n_features, self.units))  # 2차원으로 만들어야 행렬곱 가능
        self.b1 = np.zeros(self.units)
        self.w2 = np.ones((self.units, 1))
        self.b2 = 0
    def training(self, x, y, m): ## 마지막 활성함수므로 분류할지 회귀할지 정하고 코드수정할것          
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad 
        self.b1 -= b1_grad
        self.w2 -= w2_grad 
        self.b2 -= b2_grad
        return a
    def fit(self, x, y, epochs = 100):  # 
        y = y.reshape(-1,1)
        m = len(x)
        self.init_weights(x.shape[1])
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)


from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.preprocessing import StandardScaler

cancer = load_breast_cancer()
X = cancer.data
y = cancer.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)






class Learning(DualLayer):   # 클래스 DualLayer 상속
    def __init__(self, units = 10, learning_rate = 0.1):
        self.units = units 
        self.w1 = None 
        self.b1 = None 
        self.w2 = None 
        self.b2 = None 
        self.a1 = None 
        self.lr = learning_rate
        self.w_history = []
        self.losses = []
            
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= self.lr * w1_grad     # 학습률을 가중치(w)에 계산
        self.b1 -= b1_grad
        self.w2 -= self.lr * w2_grad
        self.b2 -= b2_grad
        return a
    
    def fit(self, x, y, epochs = 100):
        y = y.reshape(-1,1)
        m = len(x)
        self.init_weights(x.shape[1])
        self.w_history.append([self.w1.copy(), self.w2.copy()])
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)
            self.w_history.append([self.w1.copy(), self.w2.copy()])



learning = Learning(learning_rate=0.1)                # 학습율과
learning.fit(X_train_scaled, y_train, epochs = 10000) # 에폭스값에 따라 결과 달라짐 
print(learning.score(X_test_scaled, y_test))
w1 = []
for w in learning.w_history:
    w1.append(w[0])
for i,w in enumerate(w1):
    plt.plot(i,w[0,0],'o')      ## 첫번째 w값의 첫번째 깂...첫.


# 손실/ 오차률
learning = Learning(learning_rate=0.1)
learning.fit(X_train_scaled, y_train, epochs = 1000)  

plt.plot(learning.losses)
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
print(learning.losses[-1])


learning.w_history      # 모든 가중치 w값 저장됨


learning.w_history[0][0][0,0]    #  첫번째 w의 첫뻔쩨...





class Validation(DualLayer):
    def __init__(self, units = 10):
        self.units = units 
        self.w1 = None 
        self.b1 = None 
        self.w2 = None 
        self.b2 = None 
        self.a1 = None 
        self.losses = []
        self.val_losses = []
    
    def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
        y = y.reshape(-1,1)
        y_val = y_val.reshape(-1, 1)
        m = len(x)
        self.init_weights(x.shape[1])
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)
            self.update_val_loss(x_val, y_val)
        
    def update_val_loss(self, x_val, y_val):
        if x_val is None:
            return
        val_loss = 0
        z = self.forpass(x_val)
        a = self.activation(z)
        a = np.clip(a, 1e-10, 1-1e-10)
        val_loss = np.sum(-(y_val * np.log(a) + (1 - y_val) * np.log(1 - a)))
        self.val_losses.append(val_loss / len(x_val))


from sklearn.model_selection import train_test_split
X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_train, 
test_size = 0.2, random_state = 0)

validation = Validation()
validation.fit(X_tr, y_tr, epochs = 1000, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))


import matplotlib.pyplot as plt
plt.ylim(0, 0.6)
plt.plot(learning.losses, 'g', label='dual_loss')
plt.plot(validation.losses, 'r', label='train_loss')
plt.plot(validation.val_losses, 'b', label='val_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend()
plt.show()


# 에폭스 횟수에 따른 정확도 차이 
validation = Validation()
validation.fit(X_tr, y_tr, epochs = 500, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))

validation = Validation()
validation.fit(X_tr, y_tr, epochs = 600, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))

validation = Validation()
validation.fit(X_tr, y_tr, epochs = 700, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))

validation = Validation()   # 정확도는 더 높으나 과적합으로 볼 수 있음
validation.fit(X_tr, y_tr, epochs = 1000, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))








class Minibatch(DualLayer):
    def __init__(self, units = 10, batch_size = 32):
        super().__init__(units)
        self.batch_size = batch_size
    
    def fit(self, x, y, epochs = 100):
        self.init_weights(x.shape[1])
        for i in range(epochs):
            loss = 0
            cnt=0
            for x_batch, y_batch in self.gen_batch(x, y):
                cnt += 1
                y_batch = y_batch.reshape(-1, 1)
                m = len(x_batch)
                a = self.training(x_batch, y_batch, m)
                a = np.clip(a, 1e-10, 1-1e-10)
                loss += np.sum(-(y_batch * np.log(a) + (1 - y_batch) * np.log(1 - a)))
            self.losses.append(loss / len(x))
 #           print('### epochs당 학습횟수', cnt)
    
    def gen_batch(self, x, y):
        length = len(x)
        bins = length // self.batch_size
        if length % self.batch_size:
            bins += 1
        indexes = np.random.permutation(np.arange(len(x)))
        x = x[indexes]
        y = y[indexes]
        for i in range(bins):
            start = self.batch_size * i
            end = self.batch_size * (i + 1)
            yield x[start:end], y[start:end]


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 1000)
print(dual_layer.score(X_test_scaled, y_test))

minibatch = Minibatch(batch_size=5)
minibatch.fit(X_train_scaled, y_train, epochs = 1000)   # 
print(minibatch.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'r', label = 'dual')
plt.plot(minibatch.losses, 'b', label = 'minibatch')
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()





class Dropout(DualLayer):
    def __init__(self, units = 10, dropout = 0.5):
        super().__init__(units)
        self.dropout = dropout
    def drop(self, z):
        if self.dropout == 1:
            return np.zeros_like(z)
        mask = np.random.uniform(0, 1, z.shape) > self.dropout
        return (mask * z) / (1.0 - self.dropout)
    
    def training(self, x, y, m):
        z = self.forpass(x)
        z = self.drop(z) # 드롭 아웃 적용
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad
        self.b1 -= b1_grad
        self.w2 -= w2_grad
        self.b2 -= b2_grad
        return a


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 1000)
print(dual_layer.score(X_test_scaled, y_test))

dropout = Dropout()
dropout.fit(X_train_scaled, y_train, epochs = 1000)
print(dropout.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'b', label = 'dual')
plt.plot(dropout.losses, 'r', label = 'dropout')
plt.ylim(0,1.5)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()















