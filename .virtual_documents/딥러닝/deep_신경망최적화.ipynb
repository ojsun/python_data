


import tensorflow as tf


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')


plt.rcParams['figure.figsize']=(7,4)  # figure 사이즈 변경
sns.set_palette('twilight')
pal_1=sns.color_palette('coolwarm', 10)
pal_2=sns.color_palette('deep',10)

from matplotlib import font_manager, rc
font = 'C:/Windows/Fonts/malgun.ttf'
font_name = font_manager.FontProperties(fname=font).get_name()
rc('font', family=font_name)











## 다층 신경망  : 가중치 초기화 문제 
class DualLayer:     # 은닉층 1개
    def __init__(self, units = 8):  # 클래스 만들때 모든 은닉층, 가중치 등 정해 틀 만들고 시작
        self.units = units # 은닉층의 뉴런 개수
        self.w1 = None    # 입력 > 은닉 가중치
        self.b1 = None    # 입력 > 은닉 절편
        self.w2 = None    # 은닉 > 출력 가중치
        self.b2 = None    # 은닉 > 출력 절편
        self.a1 = None    # 은닉층의 활성화 출력
        self.losses = []
    def forpass(self, x):
        z1 = np.dot(x, self.w1) + self.b1
        self.a1 = self.activation(z1)            # 상속 받아 활성함수는 시그모이드
        z2 = np.dot(self.a1, self.w2) + self.b2  # np.dot 행렬
        return z2
    def backprop(self, x, err):
        m = len(x)
        # 은닉층 > 출력층 가중치, 절편 업데이트
        w2_grad = np.dot(self.a1.T, err) / m    # 평균
        b2_grad = np.sum(err) / m               # 평균
        # 은닉층 오차
        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)
        # 입력층 > 은닉층 가중치, 절편 업데이트        
        w1_grad = np.dot(x.T, err_to_hidden) / m
        b1_grad = np.sum(err_to_hidden, axis=0) / m
        return w1_grad, b1_grad, w2_grad, b2_grad
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a
    def predict(self, x):     # y 결과 값
        z = self.forpass(x)
        return z > 0
    def score(self, x, y):
        return np.mean(self.predict(x) == y.reshape(-1, 1))  
  
    def init_weights(self, n_features):        # 가중치 초기화  
        self.w1 = np.ones((n_features, self.units))  # 2차원으로 만들어야 행렬곱 가능
        self.b1 = np.zeros(self.units)
        self.w2 = np.ones((self.units, 1))
        self.b2 = 0
    def training(self, x, y, m): ## 마지막 활성함수므로 분류할지 회귀할지 정하고 코드수정할것          
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad 
        self.b1 -= b1_grad
        self.w2 -= w2_grad 
        self.b2 -= b2_grad
        return a
    def fit(self, x, y, epochs = 100):  # 
        y = y.reshape(-1,1)
        m = len(x)
        self.init_weights(x.shape[1])
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)


# 가우시안 정규 분포를 따르는 랜덤한 수로 가중치 초기화

class RandomInitialization(DualLayer):
    def init_weights(self, n_features):
        np.random.seed(0)
        self.w1 = np.random.normal(0, 1, (n_features, self.units))
        self.b1 = np.zeros(self.units)
        self.w2 = np.random.normal(0, 1, (self.units, 1))
        self.b2 = 0


# Xavier 가중치 초기화

class XavierInitialization(DualLayer):
    def init_weights(self, n_features):
        self.w1 = np.random.normal(0, 2 / np.sqrt(n_features + self.units), (n_features, self.units))
        self.b1 = np.zeros(self.units)
        self.w2 = np.random.normal(0, 2 / np.sqrt(n_features + self.units), (self.units, 1))
        self.b2 = 0


## 가중치 초기화 비교(기본, 랜덤, 싸비어)
random_init = RandomInitialization()
random_init.fit(X_train_scaled, y_train, epochs = 1000)

xavier_init = XavierInitialization()
xavier_init.fit(X_train_scaled, y_train, epochs = 1000)

plt.plot(dual_layer.losses, 'b', label = 'Dual')
plt.plot(random_init.losses, 'r', label = 'Random')
plt.plot(xavier_init.losses, 'g', label = 'Xavier')
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()





from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.preprocessing import StandardScaler

cancer = load_breast_cancer()
X = cancer.data
y = cancer.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)






class L1_regular(DualLayer): 
    def __init__(self, units = 10, l1 = 0):
        self.units = units 
        self.w1 = None 
        self.b1 = None 
        self.w2 = None 
        self.b2 = None 
        self.a1 = None 
        self.l1 = l1
        self.losses = []
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad + self.l1 * np.sign(self.w1) 
        self.b1 -= b1_grad
        self.w2 -= w2_grad + self.l1 * np.sign(self.w2)
        self.b2 -= b2_grad 
        return a


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 1000)
print(dual_layer.score(X_test_scaled, y_test))

l1 = L1_regular(l1 = 0.01)
l1.fit(X_train_scaled, y_train, epochs = 1000)
print(l1.score(X_test_scaled, y_test))


class L2_regular(DualLayer): 
    def __init__(self, units = 10, l2 = 0):
        self.units = units 
        self.w1 = None 
        self.b1 = None 
        self.w2 = None 
        self.b2 = None 
        self.a1 = None 
        self.l2 = l2
        self.losses = []
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad + self.l2 * self.w1
        self.b1 -= b1_grad
        self.w2 -= w2_grad + self.l2 * self.w2
        self.b2 -= b2_grad 
        return a
    
    def fit(self, x, y, epochs = 100):
        y = y.reshape(-1,1)
        m = len(x)
        self.init_weights(x.shape[1]) 
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a))) + (self.l2 / 2) * (np.sum(self.w1 ** 2) + np.sum(self.w2 ** 2))
            # l1일때
#             loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a))) + (self.l1) * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2)))
            self.losses.append(loss / m)


class Regularization(DualLayer):   # 엘라스틱
    def __init__(self, units = 10, l1 = 0, l2 = 0):
        self.units = units 
        self.w1 = None 
        self.b1 = None 
        self.w2 = None 
        self.b2 = None 
        self.a1 = None 
        self.l1 = l1
        self.l2 = l2
        self.losses = []
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad + self.l1 * np.sign(self.w1) + self.l2 * self.w1
        self.b1 -= b1_grad
        self.w2 -= w2_grad + self.l1 * np.sign(self.w2) + self.l2 * self.w2
        self.b2 -= b2_grad
        return a


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 1000)
print(dual_layer.score(X_test_scaled, y_test))

regular = Regularization(l1 = 0.01, l2 = 0.01)
regular.fit(X_train_scaled, y_train, epochs = 1000)
print(regular.score(X_test_scaled, y_test))

l2 = L2_regular(l2 = 0.01)
l2.fit(X_train_scaled, y_train, epochs = 1000)
print(l2.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'b', label = 'Dual')
plt.plot(l1.losses, 'r', label = 'L1')
plt.plot(l2.losses, 'g', label = 'L2')
plt.plot(regular.losses, 'y', label = 'L1*L2')
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()





class Learning(DualLayer):   # 클래스 DualLayer 상속
    def __init__(self, units = 10, learning_rate = 0.1):
        self.units = units 
        self.w1 = None 
        self.b1 = None 
        self.w2 = None 
        self.b2 = None 
        self.a1 = None 
        self.lr = learning_rate
        self.w_history = []
        self.losses = []
            
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= self.lr * w1_grad     # 학습률을 가중치(w)에 계산
        self.b1 -= b1_grad
        self.w2 -= self.lr * w2_grad
        self.b2 -= b2_grad
        return a
    
    def fit(self, x, y, epochs = 100):
        y = y.reshape(-1,1)
        m = len(x)
        self.init_weights(x.shape[1])
        self.w_history.append([self.w1.copy(), self.w2.copy()])
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)
            self.w_history.append([self.w1.copy(), self.w2.copy()])



learning = Learning(learning_rate=0.1)                # 학습율과
learning.fit(X_train_scaled, y_train, epochs = 10000) # 에폭스값에 따라 결과 달라짐 
print(learning.score(X_test_scaled, y_test))
w1 = []
for w in learning.w_history:
    w1.append(w[0])
for i,w in enumerate(w1):
    plt.plot(i,w[0,0],'o')      ## 첫번째 w값의 첫번째 깂...첫.


# 손실/ 오차률
learning = Learning(learning_rate=0.1)
learning.fit(X_train_scaled, y_train, epochs = 1000)  

plt.plot(learning.losses)
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
print(learning.losses[-1])


learning.w_history      # 모든 가중치 w값 저장됨


learning.w_history[0][0][0,0]    #  첫번째 w의 첫뻔쩨...





class Validation(DualLayer):
    def __init__(self, units = 10):
        self.units = units 
        self.w1 = None 
        self.b1 = None 
        self.w2 = None 
        self.b2 = None 
        self.a1 = None 
        self.losses = []
        self.val_losses = []
    
    def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
        y = y.reshape(-1,1)
        y_val = y_val.reshape(-1, 1)
        m = len(x)
        self.init_weights(x.shape[1])
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)
            self.update_val_loss(x_val, y_val)   # 에포크가 한바뀌 돈 뒤에 1번씩 작동
        
    def update_val_loss(self, x_val, y_val):
        if x_val is None:   # 없으면 함수 작동안함
            return
        val_loss = 0
        z = self.forpass(x_val)
        a = self.activation(z)
        a = np.clip(a, 1e-10, 1-1e-10)
        val_loss = np.sum(-(y_val * np.log(a) + (1 - y_val) * np.log(1 - a)))
        self.val_losses.append(val_loss / len(x_val))    # 학습정도 확인하기 위한  val_losses: 이용 오차 확인


from sklearn.model_selection import train_test_split
X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_train, 
test_size = 0.2, random_state = 0)

validation = Validation()
validation.fit(X_tr, y_tr, epochs = 1000, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))


import matplotlib.pyplot as plt
plt.ylim(0, 0.6)
plt.plot(learning.losses, 'g', label='dual_loss')
plt.plot(validation.losses, 'r', label='train_loss')
plt.plot(validation.val_losses, 'b', label='val_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend()
plt.show()


# 에폭스 횟수에 따른 정확도 차이 : 그래프의 겹쳐진 부분 적합한 횟수 확인 가능
validation = Validation()
validation.fit(X_tr, y_tr, epochs = 500, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))

validation = Validation()
validation.fit(X_tr, y_tr, epochs = 600, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))

validation = Validation()
validation.fit(X_tr, y_tr, epochs = 700, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))

validation = Validation()   # 정확도는 더 높으나 과적합으로 볼 수 있음
validation.fit(X_tr, y_tr, epochs = 1000, x_val=X_val, y_val=y_val)
print(validation.score(X_test_scaled, y_test))








class Minibatch(DualLayer):
    def __init__(self, units = 10, batch_size = 4):
        super().__init__(units)
        self.batch_size = batch_size    # 몇 개씩 묶을지
    
    def fit(self, x, y, epochs = 100):
        self.init_weights(x.shape[1])
        for i in range(epochs):
            loss = 0
            cnt=0
            for x_batch, y_batch in self.gen_batch(x, y):
                cnt += 1
                y_batch = y_batch.reshape(-1, 1)
                m = len(x_batch)
                a = self.training(x_batch, y_batch, m)
                a = np.clip(a, 1e-10, 1-1e-10)
                loss += np.sum(-(y_batch * np.log(a) + (1 - y_batch) * np.log(1 - a)))
            self.losses.append(loss / len(x))   # 에포크가 한바뀌 돈 뒤에 1번씩 작동
 #           print('### epochs당 학습횟수', cnt)
    
    def gen_batch(self, x, y):
        length = len(x)
        bins = length // self.batch_size  
        if length % self.batch_size:
            bins += 1
        indexes = np.random.permutation(np.arange(len(x)))
        x = x[indexes]
        y = y[indexes]
        for i in range(bins):
            start = self.batch_size * i
            end = self.batch_size * (i + 1)
            yield x[start:end], y[start:end]


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 1000)
print(dual_layer.score(X_test_scaled, y_test))

minibatch = Minibatch(batch_size=5)
minibatch.fit(X_train_scaled, y_train, epochs = 1000)   # 
print(minibatch.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'r', label = 'dual')
plt.plot(minibatch.losses, 'b', label = 'minibatch')
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()





class Dropout(DualLayer):
    def __init__(self, units = 10, dropout = 0.5):
        super().__init__(units)
        self.dropout = dropout
    def drop(self, z):
        if self.dropout == 1:
            return np.zeros_like(z)
        mask = np.random.uniform(0, 1, z.shape) > self.dropout   # 핵심코드
        return (mask * z) / (1.0 - self.dropout)
    
    def training(self, x, y, m):
        z = self.forpass(x)
        z = self.drop(z)        # 드롭 아웃 적용
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad
        self.b1 -= b1_grad
        self.w2 -= w2_grad
        self.b2 -= b2_grad
        return a


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 1000)
print(dual_layer.score(X_test_scaled, y_test))

dropout = Dropout()
dropout.fit(X_train_scaled, y_train, epochs = 1000)
print(dropout.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'b', label = 'dual')
plt.plot(dropout.losses, 'r', label = 'dropout')
plt.ylim(0,1.5)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()





class Early_stop(Validation):
    def __init__(self, units = 10, earlystop = 10):
        super().__init__(units)
        self.minloss = None
        self.earlystop = earlystop
    
    def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
        y = y.reshape(-1,1)
        y_val = y_val.reshape(-1, 1)
        m = len(x)
        self.init_weights(x.shape[1])
        for i in range(epochs):        
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)
            self.update_val_loss(x_val, y_val) 
            if i % 10 == 0:
                print(f'### epoch : {i}', end = '\t')
                print(f'val_loss : {self.val_losses[-1]}')
            if i == (self.earlystop - 1):
                self.minloss = min(self.val_losses)  # 최소값
            if i >= self.earlystop:
                if self.minloss < min(self.val_losses[-self.earlystop:]): # 최솟값이 갱신된다면
                    print(f'EarlyStop : Epochs {i}')
                    break
                else:
                    self.minloss = min(self.val_losses)


early_stop = Early_stop(earlystop = 100)
early_stop.fit(X_tr, y_tr, epochs = 1000, x_val=X_val, y_val=y_val)
print(early_stop.score(X_test_scaled, y_test))


plt.plot(early_stop.losses, 'r', label='train_loss')
plt.plot(early_stop.val_losses, 'b', label='val_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.ylim(0.1, 0.6)
plt.legend()
plt.show()








## Momentum : 복잡한 데이터에 적합한 옵티마이저
class Momentum(DualLayer):
    def __init__(self, units = 10, learning_rate = 0.01, momentum = 0.9):
        super().__init__(units)
        self.lr = learning_rate
        self.momentum = momentum
        self.v1 = None
        self.v2 = None
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        if self.v1 is None:                # 초기화
            self.v1 = np.zeros_like(self.w1)
            self.v2 = np.zeros_like(self.w2)
        self.v1 = self.lr * w1_grad + self.momentum * self.v1
        self.v2 = self.lr * w2_grad + self.momentum * self.v2
        self.w1 -= self.v1
        self.b1 -= b1_grad
        self.w2 -= self.v2
        self.b2 -= b2_grad
        return a


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 10000)
print(dual_layer.score(X_test_scaled, y_test))

momentum = Momentum()
momentum.fit(X_train_scaled, y_train, epochs = 10000)
print(momentum.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'r', label = 'dual')
plt.plot(momentum.losses, 'b', label = 'momentum')
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()


## RMSprop(
class RMSprop(DualLayer):
    def __init__(self, units = 10, learning_rate = 0.1, decay_rate = 0.99):
        super().__init__(units)
        self.lr = learning_rate
        self.decay_rate = decay_rate
        self.h1 = None
        self.h2 = None
        self.epsilon = 1e-6
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        if self.h1 is None:
            self.h1 = np.zeros_like(self.w1)
            self.h2 = np.zeros_like(self.w2)
        self.h1 *= self.decay_rate
        self.h2 *= self.decay_rate
        self.h1 += w1_grad * w1_grad * (1-self.decay_rate) 
        self.h2 += w2_grad * w2_grad * (1-self.decay_rate)
        self.w1 -= self.lr * w1_grad / (np.sqrt(self.h1 + self.epsilon))
        self.b1 -= b1_grad
        self.w2 -= self.lr * w2_grad / (np.sqrt(self.h2 + self.epsilon))
        self.b2 -= b2_grad
        return a


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 15000)
print(dual_layer.score(X_test_scaled, y_test))

rmsprop = RMSprop()
rmsprop.fit(X_train_scaled, y_train, epochs = 15000)
print(adagrad.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'r', label = 'dual')
plt.plot(rmsprop.losses, 'b', label = 'rmsprop')
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()


## Adam
class Adam(DualLayer):
    def __init__(self, units = 10, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999):
        super().__init__(units)
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.m1 = None
        self.m2 = None
        self.v1 = None
        self.v2 = None
        self.iter = 0
        self.epsilon = 1e-6
    def training(self, x, y, m):
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        if self.m1 is None:
            self.m1 = np.zeros_like(self.w1)
            self.m2 = np.zeros_like(self.w2)
            self.v1 = np.zeros_like(self.w1)
            self.v2 = np.zeros_like(self.w2)
        self.m1 *= self.beta1
        self.v1 *= self.beta2
        self.m1 += ((1 - self.beta1) * w1_grad) / (1 - self.beta1)
        self.v1 += ((1 - self.beta2) * w1_grad * w1_grad) / (1 - self.beta2)
        self.m2 *= self.beta1
        self.v2 *= self.beta2
        self.m2 += ((1 - self.beta1) * w2_grad) / (1 - self.beta1)
        self.v2 += ((1 - self.beta2) * w2_grad * w2_grad) / (1 - self.beta2)
        self.w1 -= self.lr * self.m1 / (np.sqrt(self.v1) + self.epsilon)
        self.b1 -= b1_grad
        self.w2 -= self.lr * self.m2 / (np.sqrt(self.v2) + self.epsilon)
        self.b2 -= b2_grad
        return a


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs = 10000)
print(dual_layer.score(X_test_scaled, y_test))
adam = Adam()
adam.fit(X_train_scaled, y_train, epochs = 10000)
print(adam.score(X_test_scaled, y_test))


plt.plot(dual_layer.losses, 'r', label = 'dual')
plt.plot(adam.losses, 'b', label = 'Adam')
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()
