import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')


####  신경망 학습 : 벡터 연산, 행렬 연산


from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)


import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
# 스케일링
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)


## 3/22 코드  : 분류
## 단층 퍼셉트론
class SingleLayer:
    def __init__(self):
        self.w = None
        self.b = None
        self.losses = []        
    def forpass(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    def backprop(self, x, err):
        m = len(x)
        w_grad = np.dot(x.T, err) / m
        b_grad = np.sum(err) / m  # np.mean(err)
        return w_grad, b_grad
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a
    def predict(self, x):     # y 결과 값
        z = self.forpass(x)
        return z > 0
    def score(self, x, y):
        return np.mean(self.predict(x) == y.reshape(-1, 1))  
    def fit(self, x, y, epochs = 100, random_state = None):
        y = y.reshape(-1,1) # 열 벡터로 변환
        m = len(x)
        self.w = np.ones((x.shape[1], 1)) # 가중치 초기화
        self.b = 0 # 절편 초기화
        for i in range(epochs):
            z = self.forpass(x)
            a = self.activation(z)
            err = -(y - a)
            w_grad, b_grad = self.backprop(x, err)
            self.w -= w_grad
            self.b -= b_grad
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.mean(-(y*np.log(a) + (1-y)*np.log(1-a))) 
            self.losses.append(loss)


## 다층 신경망
class DualLayer(SingleLayer):     # 클래스 SingleLayer 상속// 은닉층 1개
    def __init__(self, units = 8):  # 클래스 만들때 모든 은닉층, 가중치 등 정해 틀 만들고 시작
        self.units = units # 은닉층의 뉴런 개수
        self.w1 = None    # 입력 > 은닉 가중치
        self.b1 = None    # 입력 > 은닉 절편
        self.w2 = None    # 은닉 > 출력 가중치
        self.b2 = None    # 은닉 > 출력 절편
        self.a1 = None    # 은닉층의 활성화 출력
        self.losses = []
    def forpass(self, x):
        z1 = np.dot(x, self.w1) + self.b1
        self.a1 = self.activation(z1)            # 상속 받아 활성함수는 시그모이드
        z2 = np.dot(self.a1, self.w2) + self.b2  # np.dot 행렬
        return z2
    def backprop(self, x, err):
        m = len(x)
        # 은닉층 > 출력층 가중치, 절편 업데이트
        w2_grad = np.dot(self.a1.T, err) / m    # 평균
        b2_grad = np.sum(err) / m               # 평균
        # 은닉층 오차
        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)
        # 입력층 > 은닉층 가중치, 절편 업데이트        
        w1_grad = np.dot(x.T, err_to_hidden) / m
        b1_grad = np.sum(err_to_hidden, axis=0) / m
        return w1_grad, b1_grad, w2_grad, b2_grad
    def init_weights(self, n_features):        # 가중치 초기화  
        self.w1 = np.ones((n_features, self.units))  # 2차원으로 만들어야 행렬곱 가능
        self.b1 = np.zeros(self.units)
        self.w2 = np.ones((self.units, 1))
        self.b2 = 0
    def training(self, x, y, m): ## 마지막 활성함수므로 분류할지 회귀할지 정하고 코드수정할것          
        z = self.forpass(x)
        a = self.activation(z)
        err = -(y - a)
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        self.w1 -= w1_grad 
        self.b1 -= b1_grad
        self.w2 -= w2_grad 
        self.b2 -= b2_grad
        return a
    def fit(self, x, y, epochs = 100):  # 
        y = y.reshape(-1,1)
        m = len(x)
        self.init_weights(x.shape[1])
        for i in range(epochs):
            a = self.training(x, y, m)
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
            self.losses.append(loss / m)


dual_layer = DualLayer()
dual_layer.fit(X_train_scaled, y_train, epochs=1000)
print(dual_layer.score(X_test_scaled, y_test))

plt.plot(dual_layer.losses)
plt.ylim(0,0.6)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
print(dual_layer.losses[-1])



































#### 신경망 학습: 다층 신경



























