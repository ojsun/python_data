from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
x = cancer.data
y = cancer.target
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)
print(X_train.shape, X_test.shape)


cancer.target_names


cancer.feature_names


x = cancer.data[:, 22]
y = cancer.target


w = 1
b = 1


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x.reshape(-1,1))
x = x_scaled.flatten()


import numpy as np


w = 1
b = 1
for i in range(100):
    for x_i, y_i in zip(x, y):
        z = x_i * w + b
        # 분류 > 활성화함수(시그모이드)
        a = 1 / (1 + np.exp(-z))
        err = y_i - a
        w_rate = x_i
        b_rate = 1
        w = w + w_rate * err
        b = b + b_rate * err
print(w, b)


x.min(), x.max()


import matplotlib.pyplot as plt

plt.scatter(x, y)
xx = np.linspace(-2, 4.5, 100)
z = w * xx + b
a = 1 / (1 + np.exp(-z))
plt.plot(xx, a, color='red')
plt.xlabel('x'); plt.ylabel('y')
plt.ylim(-0.5, 1.5)


pt1 = (-2, -2 * w + b)
pt2 = (4.5, 4.5 * w + b)
plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]], color='green')

plt.show()


# 22, 27번 데이터
x = cancer.data[:, [22, 27]]
y = cancer.target


# 입력데이터 2개, 출력데이터 1개, 은닉층 0개
# N(2) * M(1) = 가중치의 개수 = 2
# M(1) = 절편의 개수 = 1
w1 = 1
w2 = 1
b = 1
# 데이터
x1 = x[0, 0]
x2 = x[0, 1]

# 가중합산
z = x1 * w1 + x2 * w2 + b
a = 1 / (1 + np.exp(-z))

# 오차역전파
w1 = w1 + (y[0] - a) * x1
w2 = w2 + (y[0] - a) * x2
b = b + (y[0] - a)

print(w1, w2, b)


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x = scaler.fit_transform(x)


w1 = 1
w2 = 1
b = 1
for x_i, y_i in zip(x, y):
    x1 = x_i[0]
    x2 = x_i[1]
    
    z = x1 * w1 + x2 * w2 + b
    a = 1 / (1 + np.exp(-z))
    err = (y_i - a)
    
    w1 = w1 + err * x1
    w2 = w2 + err * x2
    b = b + err
print(w1, w2, b)


np.ones(2)


x[0] * np.ones(2)
# [w1x1 , w2x2]


x_i* err
# [x1 * err , x2 * err]


w + x_i* err
# [w1 + x1 * err, w2 + x2 * err]


w = np.ones(2)
b = 1
for x_i, y_i in zip(x, y):
    
    z = np.sum(x_i * w) + b  # w1x1 + w2x2 + b
    a = 1 / (1 + np.exp(-z))
    err = (y_i - a)
    
    # w1 = w1 + err * x1
    # w2 = w2 + err * x2
    w = w + x_i * err
    b = b + err
print(w, b)


np.clip(0, 1, 5)


1 / (1 + np.exp(200))  >>>> 0으로 수렴


class LogisticNeuron:
    # 기본 가중치 생성
    def __init__(self):
        self.w = None
        self.b = None
    
    # 정방향 계산 함수
    def forpass(self, x):
        z = np.sum(x * self.w) + self.b
        return z
    
    # 가중치 업데이트
    def backprop(self, x, err):
        w_grad = x * err
        b_grad = 1 * err
        return w_grad, b_grad
    
    # 활성화 함수
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a
    
    # 훈련을 위한 fit() 메서드 생성
    def fit(self, x, y, epochs = 100):
        self.w = np.ones(x.shape[1])
        self.b = 0
        for i in range(epochs):
            for x_i, y_i in zip(x, y):
                z = self.forpass(x_i)
                a = self.activation(z)
                err = -(y_i - a)
                w_grad, b_grad = self.backprop(x_i, err)
                self.w -= w_grad
                self.b -= b_grad


    # 예측 함수 생성
    def predict(self, x):
        z = [self.forpass(x_i) for x_i in x]
        a = self.activation(np.array(z))
        return a > 0.5


# 모델 훈련
neuron = LogisticNeuron()
neuron.fit(X_train, y_train)
print(neuron.w, neuron.b)


pred = neuron.predict(X_test)


from sklearn.metrics import accuracy_score

accuracy_score(y_test, pred)


from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)

pred2 = dt_clf.predict(X_test)
accuracy_score(y_test, pred2)


from sklearn.linear_model import SGDClassifier

SGDClassifier()


class SingleLayer:
    def __init__(self):
        self.w = None
        self.b = None
        # 손실 함수 저장하기 위한 리스트
        self.losses = []
        
    def forpass(self, x):
        z = np.sum(x * self.w) + self.b
        return z
    
    def backprop(self, x, err):
        w_grad = x * err
        b_grad = 1 * err
        return w_grad, b_grad
    
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a
    
    def fit(self, x, y, epochs = 100):
        self.w = np.ones(x.shape[1])
        self.b = 0 
        for epoch in range(epochs):
            # 손실 초기화
            loss = 0
            # x의 index 랜덤하게 반환
            indexes = np.random.permutation(np.arange(len(x)))
            for i in indexes:
                z = self.forpass(x[i])
                a = self.activation(z)
                err = -(y[i] - a)
                w_grad, b_grad = self.backprop(x[i], err)
                self.w -= w_grad
                self.b -= b_grad
                # 안전한 로그 계산을 위한 범위 축소
                a = np.clip(a, 1e-10, 1-1e-10) 
                # 손실 계산
                loss += -(y[i] * np.log(a) + (1 - y[i]) * np.log(1 - a))

            # 에포크마다 평균 손실을 저장
            loss = loss / len(y)
            self.losses.append(loss)
            print(f'##### EPOCHS : {epoch + 1} ######')
            print(f'loss : {loss}')
            
    def predict(self, x):
        z = [self.forpass(x_i) for x_i in x]
        return np.array(z) > 0
    
    # 정확도 계산 함수 생성
    def score(self, x, y):
        return np.mean(self.predict(x) == y)


# 로지스틱 손실 함수

# -(y * np.log(a) + (1 - y) * np.log(1 - a))
-(0 * np.log(0.9999999999) + (1 - 0) * np.log(1 - 0.9999999999))


layer = SingleLayer()
layer.fit(X_train_scaled, y_train, epochs=1000)
print(layer.score(X_test_scaled, y_test))


plt.figure(figsize=(4,2))
plt.plot(layer.losses)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()


### 회귀에서 손실함수 저장하기  : 분류와 다른점 -활성함수(출력층)/ 손실함수 관련해서 바꿀것
# 손실함수의 결과값 저장 기능: 손실함수(loss) 계산하기-로지스틱 손실함수
class SingleLayerRegression: # 입력계층 1개, 은닉층 없음
    def __init__(self):
        self.w = None   # 입력된 값 없으므로
        self.b = None
        # 손실 함수 저장하기 위한 리스트
        self.losses = []
        
    def forpass(self, x):
        z = np.sum(x * self.w) + self.b
        return z
    
    def backprop(self, x, err):
        w_grad = x * err
        b_grad = 1 * err
        return w_grad, b_grad
    
    def activation(self, z):    #### 이걸 바꿔줘야
        a = z
        return a
    
    def fit(self, x, y, epochs = 100):
        self.w = np.ones(x.shape[1])
        self.b = 0 
        for epoch in range(epochs):
            # 손실 초기화
            loss = 0
            # x의 index 랜덤하게 반환
            indexes = np.random.permutation(np.arange(len(x)))
            for i in indexes:
                z = self.forpass(x[i])
                a = self.activation(z)
                err = -(y[i] - a)  # (y - y_hat)
                w_grad, b_grad = self.backprop(x[i], err)
                self.w -= w_grad
                self.b -= b_grad
                
                # 손실 계산 (SE)  (y - y_hat)^2
                loss += err ** 2

            # 에포크마다 평균 손실을 저장
            loss = loss / len(y) # MSE
            self.losses.append(loss)
            print(f'##### EPOCHS : {epoch + 1} ######')
            print(f'loss : {loss}')
            
    def predict(self, x):
        z = [self.forpass(x_i) for x_i in x]
        return z



from sklearn.datasets import load_diabetes

diabetes = load_diabetes()
x = diabetes.data
y = diabetes.target


X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, 
                                                    random_state=0)


reg = SingleLayerRegression()
reg.fit(X_train, y_train)


pred = reg.predict(X_test)


from sklearn.metrics import mean_squared_error

np.sqrt(mean_squared_error(y_test, pred))


plt.figure(figsize=(4,2))
plt.plot(reg.losses)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()


1 에포크는 모든 데이터를 최소 한번은 학습할때 써야지 된다.

에포크마다 학습횟수
    데이터당 학습 - 데이터를 하나씩 적용하여 가중치 업데이트
    1회 학습 - 모든 데이터를 한번에 적용하여 가중치 업데이트
    N회 학습 - 데이터를 M개씩 묶어서 가중치 업데이트


# w - 입력의 데이터 개수(컬럼(독립변수) 개수)
w = np.ones(X_train.shape[1])
b = 0


# 가중합산
z = np.dot(X_train, w.reshape(-1,1)) + b
z


# 활성함수
z = np.clip(z, -100, None)
a = 1 / (1 + np.exp(-z))
a


# 손실함수 평균값
a2 = np.clip(a, 1e-10, 1-1e-10)
L = (-(y_train.reshape(-1,1) * np.log(a2) + (1 - y_train.reshape(-1,1)) * np.log(1 - a2)))
L.mean()


# 오차
err = y_train.reshape(-1,1) - a
# 오차역전파
w = w + np.dot(err.T, X_train).reshape(-1) / X_train.shape[0]
b = b + np.mean(err).reshape(-1)


w


b


err.shape


20.5099 / 455


from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
x = cancer.data
y = cancer.target
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)
print(X_train.shape, X_test.shape)


class SingleLayer:
    def __init__(self):
        self.w = None
        self.b = None
        self.losses = []
        
    def forpass(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    def backprop(self, x, err):
        m = len(x)
        w_grad = np.dot(x.T, err) / m
        b_grad = np.sum(err) / m  # np.mean(err)
        return w_grad, b_grad
    
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a

    def predict(self, x):
        z = self.forpass(x)
        return z > 0
    def score(self, x, y):
        return np.mean(self.predict(x) == y.reshape(-1, 1))
    
    def fit(self, x, y, epochs = 100, random_state = None):
        y = y.reshape(-1,1) # 열 벡터로 변환
        m = len(x)
        self.w = np.ones((x.shape[1], 1)) # 가중치 초기화
        self.b = 0 # 절편 초기화
        for i in range(epochs):
            z = self.forpass(x)
            a = self.activation(z)
            err = -(y - a)
            w_grad, b_grad = self.backprop(x, err)
            self.w -= w_grad
            self.b -= b_grad
            a = np.clip(a, 1e-10, 1-1e-10)
            loss = np.mean(-(y*np.log(a) + (1-y)*np.log(1-a))) 
            self.losses.append(loss)


import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
# 스케일링
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)


single_layer = SingleLayer()
single_layer.fit(X_train_scaled, y_train, epochs=1000)
single_layer.score(X_test_scaled, y_test)


plt.plot(single_layer.losses)
plt.ylim(0,0.1)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
print(single_layer.losses[-1])


-(1*np.log(0.95) + (1-1)*np.log(1-0.8))


from sklearn.datasets import load_diabetes

diabetes = load_diabetes()
x = diabetes.data
y = diabetes.target
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, 
                                                    random_state=0)


# 회귀
class SingleLayer:
    def __init__(self):
        self.w = None
        self.b = None
        self.losses = []
        
    def forpass(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    def backprop(self, x, err):
        m = len(x)
        w_grad = np.dot(x.T, err) / m
        b_grad = np.sum(err) / m  # np.mean(err)
        return w_grad, b_grad
    
    def activation(self, z):
        a = z
        return a

    def predict(self, x):
        z = self.forpass(x)
        return z > 0
    def score(self, x, y):
        return np.mean(self.predict(x) == y.reshape(-1, 1))
    
    def fit(self, x, y, epochs = 100, random_state = None):
        y = y.reshape(-1,1) # 열 벡터로 변환
        m = len(x)
        self.w = np.ones((x.shape[1], 1)) # 가중치 초기화
        self.b = 0 # 절편 초기화
        for i in range(epochs):
            z = self.forpass(x)
            a = self.activation(z)
            err = -(y - a)
            w_grad, b_grad = self.backprop(x, err)
            self.w -= w_grad
            self.b -= b_grad
            loss = np.mean(err ** 2)
            self.losses.append(loss)


layer = SingleLayer()
layer.fit(X_train, y_train, epochs=1000)
layer.losses


plt.plot(layer.losses)
plt.ylim(2500,3000)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
print(layer.losses[-1])



