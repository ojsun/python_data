   <머신러닝 과정 정리>
0. 데이터 준비(수집 등)
1. 데이터 전처리: 오류데이터 보정, null값 처리 등 데이터 클렌징작업, 
    LableEncoding, OneHotEncoder, 데이터 스케일링, 정규화 작업
    StandardScaler, MinMaxScaler
2. 데이터 분리 및 교차검증 : train_test_split
3. 머신러닝 : 모델(알고리즘) 선택
  - 모델학습  dt_clf = DecisionTreeClassifier(random_state = 0)
             dt_clf.fit(X_train, y_train)
        ==> 반복수행하며 학습방법을 바꿔 성능향상
  - 예측     pred = dt_clf.predict(X_test)
  - 평가(방법 선택, 기준잡기)  
            acc = accuracy_score(y_test, pred)
4. 교차검증: KFold, StratifiedKFold
            cross_val_score, cross_validate, GridSearchCV
5. 최적의 모델, 방법으로 반복
     학습,예측,평가 반복하기
5. 반복 또는 완전히 학습종료 후, 테스트 및 배포

<scikit-learn 파일 구성>
독립변수: data(값,row)   /feature_names
종속변수: target(값,label,columns) / target_names, cross_validate

<자주 쓰는 scikit-learn 모듈>
import sklearn
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier,
                         DecisionTreeRegressor
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
                         cross_val_score, cross_validate, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
                         StandardScaler,MinMaxScaler

1. 데이터 전처리/데이터셋트 분리
-df로 만들기 / 사이킷런은 딕셔널리 비슷
df = pd.DataFrame(data['data'], columns = data['feature_names'])
-target 열 만들기
df['target'] = data['target']   # 숫자로 되어 있음
df['target'] = df['target'].apply(lambda x : data['target_names'][x])

- train, test로 데이터 나누기
  :df가 종속변수 + 독립변수 합쳐진 모양일 때 나누는 방법
 방법1)  종속 / 독립 변수로 나눠서 train_test_split한다.
    y_df = df['target']
    x_df = df.drop('target', axis = 1)
    X_train, X_test, y_train, y_test = 
       train_test_split(x_df, y_df, test_size = 0.2, random_state=0)
 방법2) train_test_split을 먼저하고 종속 / 독립 변수로 나누기
    train, test = train_test_split(df, test_size = 0.2, random_state=0)
    y_train = train.loc['target']
    x_train = train[['다른 feature_names']]
    y_test = test.iloc[:, -1]
    x_test = test.iloc[:, :-1]
    
* 함수 train_test_split() 
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test 
       =train_test_split(DATA, TARGET,test_size = 0.2,random_state = 0)

    train_test_split(
        *arrays,           # *는 데이터 무한대로 쓸 수 있음
        test_size=None,    # test와 train이 1을 나눠 가짐
        train_size=None,
        random_state=None, # 값을 설정하지 않으면 값 변동됨/ 쓰면 고정됨
        shuffle=True,      # 데이터를 분리하기전 미리 섞을지  
        stratify=None)     # 종속변수의 범주를 균등하게 만들어줌
 
2. 모델학습
    from sklearn.tree import DecisionTreeClassifier
    dt_clf = DecisionTreeClassifier(random_state = 0)
    dt_clf.fit(X_train, y_train)
3. 예측  
    pred = dt_clf.predict(X_test)   
4. 평가  
    from sklearn.metrics import accuracy_score
    acc = accuracy_score(y_test, pred)
   
## 교차검증/ 일반화 검증
1. K_Fold 교차검증
    from sklearn.model_selection import KFold
2. StratifiedKFold 교차검증
    from sklearn.model_selection import StratifiedKFold
3. cross_val_score 교차검증
    from sklearn.model_selection import cross_val_score
    scores = cross_val_score(dt_clf, data, label, scoring = 'accuracy', cv = 5)
4. cross_validate (dt_clf, data, label, scoring = ['accuracy', 'roc_auc_ovo'], cv = 5)

# 하이퍼 파라미터의 성능향상: GridSearchCV
from sklearn.model_selection import GridSearchCV
dtree=DecisionTreeClassifier()
parameters={'max_depth':[1,2,3], 'min_samples_split':[2,3]}
grid_dtree=GridSearchCV(dtree, param_grid=parameters,cv=3,refit=True)

## 데이터 전처리
1. 레이블 인코딩// 필수 전처리 1차원 (종속변수-범주형): 선형회귀에서는 X
from sklearn.preprocessing import LabelEncoder
lb_enc=LabelEncoder()
lb_enc.fit(y)                   # fit은 y의 범주 종류를 저장하는 역할
label_iris=lb_enc.transform(y)  # 위에서 저장한 범주를 맞는 숫자로 변환

=> 방법2) lbe=LabelEncoder()    # 종속 아니어도 독립 2개면 가능
    titanic['Sex']=lbe.fit_transform(titanic_df['Sex'])

2.원-핫 인코딩: 문자를 숫자로 만들기 위해 2차원 희소행렬로 만듦(독립변수-범주형)

(간단한 방법) OneHotEncoder: 3개 이상 독립변수 
oh=OneHotEncoder(sparse_output=False)
C_B=oh.fit_transform(titanic_df[['Cabin','Embarked']])
columns=np.hstack(['Cab_' + oh.categories_[0], 'Emb_'+ oh.categories_[1]])
titanic_df=pd.concat([titanic_df, pd.DataFrame(C_B, columns=columns)], axis=1)

(원칙적 방법)      
oh_encoder = OneHotEncoder()
oh_encoder.fit(items)
oh_labels = oh_encoder.transform(items)  
oh_labels.toarray()      #희소행렬을 toarray()를 이용하여 밀집행렬로 변환
titanic_df[['female','male']]= enc.toarray() # 데이터프레임으로 변환

==> .fit_transform() 데이터프레임이 문자형을 한번에 변환함
    범주형 -> 숫자형으로 변환/ 
    종속변수, 2개 독립변수 일 경우 Label인코드 // 3개 이상 onehot인코더 사용
    숫자형 범주 등 미리 .astype('object')으로 문자형으로 바꾸고 할 것

# pandas 에서는 자동으로 원핫인코딩 해주누는 함수 .get_dummies
# get_dummies - 범주형 변수에서 더미변수를 자동으로 만들어주는 함수
  pd.get_dummies(df) 

3. 데이터 전처리-표준화(독립변수-숫자 정리) // StandardScaler 함
# 평균은 0에, 분산은 1에 가깝게 배열됨 # 수의 값이 끝이 없을때 사용  
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)
iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)

4. 데이터 전처리 -정규화 :표준화와 모두 같고 MinMaxScaler() 여기만 다름
** 수의 값이 끝이 있을때 사용 / 무조건 0-1 사이 값임=> 벗어나면 이상치
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)
iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)

** train, test 데이터 스케일링 변환시 유의점 **
데이터스케일링 변환 후 학습, 테스트 데이터로 분리할 것!!
안되면, 테스트할때 fit()하지 말고 이미 fit된 scaler객체를 이용하요 transform으로 변환!!


<분류의 평가: 이진 분류의 예측성능 측정>- 예측 평가
from sklearn.metrics import accuracy_score,confusion_matrix
           recall_score, precision_score, precision_recall_curve
           f1_score, roc_curve
from sklearn.base import BaseEstimator

1) 정확도 (TN+TP)/(TN+TP+FN+FP) :0~1 얼마나 많이 맞추었는지 나타내는 수치   
    accuracy_score(y_test,pred)

2) 오차행렬 
    confusion_matrix(y_test,pred)

3) 정밀도 TP/(FP+TP)    0~1 예측을 1로 했을때 얼마나 맞췄나
    precision_score(y_test,pred)

4) 재현률  TP/(FN+TP)    0~1 실제가 1일때 얼마나 맞췄나
    recall_score(y_test,pred)

    ** 예측확률) predict_proba 
      pred_proba=lr_clf.predict_proba(X_test)
      pred=lr_clf.predict(X_test)
      pred_proba_result=np.concatenate([pred_proba, pred.reshape(-1,1)],axis=1)

    ** 임곗값 조정) Binarize: threshold 조정하여 재현률/정밀도 조정
     # 임곗값 조정하기: 값을 변환해주는 모델
    
    from sklearn.preprocessing import Binarizer
    X = [[ 1, -1, 2],[ 2, 0, 0],[ 0, 1.1, 1.2]]
    binarizer = Binarizer(threshold= 1.1)  # 크야만 1, 작거나 같으면 0
    print(binarizer.fit_transform(X))
    ** 정밀도와 재현률 곡선:  precision_recall_curve
    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1 )

5) F1 스코어: 정밀도와 재현률 결합한 지표: 0~1 밸런스가 잘 유지되는지 표현하는 수치
    f1_score(y_test, pred)
    f1_score(y_test, pred, average='macro')   --> 다중 분류(macro)에서의 범주가 다수일 때/ 평균
    f1_score(y_test, pred, average='weighted') --> macro방식에 비율에 따른 가중치 주어 계산
    f1_score(y_test, pred, average='micro')  --> 

6) roc_auc_score  : 0~1, 전반적으로 잘 학습했는지를 표현하는 수치
    ROC 곡선 : 임곗값에 따라 FPR이 변할 때, TPR의 변화 표현
    AUC :roc_auc_score(y_test, pred_proba)

    from sklearn.metrics import roc_auc_score
    pred_proba = lr_clf.predict_proba(X_test)[:, 1]
    roc_score = roc_auc_score(y_test, pred_proba)
    print(f'ROC AUC 값: {roc_score:.4f}')


## 정확도/F1_Score/ROC_ACC_score  ==> 주로 이 3가지 평가 사용
## 정밀도/재현율  ==> 뭘 높일지 방향성 있을때 사용

## 회귀 평가방법은 오차평균값(정대값)으로 평가함 => MAE 
                        또는 제곱한 후 루트화  => MSE

<다중 분류>
1) 평균값으로 f1_score  
    f1_score(y_test, pred, average='macro')   --> 다중 분류(macro)에서의 범주가 다수일 때
    f1_score(y_test, pred, average='weighted') --> macro방식에 비율에 따른 가중치 주어 계산
    f1_score(y_test, pred, average='micro')  
2) ROC_AUC_Score
    roc_auc_score(y_test, pred_proba, multi_class='ovo') : 1:1 1대 일
    roc_auc_score(y_test, pred_proba, multi_class='ovr')  : 1 대 다수

3)  scoring='roc_auc_score_ovo' 언더바 다음에 옵션 쓰는 방법으로 다중분류 검정
    cross_val_score(dt_clf,X_train, y_train, scoring = 'f1_micro', cv = 5)


< 알아 둘 것들 -tip>
- np.vstack 넘파이 배열을 수직으로 붙이기/합치기(np.hstack은 수평으로 붙이기
category=np.hstack(('cab_'+ oh_enc.categories_[0],
                               'em_'+ oh_enc.categories_[1] )) #OneHotEncoder

- StandardScaler 한 후에 결과값은 이상치 제거를 위한 z-score 값과 같음
  그러므로 2.5 이상, -2.5 이하값 제거 하면 이상치 제거임
    ==> 이상치 제거 z_score  // titanic_d[titanic_d['Fare'] <=2.5]
    
- 종속변수와 독립변수 나누기
X_titanic=titanic_df.drop(['Survived'], axis=1)
y_titanic=titanic_df['Survived']
/  
X=diabets_data.iloc[:,:-1]
y=diabets_data.iloc[:,-1]







