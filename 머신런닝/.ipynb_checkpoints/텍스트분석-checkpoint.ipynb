{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570d42f0-6116-4ddd-a60e-198959279130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdc63f-7f21-459f-a903-629288f9db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### stevejobs.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce445f29-ad4e-4fd0-a8d0-5e9cc03843ad",
   "metadata": {},
   "source": [
    "0. 불러오기\n",
    "1. 텍스트 전처리(정규화): 클렌징/토큰화/불용어/형태소 축출\n",
    "2. (학습셋트 만들기)\n",
    "3. 벡터화\n",
    "4. 모델로 학습/성능평가\n",
    "5. 파라미터 성능 향상/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751bcc6-2bb9-4b4e-966c-2891af494be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "with open('../data/stevejobs.txt', 'r', encoding='utf8') as f:\n",
    "    rows=f.readlines()\n",
    "    lines=[row for row in rows]\n",
    "text=' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f908c2-c897-49cd-b859-253028e337a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1-1.텍스트 전처리(정규화): 클렌징\n",
    "import re\n",
    "def cleaning(text):\n",
    "    p = re.compile('[^ ㄱ-ㅣ가-힣]+')#영어 ('[^ a-zA-Z0-9\\.]+') ('[^ a-zA-Zㄱ-ㅣ가-힣]+')\n",
    "result = p.sub('',text).lower()\n",
    "    return result\n",
    "df['content']=df['content'].apply(cleaning)\n",
    "#방법2\n",
    "import re\n",
    "df['content'] = df['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))\n",
    "\n",
    "#방법3 영어\n",
    "from nltk.corpus import stopwords\n",
    "eng_stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf9150-20dc-4d6c-a81e-c9f58b12a13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1-2.토큰화(문장)\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "#방법1\n",
    "def okt_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text, stem = True)\n",
    "    return tokens_ko\n",
    "#방법2\n",
    "def okt_tokenizer(text):   # 자르기 + 조사 삭제\n",
    "    words=okt.pos(text, stem=True)\n",
    "    filtered_words=[]\n",
    "    for word, pos in words:\n",
    "        if pos not in ['Josa']:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "#방법0\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences=nltk.sent_tokenize(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7e1fb-6ef6-444b-9121-1dfbc47c897c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 토큰화(단어) extend: 리스트로 \n",
    "word_token=[]\n",
    "for sentence in sentences:\n",
    "    words=nltk.word_tokenize(sentence)\n",
    "    word_token.extend(words)\n",
    "print(word_token[:3])\n",
    "\n",
    "# 토큰화(문단 --> 단어) append: 2차원 데이터프레임으로\n",
    "word_token2=[]\n",
    "for sentence in sentences:\n",
    "    words=nltk.word_tokenize(sentence)\n",
    "    word_token2.append(words)\n",
    "print(word_token2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33578ac3-c291-4278-b573-0101052f4d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 토큰화 함수: 문서의 모든 단어\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentense) for sentense in sentences]\n",
    "    return word_tokens\n",
    "word_tokens = tokenize_text(text)\n",
    "print(word_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46968f-720f-4aea-bd0e-00abb40081b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1-3.불용어(stop_words) 제거\n",
    "nltk.download('stopwords')\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "all_tokens=[]\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "all_tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70638718-508f-4d09-8702-eda64e7d8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-4. 어근(형태소) 추출: Stemming/ Lemmatization -영어에서만 사용\n",
    "from nltk import LancasterStemmer\n",
    "stemmer=LancasterStemmer()\n",
    "stemmer.stem('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdb395-86aa-490d-9fdb-9f2d58086486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.피처 벡터화/축출(BOW : CountVectorizer/TfidfVectorizer)\n",
    "# 2-1. 카운트 기반의 벡터화: CountVectorizer\n",
    "#전처리: 한글의 경우\n",
    "import re\n",
    "def cleaning(text):\n",
    "    p = re.compile('[^ a-zA-Zㄱ-ㅣ가-힣]+')  #영어 ('[^ a-zA-Z0-9\\.]+')\n",
    "    result = p.sub('',text).lower()\n",
    "    return result\n",
    "df['content']=df['content'].apply(cleaning)\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "def okt_tokenizer(text):   # 자르기 + 조사 삭제\n",
    "    words=okt.pos(text, stem=True)\n",
    "    filtered_words=[]\n",
    "    for word, pos in words:\n",
    "        if pos not in ['Josa']:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "with open('data/stopword.txt','r',encoding='utf-8') as f:\n",
    "    word = f.read()\n",
    "    stopwords = word.split('\\n')\n",
    "\n",
    "# 셋트 만들기\n",
    "y_df=review_df['sentiment']\n",
    "X_df=review_df.drop(['id','sentiment'], axis=1, inplace=False)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_df, y_df, test_size=0.3, random_state=156)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# 벡터화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vct=CountVectorizer(max_df=0.95,max_features=1000, min_df=2,\n",
    "                        tokenizer=okt_tokenizer, stop_words=stopwords)\n",
    "# 영어 count_vect = CountVectorizer(max_df=0.95,max_features=1000, min_df=2,stop_words='english', token_pattern = '[a-zA-Z]+',ngram_range=(1,2))\n",
    "cnt_vct.fit(X_train)   # y값 없음!!\n",
    "X_train_cnt=cnt_vct.transform(X_train)\n",
    "X_test_cnt=cnt_vct.transform(X_test)\n",
    "\n",
    "# 3.모델로 분류\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf=LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt, y_train)\n",
    "pred=lr_clf.predict(X_test_cnt)\n",
    "print('예측 정확도 : ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f05b9f-5c6d-4cb1-9fb2-c4986d841a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2-2. TF-IDF: TfidfVectorizer\n",
    "# (자주 나오는 단어에 가중치/모든 문서에서 자주나오는 단어에 패털티)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vct=TfidfVectorizer(max_df=0.95,max_features=1000, min_df=2,\n",
    "                          tokenizer=okt_tokenizer, stop_words=stopwords)\n",
    "# 영어 count_vect = CountVectorizer(max_df=0.95,max_features=1000, min_df=2,stop_words='english', token_pattern = '[a-zA-Z]+',ngram_range=(1,2))\n",
    "tf_vct.fit(X_train)\n",
    "X_train_tf=tf_vct.transform(X_train)\n",
    "X_test_tf=tf_vct.transform(X_test)\n",
    "\n",
    "# 3.모델로 분류\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf=LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tf, y_train)\n",
    "pred=lr_clf.predict(X_test_tf)\n",
    "print('예측 정확도 accuracy: ', accuracy_score(y_test, pred))\n",
    "print('예측 정확도 f1: ', f1_score(y_test,pred)\n",
    "      \n",
    "# 4. 적용\n",
    "test_text = '욕나온다. 쓰레기'\n",
    "predict = tf_vct.transform([test_text])\n",
    "lr_clf.predict(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbac3bd-df5f-4644-add5-c4d493a8fdf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3-1. GridSearchCV로 파라미터 성능 향상\n",
    "params={'C':[0.01, 0.1, 1, 5, 10]}\n",
    "gr_lr_clf = GridSearchCV(lr_clf,param_grid=params,cv=3,scoring='accuracy', verbose=1)\n",
    "gr_lr_clf.fit(X_train_tf , y_train)\n",
    "print('LogisticRegression의 최적 파라미터: ', gr_lr_clf.best_params_)\n",
    "\n",
    "# 최적 C값으로 예측, 정확도 평가\n",
    "pred=gr_lr_clf.predict(X_test_tf)\n",
    "print('예측 정확도 : ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df68a37-a6e2-4d0c-a085-ed3ad83f037f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline으로 만들기:TF-IDF 벡터화, GridSearchCV 최적찾기\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline=Pipeline([('tf_vct', TfidfVectorizer(stop_words='english')),\n",
    "                   ('lr_clf', LogisticRegression())])\n",
    "params={'tf_vct__ngram_range': [(1,1),(1,2)],\n",
    "        'tf_vct__max_df': [100,200,300],\n",
    "        'lr_clf__C': [1,5,7]}\n",
    "grid_cv_pipe=GridSearchCV(pipeline, param_grid=params,cv=3,scoring='accuracy')\n",
    "grid_cv_pipe.fit(X_train,y_train)\n",
    "print('최적 파라미터: ', grid_cv_pipe.best_params_)\n",
    "pred=gr_lr_clf.predict(X_test)\n",
    "print('예측 정확도 : ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38837ba-e77b-44b4-909c-aa1c0302be7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeae9f6-a068-49af-9f75-a178625704f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 단어 토큰\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    word = okt.morphs(sentence)\n",
    "    words.append(word)\n",
    "print(words[:2])\n",
    "test_text = '나는 정말로 파이썬을 좋아한다. 아니 머신러닝을 더 좋아한다.'\n",
    "print('normalize :', okt.normalize(test_text)) # 문장으로 추출\n",
    "print('morphs :', okt.morphs(test_text))       # 구문 분석\n",
    "print('nouns :', okt.nouns(test_text))         # 명사만\n",
    "print('phrases :', okt.phrases(test_text))     # 구문\n",
    "print('pos :', okt.pos(test_text))             # 품사와 함께 값고 함께 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb0f54-58cd-4d5c-a5a0-78772083f73b",
   "metadata": {},
   "source": [
    "#### 감성 분석: 지도학습, 비지도학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9787b430-d110-4fb5-b352-5eab05a318a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 영화 평가 분석// 지도학습 기반  (1) CountVectorizer + pipeline\n",
    "review_df = pd.read_csv('data/labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa046b22-b1e8-49b6-aa61-b7e3c2aa3478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
    "review_df['review'] = review_df['review'].apply( lambda x : re.sub(\"[^a-zA-Z]\", \" \", x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b3038f-7ca5-4916-8eaf-2457e1723107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 1) (7500, 1)\n"
     ]
    }
   ],
   "source": [
    "y_df=review_df['sentiment']\n",
    "X_df=review_df.drop(['id','sentiment'], axis=1, inplace=False)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_df, y_df, test_size=0.3, random_state=156)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefdc34b-199c-405d-b9eb-82a255d48bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "pipeline = Pipeline([('cnt_vect', CountVectorizer(stop_words='english',ngram_range=(1,2) )),\n",
    "                     ('lr_clf', LogisticRegression(solver='liblinear', C=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d737a-3355-46c7-aa0a-607990d54464",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "print(f'예측 정확도는 {accuracy_score(y_test ,pred):.4f}')\n",
    "print(f'ROC-AUC는 {roc_auc_score(y_test, pred_probs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef5119-2760-4a28-aefd-e0b6a0a9921a",
   "metadata": {},
   "source": [
    "#### 영화 평가 분석// 지도학습 기반 (2) 한글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99880d15-9e4f-4dd2-86b8-b2d7b1f1b853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 전처리(한글)\n",
    "def cleaning(text):\n",
    "    p = re.compile('[^ a-zA-Zㄱ-|가-힣\\]+')\n",
    "    result = p.sub('',text).lower()\n",
    "    return result\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "def okt_tokenizer(text):   # 자르기 + 조사 삭제\n",
    "    words=okt.pos(text, stem=True)\n",
    "    filtered_words=[]\n",
    "    for word, pos in words:\n",
    "        if pos not in ['Josa','KoreanParticle']:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "with open('data/stopword.txt','r',encoding='utf-8') as f:\n",
    "    word = f.read()\n",
    "    stopwords = word.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d50eba-e5dc-4de0-bcf5-bd6c2482fb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(df['content'],df['score'], test_size=0.2, random_state=0 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c929d195-21e4-4986-96c8-fb3431a240e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vct=TfidfVectorizer(tokenizer=okt_tokenizer, stop_words=stopwords, max_features=5000)\n",
    "tfidf_vct.fit(X_train)\n",
    "X_train_tf=tfidf_vct.transform(X_train)\n",
    "X_test_tf=tfidf_vct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1e6771-64d6-4fd2-9bad-6329dbb49a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 :  0.5818540433925049\n"
     ]
    }
   ],
   "source": [
    "lr_clf=LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tf, y_train)\n",
    "pred=lr_clf.predict(X_test_tf)\n",
    "print('예측 정확도 : ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067b8be-08f3-4767-b637-01efda015ecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LDA 토픽 모델링 (1): CountVectorizer만 허용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3eacf-1e2e-465f-87c6-b4f86f5e28c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c34bc7-0d2c-4458-b1c8-b6dd3fea77f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', \n",
    "    'comp.windows.x', 'talk.politics.mideast', \n",
    "    'soc.religion.christian', 'sci.electronics', 'sci.med' ]\n",
    "news_df= fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=cats, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb5fa6-3f1f-4b7e-9baa-c94fb3e578c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_df=0.95,max_features=1000, min_df=2,stop_words='english', token_pattern = '[a-zA-Z]+',ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)\n",
    "print(lda.components_.shape)\n",
    "print(lda.components_)  # 볼 수 없음, 숫자만 가득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff9336-9076-4865-b5e0-5ceb4747a507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #',topic_index)\n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
    "        print(feature_concat)\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "display_topics(lda, feature_names, 15)     # 가장 연관도 높은 word 15개 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282995d-054e-462e-8e2e-d161bb84bdc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LDA 토픽 모델링-(2) 한글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841846b-c3fe-4124-b5ac-0866db07a57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/petition.csv')\n",
    "cats = ['정치개혁', '인권/성평등', '안전/환경', '교통/건축/국토', '육아/교육']\n",
    "df_cats = df[df['category'].isin(cats)]\n",
    "df_samples = df_cats.sample(frac=0.05, random_state = 0)\n",
    "df_samples['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08266f-4d25-4cd5-933d-72b7560820f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "def okt_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text, stem = True)\n",
    "    return tokens_ko\n",
    "\n",
    "with open('data/stopword.txt','r',encoding='utf-8') as f:\n",
    "    word = f.read()\n",
    "stopwords = word.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe14872-0846-4be6-b3bd-b1e9986b7687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df = 0.9,max_features = 1000,min_df = 2, ngram_range = (1, 2),tokenizer = okt_tokenizer,stop_words = stopwords)\n",
    "tfidf_df = tfidf_vect.fit_transform(df_samples['content'])\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 5, random_state = 0)\n",
    "lda.fit(tfidf_df)\n",
    "print(lda.components_.shape)\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac93c4-b9eb-452d-a655-6b4d3c171ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "display_topics(lda, feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361eefe1-052d-4ec7-bb8c-2acda24c05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA 토픽 모델링-(3) 한글/ 하나의 주제에서 주요 주제 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999ddef-00b7-4343-9006-1f13baa40dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/petition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29348629-6df8-4215-8759-dd14f42449d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cats = ['육아/교육']\n",
    "df_cats = df[df['category'].isin(cats)]\n",
    "df_samples = df_cats.sample(frac=0.5, random_state = 0)\n",
    "df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b06468-40db-4f96-b948-cf28ae443fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_df = 0.9, max_features = 1000,min_df = 2, ngram_range = (1, 2),tokenizer = okt_tokenizer,stop_words = stopwords)\n",
    "tfidf_df = tfidf_vect.fit_transform(df_samples['content'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6438d-f9f7-49b5-a471-d9e147f82f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 5, random_state = 0)    \n",
    "lda.fit(tfidf_df)\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "display_topics(lda, feature_names, 15)    # 토픽보기 함수 display_topics  15개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388b2a9-a717-4183-b450-4634bdbbeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.argsort()[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df3fef-b159-42c4-b2b1-485afd3ebfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df=pd.DataFrame(lda.components_, columns=tfidf_vect.get_feature_names_out())\n",
    "lda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9fd3d-14c3-4879-8be2-5add5409a62e",
   "metadata": {},
   "source": [
    "#### KMeans 군집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e96813-8205-47d8-b33e-98c839a0882d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 앞에 이미: 불러오기/ 클리닝/ TfidfVectorizer 벡터화,학습,변환\n",
    "from sklearn.cluster import KMeans\n",
    "km_cluster = KMeans(n_clusters = 4, max_iter = 10000, random_state = 0)\n",
    "km_cluster.fit(tfidf_df)\n",
    "cluster_label = km_cluster.labels_\n",
    "clust_df = df_samples[['content', 'category']]\n",
    "clust_df['cluster'] = cluster_label\n",
    "clust_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3215df-efb2-4268-8cea-ffb7fff9da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군집별 top n 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명을 반환\n",
    "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
    "    cluster_details = {}\n",
    "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
    "    for cluster_num in range(clusters_num):\n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
    "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
    "        top_features = [ feature_names[ind] for ind in top_feature_indexes ]\n",
    "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
    "        cluster_details[cluster_num]['top_features'] = top_features\n",
    "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
    "    return cluster_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39a502-7e8e-4a6c-9b72-96c23f256816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보기좋게 함수  get_cluster_details 를 표현하는 함수\n",
    "def print_cluster_details(cluster_details):\n",
    "    for cluster_num, cluster_detail in cluster_details.items():\n",
    "        print('# Cluster {0}'.format(cluster_num))\n",
    "        print('Top features:', cluster_detail['top_features'])\n",
    "clust_centers = km_cluster.cluster_centers_\n",
    "\n",
    "# 위 두 함수 불러오기\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=clust_df, \n",
    "                                      feature_names=feature_names, clusters_num=4, top_n_features=10 )\n",
    "print_cluster_details(cluster_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f34ec0-286f-450b-841b-327a931171cb",
   "metadata": {},
   "source": [
    "#### 유사도 분석 cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec7330-ccea-4b7e-8380-9a1e1aa04234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리(정규화): 클렌징/토큰화/불용어/형태소 축출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5d0bd-121e-44d7-b088-5efc2d1c2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_df = 0.85, min_df = 2, tokenizer=okt_tokenizer, stop_words=stopwords, max_features=1000)       \n",
    "feature_vect = tfidf_vect.fit_transform(df['content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d86e0-7208-4ae4-a45e-a9d682e190a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_pair = cosine_similarity(feature_vect)\n",
    "print(similarity_pair)\n",
    "print('shape:',similarity_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b398eea-4345-4603-a414-af9abf8a7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_vect_simple.toarray(), columns=tfidf_vect_simple.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661ff10-6efe-48c4-9738-813d097f9812",
   "metadata": {},
   "source": [
    "#### 감정사전 함수(Rexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec0b82-5f26-45a3-9a26-d24bb74a2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setiment_analyzer(text):\n",
    "    import pandas as pd\n",
    "    from konlpy.tag import Kkma\n",
    "    from nltk.util import ngrams\n",
    "    \n",
    "    senti_words = pd.read_csv('./data/polarity.csv')\n",
    "    kkma = Kkma()\n",
    "    ngram1 = kkma.pos(text, join = True)\n",
    "    ngram2 = list(ngrams(ngram1, n=2)) # result를 ngram1로 바꿈\n",
    "    new_ngram2 = []\n",
    "    for n in ngram2:\n",
    "        new_ngram2.append(';'.join(n))\n",
    "    ngram3 = list(ngrams(ngram1, n=3))  # result를 ngram1로 바꿈\n",
    "    new_ngram3 = []\n",
    "    for n in ngram3:\n",
    "        new_ngram3.append(';'.join(n))\n",
    "    words = ngram1 + new_ngram2 + new_ngram3\n",
    "    result_df = senti_words[senti_words['ngram'].isin(words)]\n",
    "\n",
    "    neg_df = result_df[result_df['max.value'] == 'NEG']\n",
    "    pos_df = result_df[result_df['max.value'] == 'POS']\n",
    "    neg_value = (neg_df['NEG'] / neg_df['freq']).sum()\n",
    "    pos_value = (pos_df['NEG'] / pos_df['freq']).sum()\n",
    "    neg_length = neg_df.shape[0]\n",
    "    pos_length = pos_df.shape[0]\n",
    "\n",
    "    if pos_length == 0:\n",
    "        final_value = (pos_value ) - (neg_value / neg_length)\n",
    "    elif neg_length == 0:\n",
    "        final_value = (pos_value / pos_length) - (neg_value)\n",
    "    else:\n",
    "        final_value = (pos_value / pos_length) - (neg_value / neg_length)\n",
    "\n",
    "\n",
    "    if final_value >= 0:\n",
    "        print('긍정문장입니다.')\n",
    "    else:\n",
    "        print('부정문장입니다.')\n",
    "    return final_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8511bf8-8b1e-4807-bbf5-83d0bae5af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['content'].sample(1).iloc[0]\n",
    "print(text)\n",
    "setiment_analyzer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f117757-0004-48c8-b6fc-e07445615a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립변수들이 원핫형식(희소행렬)일때 한 독립변수로 모으는 법\n",
    "\n",
    "y = df.iloc[:, 1:]\n",
    "y_label = pd.DataFrame({'target': y.columns})\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_enc = OneHotEncoder()\n",
    "oh_enc.fit(y_label)\n",
    "\n",
    "y_oh = y[oh_enc.categories_[0]]\n",
    "y_oh.drop([5876, 11942], inplace=True)\n",
    "oh_enc.inverse_transform(y_oh)\n",
    "\n",
    "data = df[['문장']]\n",
    "data.drop([5876, 11942], inplace=True)\n",
    "data[['정답']] = oh_enc.inverse_transform(y_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad64c9-823e-4630-ae1b-637f3bc4c60a",
   "metadata": {},
   "source": [
    "#### 추천시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9d8c2-20fe-4fb7-b343-71c2b21177e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 콘텐츠 기반 추천시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8ca9a-244c-461b-881a-83f85f32d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies =pd.read_csv('../data/tmdb_5000_movies.csv')\n",
    "movies_df = movies[['id','title', 'genres', 'vote_average', 'vote_count', 'popularity', 'keywords', 'overview']]\n",
    "\n",
    "#데이터 가공\n",
    "from ast import literal_eval         # literal_eval 리스트 등의 집합적 코드만 실행하여 안전을 기함/ eval은 모두 다 됨\n",
    "movies_df['genres'] = movies_df['genres'].apply(literal_eval)\n",
    "movies_df['keywords'] = movies_df['keywords'].apply(literal_eval)\n",
    "movies_df[['genres','keywords']]\n",
    "# name값 리스트객체로 만들기\n",
    "movies_df['genres'] = movies_df['genres'].apply(lambda x : [ y['name'] for y in x])  # list로 가져오기\n",
    "movies_df['keywords'] = movies_df['keywords'].apply(lambda x : [ y['name'] for y in x])  # list로 가져오기\n",
    "\n",
    "# 내용이 많으니 단어들을 한데 묶어서 벡터화 시킴(공백으로 word단위가 구분되는 문자열로 반환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "movies_df['genres_literal'] = movies_df['genres'].apply(lambda x : (' ').join(x))\n",
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "genre_mat = count_vect.fit_transform(movies_df['genres_literal'])\n",
    "# 유사도가 큰 순서로 정렬후 인덱스 반환\n",
    "genre_sim = cosine_similarity(genre_mat, genre_mat)\n",
    "genre_sim_sorted_ind = genre_sim.argsort()[:, ::-1]  # 무조건 1개 밖에 추천 못함// [::-1]은 내림차순 [::1]은 오름차순\n",
    "\n",
    "# 장르 유사도에 따라 영화를 추천하는 함수\n",
    "def find_sim_movie(df, sorted_ind, title_name, top_n=10):\n",
    "    title_movie = df[df['title'] == title_name]\n",
    "    title_index = title_movie.index.values\n",
    "    similar_indexes = sorted_ind[title_index, :(top_n)]\n",
    "    # top_n index는 2차원이므로 데이터프레임 index로 사용하기위해서 1차원 array로 변경\n",
    "    similar_indexes = similar_indexes.reshape(-1)\n",
    "    return df.iloc[similar_indexes]\n",
    "    \n",
    "similar_movies = find_sim_movie(movies_df, genre_sim_sorted_ind, 'The Godfather',5)\n",
    "similar_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3fe27-92b2-4b2d-b022-ff8a3af73af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#장르만으로 추천할때는 모두 같은 장르만 나오므로 추천시스템 문제가 있음 \n",
    "np.sort(genre_sim[2731])[::-1][:10] \n",
    "# 같은 장르 + 평점 높은 작품 먼저 추천 - 이것도 투표수 적을땐 객관성 없음\n",
    "movies_df[['title','vote_average','vote_count']].sort_values('vote_average', ascending=False)[:10]\n",
    "## 장르 유사도에 따른 영화 추천\n",
    "C = movies_df['vote_average'].mean()       # 전체 영화에 대한 평균평점\n",
    "m = movies_df['vote_count'].quantile(0.5)  # 평점을 부여하기 위한 최소횟수\n",
    "print('C:',round(C,3), ', m:',round(m,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e3213-85ac-4d85-b3e3-764054ea31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장르 유사도 함수(가중치 계산)\n",
    "percentile = 0.6\n",
    "m = movies_df['vote_count'].quantile(percentile) # 평점을 부여하기 위한 최소횟수\n",
    "C = movies_df['vote_average'].mean()             # 전체 영화에 대한 평균평점\n",
    "def weighted_vote_average(record):\n",
    "    v = record['vote_count']                     # 개별 영화에 평점을 투표한 수\n",
    "    R = record['vote_average']                   # 개별 영화에 대한 평균 평점\n",
    "    return ( (v/(v+m)) * R ) + ( (m/(m+v)) * C ) \n",
    "movies_df['weighted_vote'] = movies_df.apply(weighted_vote_average, axis=1)   # 칼럼 추가\n",
    "movies_df['weighted_vote'] \n",
    "\n",
    "# 가중치 변수로 추천영화 정렬\n",
    "movies_df[['title','vote_average','weighted_vote']].sort_values('weighted_vote', ascending = False)[:10]\n",
    "\n",
    "# 투표수로 정렬/ 투표수를 감안하지 않아서 점수는 높지만 문제 있음\n",
    "movies_df[['title','vote_average','weighted_vote']].sort_values('vote_average', ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1f0e8-fbe8-4214-ac73-0155c21b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 아이템 기반 최근접이웃 협업 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567b5f0-a6af-4856-903d-4f3c3993a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가공: NaN값 제거 등\n",
    "# 영화간 유사도 산출\n",
    "# 영화(아이템) 간 유사도 산출  : 평점만 가지고 유사도 산출\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "ratings_matrix_T = ratings_matrix.transpose()\n",
    "item_sim = cosine_similarity(ratings_matrix_T)\n",
    "item_sim_df = pd.DataFrame(data=item_sim, index=ratings_matrix.columns, columns=ratings_matrix.columns)\n",
    "# 가장 가까운(유사도) 영화 뽑아보기\n",
    "item_sim_df[\"Godfather, The (1972)\"].sort_values(ascending=False)[:6]\n",
    "\n",
    "# 개인 예측 평점 함수(가중치 계산)\n",
    "def predict_rating(ratings_arr, item_sim_arr ):\n",
    "    ratings_pred = ratings_arr.dot(item_sim_arr)/ np.array([np.abs(item_sim_arr).sum(axis=1)]) # 계산을 위해 행렬로 바꿈\n",
    "    return ratings_pred\n",
    "ratings_pred = predict_rating(ratings_matrix.values , item_sim_df.values)\n",
    "\n",
    "# 유저별 평점 예측 - 빈칸채우기 --> 실제 유저가 평가한 점수들과도 오차가 큼/ 그래도 의미가 있음\n",
    "ratings_pred_matrix = pd.DataFrame(data=ratings_pred, index= ratings_matrix.index, columns = ratings_matrix.columns)\n",
    "\n",
    "# 평가해보기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def get_mse(pred, actual):                         # actual 실제데이터/ 비어있는 상태\n",
    "    pred = pred[actual.nonzero()].flatten()        # 그래서 제로가 아닌 것들만 가져옴   \n",
    "    actual = actual[actual.nonzero()].flatten()     \n",
    "    return np.sqrt(mean_squared_error(pred, actual))  # 이미 루트값이므로 \n",
    "print('아이템 기반 모든 인접 이웃 RMSE: ', get_mse(ratings_pred, ratings_matrix.values ))\n",
    "\n",
    "# 예측 평점 계산, 실제 평점과의 MSE 구하기\n",
    "def predict_rating_topsim(ratings_arr, item_sim_arr, n=20):  # top 20개만    \n",
    "    pred = np.zeros(ratings_arr.shape)            # 유저-아이템 행렬만큼 0행렬 생성 \n",
    "    for col in range(ratings_arr.shape[1]):       # 아이템 개수 만큼 수행\n",
    "        top_n_items = [np.argsort(item_sim_arr[:, col])[:-n-1:-1]]  \n",
    "        for row in range(ratings_arr.shape[0]):       # 모든 유저에 대한\n",
    "            pred[row, col] = item_sim_arr[col, :][top_n_items].dot(ratings_arr[row, :][top_n_items].T) \n",
    "            pred[row, col] /= np.sum(np.abs(item_sim_arr[col, :][top_n_items]))  # /= 왼쪽 변수에서 오른쪽 값을 나누고 결과를 왼쪽변수에 할당\n",
    "    return pred\n",
    "ratings_pred = predict_rating_topsim(ratings_matrix.values , item_sim_df.values, n=10)\n",
    "print('아이템 기반 인접 TOP-10 이웃 RMSE: ', get_mse(ratings_pred, ratings_matrix.values ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf348034-a79e-4b84-af34-537bfab46736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 분해를 이용한 잠재요인 협업 필터링 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aefd28-21eb-4437-a771-032a7f667c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 잠재요인 협업 필터  : 차원축소의 잠재요인 (가중치), 경사하강법) ==> 모델기반 협업(알고리즘적 예측)\n",
    "# 잠재요인 협업필더링 실습 : 오차계산\n",
    "def get_rmse(R, P, Q, non_zeros): # P, Q 예측행렬/ R 실제값\n",
    "    error = 0\n",
    "    # 두개의 분해된 행렬 P와 Q.T의 내적 곱(np.dot)으로 예측 R 행렬 생성\n",
    "    full_pred_matrix = np.dot(P, Q.T) \n",
    "    # 실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출\n",
    "    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n",
    "    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n",
    "    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]\n",
    "    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n",
    "    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71377260-a2ab-4199-90a6-f37c2f3037ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률적 경사하강법 학습 함수/ 행렬분해 예제\n",
    "def matrix_factorization(R, K, steps=100, learning_rate=0.01, r_lambda = 0.01):\n",
    "    # 만약 데이터프레임이 들어오면\n",
    "    if type(R) == 'pandas.core.frame.DataFrame':\n",
    "        R = R.values\n",
    "\n",
    "    num_users, num_items = R.shape\n",
    "    # P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 랜덤한 값으로 입력합니다. \n",
    "    np.random.seed(1)\n",
    "    P = np.random.normal(scale=1./ K, size=(num_users, K)) \n",
    "    Q = np.random.normal(scale=1./ K, size=(num_items, K))  # 나중에 전치시킬 예정\n",
    "    break_count = 0\n",
    "    # R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트 객체에 저장. \n",
    "    non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ]  # 빈칸에 0넣기/ 0아닌 곳은 남기기\n",
    "    # 이전 코드에 이어서 작성\n",
    "    # SGD기법(차원축소)으로 P와 Q 매트릭스를 계속 업데이트.   \n",
    "    for step in range(steps):\n",
    "        for i, j, r in non_zeros:\n",
    "            # 실제 값(r)과 예측 값의 차이인 오류 값 구함\n",
    "            eij = r - np.dot(P[i, :], Q[j, :].T)\n",
    "            # Regularization을 반영한 SGD 업데이트 공식 적용\n",
    "            P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])  # P[i,:] 가중치  *** 중요\n",
    "            Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])  # Q[j,:]  가중치  ** 중요 \n",
    "            # // 둘다 가동하면 속도문제, 결과 문제있음: 번갈아가면서 실행하는 ASL기법(SGD와 비교) 있음\n",
    "        rmse = get_rmse(R, P, Q, non_zeros)\n",
    "        if (step % 10) == 0 :\n",
    "            print(\"### iteration step : \", step,\" rmse : \", rmse)\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05730b-afa1-4f72-b34f-77f095d8964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Q = matrix_factorization(ratings_matrix.values, K=50, steps=100, learning_rate=0.01, r_lambda = 0.01)\n",
    "pred_matrix = np.dot(P, Q.T)\n",
    "\n",
    "# 예측 평점 출력\n",
    "ratings_pred_matrix = pd.DataFrame(data=pred_matrix, index= ratings_matrix.index,columns = ratings_matrix.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c92e5e-02cd-44cf-9ffe-312c3f7a0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 추천\n",
    "# 9번 사용자가 관람하지 않는 영화명 추출\n",
    "unseen_list = get_unseen_movies(ratings_matrix, 9)\n",
    "# 아이템 기반의 인접 이웃 협업 필터링으로 영화 추천\n",
    "recomm_movies = recomm_movie_by_userid(ratings_pred_matrix, 9, unseen_list, top_n=10)\n",
    "# 평점 데이타를 DataFrame으로 생성. \n",
    "recomm_movies = pd.DataFrame(data=recomm_movies.values, index=recomm_movies.index, columns=['pred_score'])\n",
    "recomm_movies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
