(0) 가져오기  : 영어
with open('../data/stevejobs.txt', 'r', encoding='utf8') as f:
    rows = f.readlines()
    lines = [row for row in rows]
text = ' '.join(lines)

(1) 전처리: 클렌징, 토큰화(문장,단어),불용어 처리, 어근추룰, 벡터화 
1. 클렌징 : 정규표현식 사용-불필요한 문자, 기호 등을 사전에 제거 
import re
compile = re.compile("[^ a-zA-Z0-9\.]+")  
# ^ 아닌것(부정) 줄바꾸기 제거/ 문장별로 자르지않을거면 . 삭제
text = compile.sub('',text).lower()       
# 모두 소문자 처리/ # 이후에 .과 숫자 삭제여부 정해야

2. 문장 토큰화 라이브러리 nltk
import nltk         
nltk.download('punkt')                   
# nltk의 토큰아이즈 : sent_tokenize  
sentences = nltk.sent_tokenize(text = text)  
# 보통 마침표를 기준으로 자름/ ! ? 줄바꾸기 등으로 자르기도

3. 단어 토큰화
word_token = []
for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    word_token.extend(words)

### 텍스트 토큰화 함수
def tokenize_text(text):
    sentences = nltk.sent_tokenize(text)
    word_tokens = [nltk.word_tokenize(sentense) 
        for sentense in sentences]
    return word_tokens
word_tokens = tokenize_text(text)
print(word_tokens[:2])

4. 스톱워드 제거: 분석에 큰 의미없는 단러 제거
nltk.download('stopwords')      # 스톱워드(불용어) 사전
stopwords = nltk.corpus.stopwords.words('english')
print('영어 stop words 개수: ',len(stopwords))

### 스톱워드 제거
all_tokens = []
for sentence in word_tokens:
    filtered_words = []
    for word in sentence:
        if word not in stopwords:
            filtered_words.append(word)
    all_tokens.append(filtered_words)
print(all_tokens[:1])

5. 단어의 원형을 찾는 기법 
1) LancasterStemmer -표제어(어근) 추출: 조금 부실
from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()
print(stemmer.stem('working'))

(2) 피처 벡터화/추출
1. CountVectorizer: 벡터화 할 때 한꺼번에 다 처리
 : (사전데이터가공/토큰화/텍스트 정규화,피처 벡터화)
from sklearn.datasets import fetch_20newsgroups
news_data = fetch_20newsgroups(subset = 'all', random_state = 156)
train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)
X_train = train_news.data
y_train = train_news.target
X_train = train_news.data

from sklearn.feature_extraction.text import CountVectorizer
cnt_vect = CountVectorizer()
cnt_vect.fit(X_train)
X_train_cnt_vect = cnt_vect.transform(X_train)
X_test_cnt_vect = cnt_vect.transform(X_test)
print('학습 데이터 Text의 CountVectorizer Shape:', X_train_cnt_vect.shape)

2. TfidfVectorizer: 벡터화 할 때 한꺼번에 다 처리
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )
tfidf_vect.fit(X_train)
X_train_tfidf_vect = tfidf_vect.transform(X_train)
X_test_tfidf_vect = tfidf_vect.transform(X_test)
lr_clf = LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_tfidf_vect , y_train)
pred = lr_clf.predict(X_test_tfidf_vect)
print('예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))

(3) 머신러닝 모델 수립 및 학습/예축/평가
lr_clf = LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_cnt_vect , y_train)
pred = lr_clf.predict(X_test_cnt_vect)
print('예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))

