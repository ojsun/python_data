<텍스트 분류>

(0) 가져오기  : 영어
with open('../data/stevejobs.txt', 'r', encoding='utf8') as f:
    rows = f.readlines()
    lines = [row for row in rows]
text = ' '.join(lines)

(1) 전처리(정규화): 클렌징, 토큰화(문장,단어),불용어 처리, 어근추룰, 벡터화 
1-1 클렌징 : 정규표현식 사용-불필요한 문자, 기호 등을 사전에 제거 
import re
compile = re.compile("[^ a-zA-Z0-9\.]+")  
        # ^ 아닌것(부정) 줄바꾸기 제거/ 문장별로 자르지않을거면 . 삭제
text = compile.sub('',text).lower()       
        # 모두 소문자 처리/ # 이후에 .과 숫자 삭제여부 정해야

1-2-1. 문장 토큰화 라이브러리 nltk
import nltk         
nltk.download('punkt')  # 마침표, 개행문자 등의 데이터셋트                  
        # nltk의 토큰아이즈 : sent_tokenize  
sentences = nltk.sent_tokenize(text = text)  
        # 보통 마침표를 기준으로 자름/ ! ? 줄바꾸기 등으로 자르기도

1-2-2. 단어 토큰화
word_token = []
for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    word_token.extend(words)       # append하면 2차원데이터

1-2-3. 텍스트 토큰화 함수
from nltk import sent_tokenize, word_tokenize
def tokenize_text(text):
    sentences = nltk.sent_tokenize(text)
    word_tokens = [nltk.word_tokenize(sentense) 
        for sentense in sentences]
    return word_tokens
word_tokens = tokenize_text(text)
print(word_tokens[:2])

1-3. 불용어 제거: 분석에 큰 의미없는 단러 제거
nltk.download('stopwords')      # stopword(불용어) 사전
stopwords = nltk.corpus.stopwords.words('english')

all_tokens = []
for sentence in word_tokens:
    filtered_words = []
    for word in sentence:
        if word not in stopwords:
            filtered_words.append(word)
    all_tokens.append(filtered_words)
print(all_tokens[:1])

5. 단어의 원형을 찾는 기법 
1) LancasterStemmer -표제어(어근) 추출: 조금 부실
from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()
print(stemmer.stem('working'))

(2) 피처 벡터화/추출(BOW : CountVectorizer/TfidfVectorizer)
2-1. 카운트 기반의 벡터화: CountVectorizer
파라미터: max_df 높은 빈도수 허용치/min_df 낮은 빈도수 제외치/max_features 허용 피처 개수/
         n_gram_range 튜틀형태 단어갯수/ token_pattern 정규표현식 패턴 지정 /stop_words

# 셋트 만들기
from sklearn.datasets import fetch_20newsgroups
train_news=fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'), random_state=156)
X_train=train_news.data
y_train=train_news.target
test_news=fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'), random_state=156)
X_test=test_news.data
y_test=test_news.target

# 벡터화
from sklearn.feature_extraction.text import CountVectorizer
cnt_vct=CountVectorizer()
cnt_vct.fit(X_train)   # y값 없음!!
X_train_cnt_vct=cnt_vct.transform(X_train)
X_test_cnt_vct=cnt_vct.transform(X_test)

#(3)모델로 분류
from sklearn.linear_model import LogisticRegression
lr_clf=LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_cnt_vct, y_train)
pred=lr_clf.predict(X_test_cnt_vct)
print('예측 정확도 : ', accuracy_score(y_test, pred))

2-2. TF-IDF: TfidfVectorizer
      (자주 나오는 단어에 가중치/모든 문서에서 자주나오는 단어에 패럴티)
from sklearn.feature_extraction.text import TfidfVectorizer
tfdf_vct=TfidfVectorizer(max_df = 0.85, stop_words='english',
         max_features=3000,ngram_range=(1,2),token_pattern='[a-zA-Z]+')
tfdf_vct.fit(X_train)
X_train_tfdf_vct=tfdf_vct.transform(X_train)
X_test_tfdf_vct=tfdf_vct.transform(X_test)
#(3)
from sklearn.linear_model import LogisticRegression
lr_clf=LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_tfdf_vct, y_train)
pred=lr_clf.predict(X_test_tfdf_vct)
print('예측 정확도 : ', accuracy_score(y_test, pred))

# GridSearchCV로 파라미터 성능 향상
params={'C':[0.01, 0.1, 1, 5, 10]}
gr_lr_clf = GridSearchCV(lr_clf,param_grid=params,cv=3,scoring='accuracy', verbose=1)
gr_lr_clf.fit(X_train_tfdf_vct , y_train)
print('LogisticRegression의 최적 파라미터: ', gr_lr_clf.best_params_)

# 최적 C값으로 예측, 정확도 평가
pred=gr_lr_clf.predict(X_test_tfdf_vct)
print('예측 정확도 : ', accuracy_score(y_test, pred))

#(3) pipeline으로 만들기:TF-IDF 벡터화, GridSearchCV 최적찾기
#X_train 값, model들 import 등 해놔야 작동

from sklearn.pipeline import Pipeline
pipeline=Pipeline([('tfdf_vect', TfidfVectorizer(stop_words='english')),
                   ('lr_clf', LogisticRegression())])
params={'tfdf_vect__ngram_range': [(1,1),(1,2),(1,3)],
        'tfdf_vect__max_df': [100,300,700],
        'lr_clf__C': [1,5,10]}
grid_cv_pipe=GridSearchCV(pipeline, param_grid=params,cv=3,scoring='accuracy')
grid_cv_pipe.fit(X_train,y_train)
print('최적 파라미터: ', grid_cv_pipe.best_params_)
pred=gr_lr_clf.predict(X_test)
print('예측 정확도 : ', accuracy_score(y_test, pred))

-----------------
<한글 텍스트 분석>
#불러오기
with open('../data/소나기.txt', 'r', encoding='utf8') as f:
    text = f.read()
print(text[:200])

# 1.텍스트 전처리(정규화): 클렌징
import re
compile = re.compile("[^ ㄱ-ㅣ가-힣\.]+")
text = compile.sub('',text)
print(text[:200])

#2-1. 문장 토큰화
import nltk
sentences = nltk.sent_tokenize(text)

#2-2. 단어 토큰 
from konlpy.tag import Okt
okt = Okt()
words = []
for sentence in sentences:
    word = okt.morphs(sentence)
    words.append(word)

print(words[:2])
print('normalize :', okt.normalize(text)) # 문장으로 추출
print('morphs :', okt.morphs(text))       # 구문 분석
print('nouns :', okt.nouns(text))         # 명사만
print('phrases :', okt.phrases(text))     # 구문
print('pos :', okt.pos(text))             # 품사와 함께 출력


