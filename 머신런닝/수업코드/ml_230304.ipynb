{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd792d-b7ee-4aa1-a0a1-83ac59da76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b6a88-bfc0-48c3-ab51-0ec1cc11dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fe27c-5448-4f41-88ea-56acf6904a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95092148-a173-4576-aa01-19223ceddd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클렌징 1: 불필요한 문자, 기호 등을 사전에 제거 \n",
    "with open('../data/stevejobs.txt', 'r', encoding='utf8') as f:\n",
    "    rows = f.readlines()\n",
    "    lines = [row for row in rows]\n",
    "text = ' '.join(lines)\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae42d6-aac0-492d-a9fb-44428cd57980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클렌징 : 정규표현식 사용-불필요한 문자, 기호 등을 사전에 제거 \n",
    "import re\n",
    "compile = re.compile(\"[^ a-zA-Z0-9\\.]+\")  # ^ 아닌것(부정) 줄바꾸기 제거/ 문장별로 자르지않을거면 . 삭제\n",
    "text = compile.sub('',text).lower()       # 모두 소문자 처리\n",
    "print(text[:200])                         # 이후에 .과 숫자 삭제여부 정해야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048fb9a-72a8-4471-92f2-686c65fa9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화 라이브러리 nltk\n",
    "import nltk         \n",
    "nltk.download('punkt')                   # nltk의 토큰아이즈 : sent_tokenize  \n",
    "sentences = nltk.sent_tokenize(text = text)\n",
    "print(sentences[:3])            # 보통 마침표를 기준으로 자름/ ! ? 줄바꾸기 등으로 자르기도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f5a1d-e56a-445d-99ee-604eaf6c3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 토큰화\n",
    "word_token = []\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    word_token.extend(words)\n",
    "print(word_token[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fafbdc-be77-4a12-8179-c822a1a912e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 전체를 넣고 자르기 가능\n",
    "sentence=nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d9e1c-7773-4f27-9c64-35d47225c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 문장처럼, 문장을 단어처럼 분석하는 경우도 있음 ?\n",
    "word_token2 = []\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    word_token2.extend(words)\n",
    "print(word_token2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25d53a-d427-4587-aa9e-c42778b8acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize('로렘 입숨은 출판이나 그래픽 디자인 분야에서 폰트, 타이포그래피, 레이아웃 같은 그래픽 요소나 시각적 연출을 보여줄 때 사용하는 표준 채우기 텍스트로, 최종 결과물에 들어가는 실제적인 문장 내용이 채워지기 전에 시각 디자인 프로젝트 모형의 채움 글로도 이용된다. 이런 용도로 사용할 때 로렘 입숨을 그리킹(greeking)이라고도 부르며, 때로 로렘 입숨은 공간만 차지하는 무언가를 지칭하는 용어로도 사용된다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a348c-132e-4de7-b0bf-952470afcd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스톱워드 제거: 분석에 큰 의미없는 단러 제거\n",
    "nltk.download('stopwords')      # 스톱워드(불용어) 사전\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print('영어 stop words 개수: ',len(stopwords))\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e7e30-39ca-4c5f-908d-1b7a83e51816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 토큰화 함수\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentense) \n",
    "        for sentense in sentences]\n",
    "    return word_tokens\n",
    "word_tokens = tokenize_text(text)\n",
    "print(word_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f8a3d-19c6-4e5d-aa13-ed4ad0e5427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스톱워드 제거\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "print(all_tokens[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b43ca7-dd23-4f7a-abdf-314467669c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.append('.')   #  . 불용어사전에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90466457-185a-4d69-9736-c75d00ca799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 원형을 찾는 기법 1) LancasterStemmer -표제어(어근) 추출: 조금 부실\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'))\n",
    "print(stemmer.stem('works'))\n",
    "print(stemmer.stem('worked'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de252d4e-450e-42ef-b391-29fe568604cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 단어의 원형을 찾는 기법 2) WordNetLemmatizer-표제어(어근) 추출: 좀 더 잘 작동\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','a')) \n",
    "print(lemma.lemmatize('amuses','a')) \n",
    "print(lemma.lemmatize('amused','v'))\n",
    "\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(lemma.lemmatize('happier','a'))\n",
    "print(lemma.lemmatize('happiest','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604b327-3d9c-4ba8-a21f-ef596cdee5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = []\n",
    "for i, sentence in enumerate(all_tokens):\n",
    "    for j, word in enumerate(sentence):\n",
    "       all_token[i][j]= stemmer.stem(word)\n",
    "print(all_token[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1ca86-4ed6-4e40-8671-bcc47fa0ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 너무 오래 걸려 안함/ 피처 벡터화: 데이터 가공/ 토큰화/텍스트 정규화/ 피처 벡터화/ \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "news_data = fetch_20newsgroups(subset = 'all', random_state = 156)\n",
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055af9d-f642-4a7b-90e6-e77660e6e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 돌림\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23a3ff-6e53-469a-9935-5bcbd4456359",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news= fetch_20newsgroups(subset='test',remove=('headers', 'footers','quotes'), random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print(f'학습 데이터 크기 {len(train_news.data)}, 테스트 데이터 크기 {len(test_news.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c67356-f30b-49dc-a1ff-ce989301c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CountVectorizer: 벡터화 할 때 한꺼번에 다 처리\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "print('학습 데이터 Text의 CountVectorizer Shape:', X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf49204-160f-4093-a092-c37d5d4848da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0591e3-3065-48f5-8e3a-203c7f79f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.Series(X_train)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8e3be-513e-46f2-92ea-7b5af22027ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "string=''.join(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc885ff-2f77-4f56-82b7-cf61ab46aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile = re.compile(\"[^ a-zA-Z0-9\\.]+\") \n",
    "text=compile.sub('',text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806d941-9aa4-4b63-ab1e-e9d8a855716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(string)\n",
    "word_series=pd.Series(words)\n",
    "print(len(word_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670078ba-3f06-4799-86b0-3b494f2df16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnt_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318546d9-60bb-4218-87a9-b873ba28471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect.get_feature_names_out()  # 벡터화 후 다시 되돌릴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba1ce9-9cf2-44ab-81b2-b5b72c81f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.target_names[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce628d9-8c11-4161-a47f-8643ca285a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect.inverse_transform(X_train_cnt_vect.toarray) # ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa5b5a-f0ac-4e6e-9a30-3ce86b5f40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31599443-1f58-47e0-939a-bbf0b3fe9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect1= CountVectorizer(ngram_range=(1,2), max_features=3000)\n",
    "cnt_vect1.fit(X_train)\n",
    "X_train_cnt_vect=cnt_vect1.transform(X_train)\n",
    "cnt_vect1.get_feature_names_out()[110:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fbe9f-2d3e-4384-bd12-623ad64aef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f03689-4a90-462f-8a0a-b2e962fd98a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect= CountVectorizer(max_df=0.85, min_df=0.05, ngram_range=(1,2))\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect=cnt_vect.transform(X_train)\n",
    "X_train_cnt_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8d90c-fa82-43b9-8e0b-bdaf113052f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect.get_feature_names_out()[110:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf287b-64f9-4d28-9de9-bdff453e6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TfidfVectorizer: 벡터화 할 때 한꺼번에 다 처리\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448df879-7078-4fb2-aa6c-2bbefa5d2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_df=0.85,stop_words='english', ngram_range=(1,2), max_df=3000,token_pattern=[a-zA-Z0-9\\.])\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_train_tfidf_vect\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcd60f-8fb9-45e6-b08b-d9a8a674252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.85, stop_words='english', max_features=3000, ngram_range=(1,2))\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780e88a-d2f5-436f-a6fb-2081157adf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=X_train_tfidf_vect,columns=tfidf_vect.get_feature_names_out())   # ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd5ddd-922b-473f-880d-d878c272952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile = re.compile(\"[^ a-zA-Z0-9\\.]+\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaad77a-d7a6-47ac-976b-6bd439eeb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect.get_feature_names_out()[110:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe80f5-90b9-4154-aca0-e6b5056c87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = { 'C':[0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, \n",
    "scoring='accuracy', verbose=True )\n",
    "grid_cv_lr.fit(X_train_tfidf_vect , y_train)\n",
    "print('Logistic Regression best C parameter :', grid_cv_lr.best_params_ )\n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae29fb5-8c50-4257-99b3-c7dbfe0592c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "('lr_clf', LogisticRegression(solver='liblinear'))])\n",
    "params = {'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "'tfidf_vect__max_df': [100, 300, 700],\n",
    "'lr_clf__C': [1, 5, 10]}\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3,\n",
    "scoring='accuracy', verbose=True)\n",
    "grid_cv_pipe.fit(X_train , y_train)\n",
    "print(grid_cv_pipe.best_params_, grid_cv_pipe.best_score_)\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print('예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5a91c-90ac-432c-a2f6-6666211abbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fbdbc13-48b8-4e96-80f7-9d4da70cb7f6",
   "metadata": {},
   "source": [
    "#### 한글 텍스트 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e9892-9681-4a01-a856-2d6cee72f73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a1c85-c9c4-42c3-be78-1863fac32cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/소나기.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099da88-e19b-4312-91c2-986b464a84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "compile = re.compile(\"[^ ㄱ-ㅣ가-힣\\.]+\")\n",
    "text = compile.sub('',text)\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31d21b-c8e7-426c-8223-c4ec62d56669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "import nltk\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76634b60-4e82-467b-ba99-323cb1fcb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca30a6c-6a7c-41dc-96e1-fee6fe8d68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    word = okt.morphs(sentence)\n",
    "    words.append(word)\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c151d18-7d19-42b1-877e-07a795bf6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(text)   # 형태소, 조사 제거 안되므로 konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5b8b7085-537f-440b-ba47-926dba0c19a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ok_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     tokens_ko \u001b[38;5;241m=\u001b[39m okt\u001b[38;5;241m.\u001b[39mmorphs(text, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens_ko\n\u001b[1;32m----> 5\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m ok_tokenizer(text)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_tokens[:\u001b[38;5;241m20\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ok_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "def okt_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text, stem=True)\n",
    "    return tokens_ko\n",
    "    \n",
    "word_tokens = okt_tokenizer(text)\n",
    "print(word_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c605e863-72f2-4c3b-9e33-62a4c56ea509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['소년', '은', '개울가에서', '소녀', '를', '보자', '곧', '윤', '초시', '네', '증손녀', '딸', '이라는', '걸', '알', '수', '있었다', '.'], ['소녀', '는', '개울', '에다', '손', '을', '잠그고', '물장난', '을', '하고', '있는', '것', '이다', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    word = okt.morphs(sentence)\n",
    "    words.append(word)\n",
    "print(words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a2df32e9-e7f1-4ffa-8823-415c77746eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize : 나는 정말로 파이썬을 좋아한다. 아니 머신러닝을 더 좋아한다.\n",
      "morphs : ['나', '는', '정말로', '파이썬', '을', '좋아한다', '.', '아니', '머신', '러닝', '을', '더', '좋아한다', '.']\n",
      "nouns : ['나', '파이썬', '머신', '러닝', '더']\n",
      "phrases : ['파이썬', '머신러닝', '머신', '러닝']\n",
      "pos : [('나', 'Noun'), ('는', 'Josa'), ('정말로', 'Adverb'), ('파이썬', 'Noun'), ('을', 'Josa'), ('좋아한다', 'Adjective'), ('.', 'Punctuation'), ('아니', 'Adjective'), ('머신', 'Noun'), ('러닝', 'Noun'), ('을', 'Josa'), ('더', 'Noun'), ('좋아한다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "test_text = '나는 정말로 파이썬을 좋아한다. 아니 머신러닝을 더 좋아한다.'\n",
    "print('normalize :', okt.normalize(test_text)) # 문장으로 추출\n",
    "print('morphs :', okt.morphs(test_text))       # 구문 분석\n",
    "print('nouns :', okt.nouns(test_text))         # 명사만\n",
    "print('phrases :', okt.phrases(test_text))     # 구문\n",
    "print('pos :', okt.pos(test_text))             # 품사와 함께 값고 함께 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a082b96-4b3c-4da5-a5aa-739703dd09f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594ea32-0675-4b36-bfbd-7622f0200260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1265d2-6f55-4d39-84f2-82ee7b42de60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3affe1c-f83e-4213-a2fd-c6a960e88b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15739339-396f-47d9-9176-6ec8ffa9a948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0334b7-3545-4d03-8b47-8088044fa5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539e2fd-c80e-49bf-ae85-dc2b460292c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a20ef-bd94-48d9-8a0a-0269be3e324d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132091a-58be-4d62-b236-396f917e2226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e5c8d-b8dd-47d4-998e-a422e9d0d456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadf607-649a-459f-939a-fe69cc8d5002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2dced-ec14-4f27-872e-e25b4c805c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
