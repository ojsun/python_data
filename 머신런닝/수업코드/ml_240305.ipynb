{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279ba51-3516-4f90-8821-a5a9f65fe06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2d22f-731f-4f06-bd97-9565cd80e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff218a42-9e81-4e24-ac04-d1fdee02a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from konlpy.tag import Okt, Hannanum, Komoran, Kkma\n",
    "from nltk.stem import LancasterStemmer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0f084-2814-4498-9db4-1f3e653813de",
   "metadata": {},
   "source": [
    "#### 한글 텍스트 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfc02c-22a4-4f5a-abf9-2dcb2bbf0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/소나기.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607458dd-f2da-4cf1-ba3a-70da63a1af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "compile = re.compile(\"[^ ㄱ-ㅣ가-힣\\.]+\")\n",
    "text = compile.sub('',text)\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b94423-1021-4f10-a657-712225d4c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "import nltk\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854259d-da6e-456d-8cec-cac56f677ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb824b-61a3-4388-97c7-4f21646cef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    word = okt.morphs(sentence)\n",
    "    words.append(word)\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a041cc6-fc27-41d1-a610-e557d790bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(text)   # 형태소, 조사 제거 안되므로 konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf9c77-c446-47ac-afed-34d47f7078db",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "def okt_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text, stem=True)\n",
    "    return tokens_ko\n",
    "word_tokens = okt_tokenizer(text)\n",
    "print(word_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eca9b7-0bde-49c9-8dc6-0e33e9bbd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    word = okt.morphs(sentence)\n",
    "    words.append(word)\n",
    "print(words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885fa98-81a9-4e66-91dc-2fafac25fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = '나는 정말로 파이썬을 좋아한다. 아니 머신러닝을 더 좋아한다.'\n",
    "print('normalize :', okt.normalize(test_text)) # 문장으로 추출\n",
    "print('morphs :', okt.morphs(test_text))       # 구문 분석\n",
    "print('nouns :', okt.nouns(test_text))         # 명사만\n",
    "print('phrases :', okt.phrases(test_text))     # 구문\n",
    "print('pos :', okt.pos(test_text))             # 품사와 함께 값고 함께 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59da14-ad00-4db8-8f7d-e58081fd4a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06385b77-aca9-4ebd-9ed3-a628e1eb116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- 수업시간에 ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59193033-7b33-4ae6-88f4-630b3de3d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('morphs :', okt.morphs(text,stem=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906597a-6b60-46a3-9cc6-92c6c5984917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('morphs :', okt.pos(test_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948215d-b896-4e01-9dc9-9c94b459b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "with open('../data/stopword.txt','r',encoding='utf-8') as f:\n",
    "    word = f.read()\n",
    "stopwords = word.split('\\n')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1aed6d-decb-4b22-97d7-9f6f9834153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stopwords(words, Stopwords = None):\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords: \n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "filtered_words = Stopwords(word_tokens, stopwords)\n",
    "print(filtered_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc553b-9e06-4e23-96f3-5c2e845e60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hah=Hannanum()\n",
    "han_stopwords=[]\n",
    "for word in stopwords:\n",
    "    han_stopwords.expend(han.morphs(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee8969-f3e8-459d-bbc3-213adcf35930",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = []\n",
    "for word in word_tokens:\n",
    "    if word not in stopwords:\n",
    "        filtered_words.append(word)\n",
    "print(filtered_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bbf9f-02da-4cc6-a82f-57beb98d6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../data/petition.csv')\n",
    "data=df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cc3e8-fed5-4533-8f23-b008e55d244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vect = CountVectorizer(tokenizer=okt_tokenizer, token_pattern='[ㄱ-| 가-힣]+',\n",
    "                           stop_words=stopwords)\n",
    "cnt_vect.fit(data)\n",
    "words_cnt_vect = cnt_vect.transform(data)\n",
    "words_cnt_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b844e1b-dd2f-43c5-aeae-3895c364b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 너무 길어서 골라 쓸 예정 : 결국  5줄\n",
    "result=cnt_vect.fit_transform(data['content'])\n",
    "pd.DataFrame(result.toarray(), columns=cnt_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e461482-09c2-47f3-a6d1-5105ba84d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab5313-b1b6-4673-a008-712104682c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect = CountVectorizer(tokenizer=okt_tokenizer, token_pattern='[ㄱ-| 가-힣]+',\n",
    "                           stop_words=stopwords)\n",
    "cnt_vect.fit(data)\n",
    "words_cnt_vect = cnt_vect.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ab91d-4c09-4370-b79d-9d5f23aee274",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=cnt_vect.fit_transform(data['content'])\n",
    "pd.DataFrame(result.toarray(), columns=cnt_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcf623-f3cf-4582-bf03-3fe726c0bd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20c1a3-5919-42c1-900d-596e199d7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../data/petition.csv')\n",
    "data=df.head()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5bdba-a9fb-4adf-a191-88e59d144db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafeb58e-7e62-49d4-b22f-a4e926cfe180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02db79-bef1-4f41-838d-3e19dfcdfe59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['category'].value_counts()  # 문화.예술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee3c45-7649-4467-92dc-566dbe479d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].isin(['문화/예술/체육/언론', '미래', '경제민주화'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9149af0-3ab1-4ce2-a651-ae308075f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5ca68-7ffa-469f-980a-0c6f6e9bbe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_data = df[['content', 'category']]\n",
    "lb_enc = LabelEncoder()\n",
    "df_data['category'] = lb_enc.fit_transform(df_data['category'])\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954962d8-bdd1-4f49-a834-4ca14ce03612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=df_data.sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7e667-92be-420c-afd2-3a56a1491e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(df_data['content'], df_data['category'], test_size = 0.2, random_state = 0)\n",
    "print(f'학습 데이터 수:{len(X_train)}, 평가 데이터 수:{len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8adc00-a542-4441-82a0-9b1187ceed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaning(text):\n",
    "    p = re.compile(\"[^ ㄱ-ㅣ가-힣\\.]+\")\n",
    "    result = p.sub('',text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fac2f3-d3ae-464d-a918-151169f2f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feab208-19e6-4962-9f8b-fba1b84752a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c0d40-3a69-4d9e-9014-0c9819c9c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean=X_train.apply(cleaning)\n",
    "X_test_clean=X_test.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608998c8-9c54-4506-bb1a-ed5c9244d047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0e4da-76bc-46c5-b177-8bc946df1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "words = []\n",
    "def okt_token(text):\n",
    "    resuit=okt.morphs(text, stem=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a89674-f280-477d-bc06-46acff978faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect=CountVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb2cf0-e7e1-40c1-ae86-25e1fd847fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간때문에 줄임\n",
    "cnt_vect.fit(X_train_clean)   \n",
    "X_train_cnt=cnt_vect.transform(X_train_clean)\n",
    "X_test_cnt=cnt_vect.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0889f-dbe1-4538-b6fd-c03b5ab43346",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_cnt_vct.toarray(), columns=cnt_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53a287-e2fe-4cef-b6cb-579001029d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_cnt_vct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706a87d-bdff-42fa-ad9c-950bcb639f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vct, y_train)\n",
    "pred=lr_clf.predict(X_test_cnt_vct)\n",
    "print('예측 정확도 : ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af0201-3184-4d32-a4a0-d687936aad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline : 같은 모양만 맞추면 돌아감// 다른 file 적용시 import 등 필요없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f9d23-367d-4338-bfd5-9cd5ce382943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline을 안쓴 상태로 예측하는 서비스 **\n",
    "\n",
    "text= '다.'\n",
    "text_trans=cnt_vect.transform([text])\n",
    "result=lr_clf.predict(text_trans)\n",
    "final_res=lb_enc.inverse_transform(result)\n",
    "print(final_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a2ce7-5ad7-416f-9a70-b30782c7a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인을 이용하여 예측하는 서비스  **\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipeline=Pipeline([('vect', TkidfVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)),\n",
    "                   ('lr_clf', LogisticRegression())])\n",
    "# params={'vect__ngram_range': [(1,1),(1,2)],\n",
    "#         'vect__max_df': [100,200,300],\n",
    "#         'lr_clf__C': [1,5,7]}\n",
    "Tkidf_vect=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ab904-e5d9-4b47-9898-3eeff6ff8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= '우리 아이 유치원 '\n",
    "result= pipe.predict([text])\n",
    "\n",
    "classes_ =['문화/예술/체육/언론', '미래', '경제민주화']\n",
    "classes_[result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579ad1d-5cae-4eb1-901d-1e45ab2ea9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vct=TfidfVectorizer(max_features=500, tokenizer=okt_tokenizer, stop_words=stopwords)\n",
    "tfidf_vct.fit(X_train)\n",
    "X_train_tfidf=tfidf_vct.transform(X_train)\n",
    "X_test_tfidf=tfidf_vct.transform(X_test)\n",
    "lr_clf=LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ac8d5-856c-4f9d-9146-84f9d6f30f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7acf1-26cf-4fcc-ac5f-747eed386e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/네이버 스마트스토어 리뷰.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ef909-1b6e-479f-955b-4de0e305af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99ec18-0957-4114-9276-ccdfc75cb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('../data/labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)\n",
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c264254-cbdc-4d3e-acb0-fc31dc1797b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220cd6b5-faf3-4acd-a874-47b2902bc1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    p=re.compile('[^ a=zA-Z]+')\n",
    "    return p.sub('',text)\n",
    "\n",
    "review_df['review'] =review_df['review'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb04f12-c867-4a5b-a4b6-c37117409c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer=LancasterStemmer()\n",
    "\n",
    "words=nltk.word_tokenize(text)\n",
    "print(word)\n",
    "#stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "filtered_words=[]\n",
    "# def **\n",
    "for word in words:\n",
    "    word= word.lower()\n",
    "    word=stemmer.stem(word)\n",
    "    if word not in stopwords:\n",
    "        filtered_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1d312-752b-42b7-bb5c-afa2574f3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(review_df['review'],review_df['sentiment'], test_size=0.2, random_state=0 ) \n",
    "\n",
    "\n",
    "# 벡처화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vct=CountVectorizer()\n",
    "cnt_vct.fit(X_train)   # y값 없음!!\n",
    "X_train_cnt_vct=cnt_vct.transform(X_train)\n",
    "X_test_cnt_vct=cnt_vct.transform(X_test)\n",
    "\n",
    "# 3.모델로 분류\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf=LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt_vct, y_train)\n",
    "pred=lr_clf.predict(X_test_cnt_vct)\n",
    "print('예측 정확도 : ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f837e-b4fc-4db4-8507-97dc0ac42ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 평가 분석\n",
    "review_df = pd.read_csv('../data/labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)\n",
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ef48d-c5a9-4212-8b9e-13a1e14f4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
    "review_df['review'] = review_df['review'].apply( lambda x : re.sub(\"[^a-zA-Z]\", \" \", x) )\n",
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5b0e8-d0de-4a4c-853f-7f258703ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df.drop(['id','sentiment'], axis=1)\n",
    "X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.3, random_state=156)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc956855-b56b-4f29-88ce-36ee46e44f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "pipeline = Pipeline([('cnt_vect', CountVectorizer(stop_words='english', \n",
    "                                                  ngram_range=(1,2) )),('lr_clf', LogisticRegression(solver='liblinear', C=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb97c1-f5c4-4b57-bfa8-369461924f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "print(f'예측 정확도는 {accuracy_score(y_test ,pred):.4f}')\n",
    "print(f'ROC-AUC는 {roc_auc_score(y_test, pred_probs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09aebc7-acec-449a-8994-eb1ea6826769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2) )),('lr_clf', LogisticRegression(solver='liblinear', C=10))])\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "print(f'예측 정확도는 {accuracy_score(y_test ,pred):.4f}')\n",
    "print(f'ROC-AUC는 {roc_auc_score(y_test, pred_probs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3689e-ca70-43e6-80cb-19d1f32a988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2번째 감성분석 사례: 한글 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b752dc7-af12-4703-814d-05fdec05130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/네이버 스마트스토어 리뷰.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa490c-6353-4cf7-96d3-81961fc83c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaning(text):\n",
    "    p = re.compile(\"[^ ㄱ-ㅣ가-힣\\.]+\")\n",
    "    result = p.sub('',text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd446c31-1b9e-477e-9487-61604be38e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content']=df['content'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c0b20-ae9e-441b-b044-7d1b4246679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(df['content'],df['score'], test_size=0.2, random_state=0 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15c526-2080-4a24-a59f-6c2bba5567f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "def okt_tokenizer(text):\n",
    "    word = okt.morphs(text, stemm=True )\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fd32b-4349-44c3-b194-a04a5c90cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vct=TfidfVectorizer(tokenizer=okt_tokenizer, stop_words=stopwords, max_features=5000)\n",
    "tfidf_vct.fit(X_train)\n",
    "X_train_tf=tfidf_vct.transform(X_train)\n",
    "X_test_tf=tfidf_vct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad4136-a434-4f9e-9bb5-38a27169060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf=LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tf, y_train)\n",
    "pred=lr_clf.predict(X_test_tf)\n",
    "print('예측 정확도 : ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9b66b-b7f2-4b60-bfc0-9efb3eaa7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA 토핑모델링\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e45fe9-c98c-4c0a-96a7-e98b94b59723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', \n",
    "    'comp.windows.x', 'talk.politics.mideast', \n",
    "    'soc.religion.christian', 'sci.electronics', 'sci.med' ]\n",
    "news_df= fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=cats, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb45839-25fd-4923-b41e-67c8e0d6efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_df=0.95,max_features=1000, min_df=2,stop_words='english', token_pattern = '[a-zA-Z]+',ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print('CountVectorizer Shape:', feat_vect.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c84c88-f8c3-43c6-8f4f-e1e8cc4de67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)\n",
    "print(lda.components_.shape)\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15d618-b1a0-4ab1-a3b7-39d69c01dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #',topic_index)\n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
    "        print(feature_concat)\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b02c1c-18b9-4b3a-aba1-9ab198d7e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/petition.csv')\n",
    "cats = ['정치개혁', '인권/성평등', '안전/환경', '교통/건축/국토', '육아/교육']\n",
    "df_cats = df[df['category'].isin(cats)]\n",
    "df_samples = df_cats.sample(frac=0.05, random_state = 0)\n",
    "df_samples['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a75ca-a74b-4d60-a1d2-40d38aabaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "def okt_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text, stem = True)\n",
    "    return tokens_ko\n",
    "with open('../data/stopword.txt','r',encoding='utf-8') as f:\n",
    "    word = f.read()\n",
    "stopwords = word.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d6c7d-1b3e-4ede-a1a6-cee0428173c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df = 0.9,max_features = 1000,min_df = 2, ngram_range = (1, 2),tokenizer = okt_tokenizer,stop_words = stopwords)\n",
    "df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])\n",
    "df_tfidf_vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656e85d-a114-4f4e-af28-b01b63f162bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 5, random_state = 0)\n",
    "lda.fit(df_tfidf_vect)\n",
    "print(lda.components_.shape)\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a32e3-5908-43ac-8d38-dfc899e6f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "display_topics(lda, feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51097b-edf2-4520-8d6d-310a9b5dcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 축소(요약)\n",
    "df = pd.read_csv('../data/petition.csv')\n",
    "# df_cat=df[df['category']=='육아/교육']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef714f3c-bbbc-4e91-816e-ebea93fab4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat['content']= df_cat['content'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f9227-8636-41a9-9a6e-96a41dd354b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 주제에서 주요 주제 뽑기\n",
    "cats = ['육아/교육']\n",
    "df_cats = df[df['category'].isin(cats)]\n",
    "df_samples = df_cats.sample(frac=0.5, random_state = 0)\n",
    "df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487696d7-ad62-47c9-8bb7-9eaf32ec88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_df = 0.9,max_features = 1000,min_df = 2, ngram_range = (1, 2),tokenizer = okt_tokenizer,stop_words = stopwords)\n",
    "df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])   # 중요 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc1a34-7aa8-4546-a342-f0e65a805b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.de composition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 5, random_state = 0)    # 중요 **\n",
    "lda.fit(df_tfidf_vect)\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cee1c0-758d-401d-b83d-2cd668282462",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.argsort()[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fcbfad-f706-41aa-bb19-85464806240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=pd.DataFrame(lda.components_, columns=tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83def507-9c9c-4c5e-bd60-c9fffd3ec64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMrans 군집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d52f0-88a1-42c5-b048-b093980b68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/petition.csv')\n",
    "cats = ['안전/환경', '교통/건축/국토', '육아/교육', '일자리']\n",
    "df_cats = df[df['category'].isin(cats)]\n",
    "df_samples = df_cats.sample(frac=0.1, random_state = 0)\n",
    "df_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd15b3-7ebe-49f4-b67c-9daa363bd84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49582c5a-e300-4a89-aeff-86bdf936ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df = 0.85,\n",
    "                            min_df = 2, \n",
    "                            ngram_range = (1, 2),\n",
    "                            tokenizer = okt_tokenizer,\n",
    "                            stop_words = stopwords)\n",
    "df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])\n",
    "df_tfidf_vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6642f5f-a8bd-42aa-b6cb-e7743d209fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km_cluster = KMeans(n_clusters = 4, max_iter = 10000, random_state = 0)\n",
    "km_cluster.fit(df_tfidf_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "clust_df = df_samples[['content', 'category']]\n",
    "clust_df['cluster'] = cluster_label\n",
    "clust_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74269ff3-c65e-4633-a34e-896ec07e6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_df.groupby(['category', 'cluster'])['content'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f5d53-e410-4a76-bce0-dc2f82b1fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_df[clust_df['cluster'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a113e3-7e4d-4733-b83e-1f4745a8aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
    "    cluster_details = {}\n",
    "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
    "    for cluster_num in range(clusters_num):\n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
    "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
    "        top_features = [ feature_names[ind] for ind in top_feature_indexes ]\n",
    "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
    "        cluster_details[cluster_num]['top_features'] = top_features\n",
    "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
    "    return cluster_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1181f2-8713-4a50-a869-8e6cb5e70610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_details(cluster_details):\n",
    "    for cluster_num, cluster_detail in cluster_details.items():\n",
    "        print('####### Cluster {0}'.format(cluster_num))\n",
    "        print('Top features:', cluster_detail['top_features'])\n",
    "        print('=============================')\n",
    "clust_centers = km_cluster.cluster_centers_\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "cluster_details = get_cluster_details(cluster_model=km_cluster, \n",
    "cluster_data=clust_df,\n",
    "                    feature_names=feature_names, \n",
    "                    clusters_num=4, \n",
    "                    top_n_features=10 )\n",
    "print_cluster_details(cluster_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679376c4-9e11-4d11-a4b3-22008c054685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96598ca7-0580-4747-ab84-406150bd10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은,는,이 조사가 날리기 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd04d9-5e2b-41b9-96c4-d605a77eb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "test_text=clust_df.iloc[0,0]\n",
    "result=okt.pos(test_text, stem=True)\n",
    "\n",
    "filtered_words=[]\n",
    "for word, type_ in result:\n",
    "    if type_ !='Josa':\n",
    "        filtered_words.append(word)\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206acc3-5223-4be9-abcd-2942f7673588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "filtered_words=[]\n",
    "for word, type_ in result:\n",
    "    if (type_ =='Noun' or 'Verb'):\n",
    "        filtered_words.append(word)\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1a35e-9b02-4795-8664-6d39a2395e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조사 지우는 함수\n",
    "def ok_not_josa(text):\n",
    "    filtered_words=[]\n",
    "    for word, type_ in result:\n",
    "        if type_ !='Josa':\n",
    "            filtered_words.append(word)\n",
    "   return filtered_words              # 문자로 받아서( text), 리스트(filtered_words)롷 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c401d2-43d9-4bbf-abb2-c6b5b076aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_enc = LabelEncoder()\n",
    "lb_enc.fit(df_samples['category'])\n",
    "df_samples['category']=lb_enc.transform(df_samples['category'])\n",
    "df_samples['content'] = df_samples['content'].apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣] +'))\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2219b83c-51dc-48ea-a9d4-4eb4fc2621ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      2\u001b[0m tfidf_vect \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.85\u001b[39m,\n\u001b[0;32m      3\u001b[0m                             min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m      4\u001b[0m                             ngram_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      5\u001b[0m                             tokenizer \u001b[38;5;241m=\u001b[39m okt_tokenizer,\n\u001b[0;32m      6\u001b[0m                             stop_words \u001b[38;5;241m=\u001b[39m stopwords)\n\u001b[1;32m----> 7\u001b[0m df_tfidf_vect \u001b[38;5;241m=\u001b[39m tfidf_vect\u001b[38;5;241m.\u001b[39mfit_transform(df_samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m df_tfidf_vect\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[173], line 6\u001b[0m, in \u001b[0;36mokt_tokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mokt_tokenizer\u001b[39m(text):\n\u001b[1;32m----> 6\u001b[0m     tokens_ko \u001b[38;5;241m=\u001b[39m okt\u001b[38;5;241m.\u001b[39mmorphs(text, stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens_ko\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos(phrase, norm\u001b[38;5;241m=\u001b[39mnorm, stem\u001b[38;5;241m=\u001b[39mstem)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjki\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[0;32m     72\u001b[0m             phrase,\n\u001b[0;32m     73\u001b[0m             jpype\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mBoolean(norm),\n\u001b[0;32m     74\u001b[0m             jpype\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mBoolean(stem))\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df = 0.85,\n",
    "                            min_df = 2, \n",
    "                            ngram_range = (1, 2),\n",
    "                            tokenizer = okt_tokenizer,\n",
    "                            stop_words = stopwords)\n",
    "df_tfidf_vect = tfidf_vect.fit_transform(df_samples['content'])\n",
    "df_tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0747787-99e1-4595-98e1-61b5a08f44c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfdc01-5eed-495b-9ae3-77ad7a8df648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cbfa2-3bc5-47f6-a658-4a901e268be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802e418-a287-4997-ad55-ad4e66c5a145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d5b12-5d89-42d7-b6cb-fb552ef48414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437eaab-3a44-4abf-b147-b7784769e0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95c443-5988-4392-86a2-0423835730f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
