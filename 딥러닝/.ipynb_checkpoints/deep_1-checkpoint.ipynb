{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fd549-79f0-4fa1-933e-0b85b416a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6757f4-1b6e-4f74-94c5-3a865eec2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(7,4)  # figure 사이즈 변경\n",
    "sns.set_palette('twilight')\n",
    "pal_1=sns.color_palette('coolwarm', 10)\n",
    "pal_2=sns.color_palette('deep',10)\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "font = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = font_manager.FontProperties(fname=font).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b0d34-751d-45af-8ac7-bbf4a6b60f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9af46-e4cf-41f6-93f0-7738371c9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀, 이진분류, 다중분류\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, kind = 0, units = 10): \n",
    "        self.kind = kind\n",
    "        # kind = {0, 1, 2}\n",
    "        # 0:회귀, 1:이진, 2:다중\n",
    "        self.units = units \n",
    "        self.w1 = None \n",
    "        self.b1 = None \n",
    "        self.w2 = None \n",
    "        self.b2 = None \n",
    "        self.a1 = None \n",
    "        self.losses = []\n",
    "    # 정방향 계산\n",
    "    def forpass(self, x): \n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "        self.a1 = self.sigmoid(z1) # 활성화함수(시그모이드)\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        return z2\n",
    "    # 역전파 계산\n",
    "    def backprop(self, x, err): \n",
    "        m = len(x)\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err) / m\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "    # 시그모이드 함수\n",
    "    def sigmoid(self, z): \n",
    "        z = np.clip(z, -100, None)\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "        return a\n",
    "    #소프트맥스 함수\n",
    "    def softmax(self, z): \n",
    "        z = np.clip(z, -100, None)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis = 1).reshape(-1, 1)\n",
    "    # 가중치 초기화\n",
    "    def init_weights(self, n_features, n_classes): \n",
    "        np.random.seed(0)\n",
    "        self.w1 = np.random.normal(0, 1, (n_features, self.units))\n",
    "        self.b1 = np.zeros(self.units)\n",
    "        self.w2 = np.random.normal(0, 1, (self.units, n_classes)) \n",
    "        self.b2 = np.zeros(n_classes) \n",
    "    # 회귀, 분류일때 달라지는 지점\n",
    "    ######################################\n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)\n",
    "        # 출력층에 활성화 함수적용\n",
    "        if self.kind == 0:\n",
    "            # 회귀 : y = x\n",
    "            a = z\n",
    "        elif self.kind == 1:\n",
    "            # 이진분류 : 시그모이드\n",
    "            a = self.sigmoid(z)\n",
    "        else:\n",
    "            # 다중분류 : 소프트맥스\n",
    "            a = self.softmax(z) \n",
    "        err = -(y - a)\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        self.w1 -= w1_grad\n",
    "        self.b1 -= b1_grad\n",
    "        self.w2 -= w2_grad\n",
    "        self.b2 -= b2_grad\n",
    "        return a\n",
    "    def loss(self, a, y, m):\n",
    "        if self.kind == 2:\n",
    "            # 다중분류 : 크로스 엔트로피 손실 함수\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.sum(-y * np.log(a)) \n",
    "        elif self.kind == 1:\n",
    "            # 이진분류 : 로지스틱 손실 함수\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))\n",
    "        else:\n",
    "            # 회귀\n",
    "            loss = np.sum((y - a) ** 2)\n",
    "        self.losses.append(loss / m)\n",
    "    ####################################################################    \n",
    "    \n",
    "    def fit(self, x, y, epochs = 100):\n",
    "        m = len(x)\n",
    "        self.init_weights(x.shape[1], y.shape[1]) # 가중치 초기화시 클래스 개수 포함\n",
    "        for I in range(epochs): \n",
    "            print('.', end='') # epochs 1번마다 . 찍음\n",
    "            a = self.training(x, y, m)\n",
    "            self.loss(a, y, m)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7fc83-e17e-46d8-a71e-0c9ba57e4bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8437cc4-dcc4-414f-ade1-db98b669c2ae",
   "metadata": {},
   "source": [
    "#### 신경망: 분류   - 은닉층 없음(퍼셉트론)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0f19e-9411-4bd8-8b99-4e4791113715",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 클라스 만들어서 전체 해보기\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583172c-e335-4283-a8d3-20bef574cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828cc13-66f2-4502-96b7-fae26da60874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayer:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.losses = []\n",
    "    def forpass(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)\n",
    "        w_grad = np.dot(x.T, err) / m\n",
    "        b_grad = np.sum(err) / m\n",
    "        return w_grad, b_grad\n",
    "    def activation(self, z):\n",
    "        z = np.clip(z, -100, None)\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "        return a\n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)\n",
    "        return z > 0\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))\n",
    "    def fit(self, x, y, epochs = 100, random_state = None):\n",
    "        y = y.reshape(-1,1) # 열 벡터로 변환\n",
    "        m = len(x)\n",
    "        self.w = np.ones((x.shape[1], 1)) # 가중치 초기화\n",
    "        self.b = 0 # 절편 초기화\n",
    "        for i in range(epochs):\n",
    "            z = self.forpass(x)\n",
    "            a = self.activation(z)\n",
    "            err = -(y - a)\n",
    "            w_grad, b_grad = self.backprop(x, err)\n",
    "            self.w-= w_grad\n",
    "            self.b-= b_grad\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.mean(-(y*np.log(a) + (1-y)*np.log(1-a))) \n",
    "            self.losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd85a61-098f-4c0d-a73d-1f4c39cda4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_layer = SingleLayer()\n",
    "single_layer.fit(X_train_scaled, y_train)\n",
    "single_layer.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61faf647-434c-4afe-ade5-2fe73bc9868a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(single_layer.losses)\n",
    "plt.ylim(0,0.1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "print(single_layer.losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec2bf1-8624-4a84-b7fd-eee03f53b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 분류 손실함수ㅡ 활성화 함수: 이진분류(시그모이드), 다중분류(soft max)\n",
    "# 손실함수의 결과값 저장 기능: 손실함수(loss) 계산하기\n",
    "# 손실함수(분류) - logistic// cross Entropy\n",
    "\n",
    "class SingleLayer:  # 입력계층 1개, 은닉층 없음\n",
    "    def __init__(self):\n",
    "        self.w = None    # 입력된 값 없으므로\n",
    "        self.b = None\n",
    "        # 손실 함수 저장하기 위한 리스트\n",
    "        self.losses = []\n",
    "    def forpass(self, x):\n",
    "        z = np.sum(x * self.w) + self.b\n",
    "        return z\n",
    "    def backprop(self, x, err):\n",
    "        w_grad = x * err\n",
    "        b_grad = 1 * err\n",
    "        return w_grad, b_grad\n",
    "    def activation(self, z):   ### 여길 바꿈\n",
    "        z = np.clip(z, -100, None)\n",
    "        a = 1 / (1 + np.exp(-z))   # 시그모이드 활성함수/이진분류\n",
    "        return a\n",
    "    def fit(self, x, y, epochs = 100):\n",
    "        self.w = np.ones(x.shape[1])\n",
    "        self.b = 0 \n",
    "        for epoch in range(epochs):\n",
    "            # 손실 초기화\n",
    "            loss = 0\n",
    "            # x의 index 랜덤하게 반환/ x,y가 각각 섞이지않도록 인덱스를 만들어 행렬로 만들어 사용\n",
    "            indexes = np.random.permutation(np.arange(len(x)))\n",
    "            for i in indexes:\n",
    "                z = self.forpass(x[i])\n",
    "                a = self.activation(z)\n",
    "                err = -(y[i] - a)\n",
    "                w_grad, b_grad = self.backprop(x[i], err)\n",
    "                self.w -= w_grad\n",
    "                self.b -= b_grad\n",
    "                 # 안전한 로그 계산을 위한 범위 축소  \n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # 이걸 회귀에서 바꿔줌: 손실 계산 : MSE \n",
    "                loss += -(y[i] * np.log(a) + (1 - y[i]) * np.log(1 - a))\n",
    "            # 에포크마다 평균 손실을 저장\n",
    "            self.losses.append(loss / len(y))  # loss값의 평균값(로스합계/y갯수)을 저장\n",
    "            print(f'###### 회귀 EPOCHS : {epoch + i}  #########')\n",
    "            print(f'평균 loss : {loss}')\n",
    "    def predict(self, x):\n",
    "        z = [self.forpass(x_i) for x_i in x]\n",
    "        return z\n",
    "    # 정확도 계산 함수 생성\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd1e3c-7f75-4621-a9eb-417a20199f04",
   "metadata": {},
   "source": [
    "#### 신경망 : 회귀  - 은닉층 없음(퍼셉트론)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747be965-bc87-4ac7-8e0d-3ddcccfdfcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 회귀에서 손실함수 저장하기  : 분류와 다른점 -활성함수(출력층)/ 손실함수 관련해서 바꿀것\n",
    "# 회귀: 활성함수 없음 또는 y=x\n",
    "# 손실함수의 결과값 저장 기능: 손실함수(loss) 계산하기 // MSE\n",
    "\n",
    "class SingleLayerRegression: # 입력계층 1개, 은닉층 없음\n",
    "    def __init__(self):\n",
    "        self.w = None   # 입력된 값 없으므로\n",
    "        self.b = None\n",
    "        # 손실 함수 저장하기 위한 리스트\n",
    "        self.losses = []\n",
    "        \n",
    "    def forpass(self, x):\n",
    "        z = np.sum(x * self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def backprop(self, x, err):\n",
    "        w_grad = x * err\n",
    "        b_grad = 1 * err\n",
    "        return w_grad, b_grad\n",
    "    \n",
    "    def activation(self, z):    #### 이걸 바꿔줘야\n",
    "        a = z\n",
    "        return a\n",
    "    \n",
    "    def fit(self, x, y, epochs = 100):\n",
    "        self.w = np.ones(x.shape[1])\n",
    "        self.b = 0 \n",
    "        for epoch in range(epochs):\n",
    "            # 손실 초기화\n",
    "            loss = 0\n",
    "            # x의 index 랜덤하게 반환\n",
    "            indexes = np.random.permutation(np.arange(len(x)))\n",
    "            for i in indexes:\n",
    "                z = self.forpass(x[i])\n",
    "                a = self.activation(z)\n",
    "                err = -(y[i] - a)  # (y - y_hat)\n",
    "                w_grad, b_grad = self.backprop(x[i], err)\n",
    "                self.w -= w_grad\n",
    "                self.b -= b_grad\n",
    "                \n",
    "                # 손실 계산 (SE)  (y - y_hat)^2\n",
    "                loss += err ** 2\n",
    "\n",
    "            # 에포크마다 평균 손실을 저장\n",
    "            loss = loss / len(y) # MSE\n",
    "            self.losses.append(loss)\n",
    "            print(f'##### EPOCHS : {epoch + 1} ######')\n",
    "            print(f'loss : {loss}')\n",
    "            \n",
    "    def predict(self, x):\n",
    "        z = [self.forpass(x_i) for x_i in x]\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "035e1fcd-cddd-44ad-8e50-f23f5797e07b",
   "metadata": {},
   "source": [
    "에포크 특징: 에포크는 모든 데이터를 최소 한번은 학습해야 한다.\n",
    "\n",
    "에포크마다 학습횟수\n",
    "    데이터당 학습 - 한번에 데이터를 하나씩 적용하여 가중치 업데이트  \n",
    "    1회 학습 -한번에 모든 데이터를 한번에 적용하여 가중치 업데이트   -- 가장 빠름/정확도도 좋음 /메모리 사용량 문제\n",
    "        : 한꺼번에 가져와서 한꺼번에 학습- 평균에러를 가지고 가중치 조절 / 행렬 곱셈 해야함 \n",
    "    N회 학습 - 데이터를 m개씩 묶어서 가중치 업데이트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e8c81-2710-46e9-9cdb-8f8213d05215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a13ea08-6e59-40d1-afca-ee624e722ef6",
   "metadata": {},
   "source": [
    "#### 다층신경망 : 분류  - 은닉층 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b82c4-7ab2-419b-8cfe-46ca73c516f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 다층 신경망\n",
    "class DualLayer:     # 은닉층 1개\n",
    "    def __init__(self, units = 8):  # 클래스 만들때 모든 은닉층, 가중치 등 정해 틀 만들고 시작\n",
    "        self.units = units # 은닉층의 뉴런 개수\n",
    "        self.w1 = None    # 입력 > 은닉 가중치\n",
    "        self.b1 = None    # 입력 > 은닉 절편\n",
    "        self.w2 = None    # 은닉 > 출력 가중치\n",
    "        self.b2 = None    # 은닉 > 출력 절편\n",
    "        self.a1 = None    # 은닉층의 활성화 출력\n",
    "        self.losses = []\n",
    "    def forpass(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "        self.a1 = self.activation(z1)            # 상속 받아 활성함수는 시그모이드\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2  # np.dot 행렬\n",
    "        return z2\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)\n",
    "        # 은닉층 > 출력층 가중치, 절편 업데이트\n",
    "        w2_grad = np.dot(self.a1.T, err) / m    # 평균\n",
    "        b2_grad = np.sum(err) / m               # 평균\n",
    "        # 은닉층 오차\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        # 입력층 > 은닉층 가중치, 절편 업데이트        \n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "    def activation(self, z):\n",
    "        z = np.clip(z, -100, None)\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "        return a\n",
    "    def predict(self, x):     # y 결과 값\n",
    "        z = self.forpass(x)\n",
    "        return z > 0\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))  \n",
    "  \n",
    "    def init_weights(self, n_features):        # 가중치 초기화  \n",
    "        self.w1 = np.ones((n_features, self.units))  # 2차원으로 만들어야 행렬곱 가능\n",
    "        self.b1 = np.zeros(self.units)\n",
    "        self.w2 = np.ones((self.units, 1))\n",
    "        self.b2 = 0\n",
    "    def training(self, x, y, m): ## 마지막 활성함수므로 분류할지 회귀할지 정하고 코드수정할것          \n",
    "        z = self.forpass(x)\n",
    "        a = self.activation(z)\n",
    "        err = -(y - a)\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        self.w1 -= w1_grad \n",
    "        self.b1 -= b1_grad\n",
    "        self.w2 -= w2_grad \n",
    "        self.b2 -= b2_grad\n",
    "        return a\n",
    "    def fit(self, x, y, epochs = 100):  # \n",
    "        y = y.reshape(-1,1)\n",
    "        m = len(x)\n",
    "        self.init_weights(x.shape[1])\n",
    "        for i in range(epochs):\n",
    "            a = self.training(x, y, m)\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))\n",
    "            self.losses.append(loss / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f354289-60bd-4703-8c36-697833194da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_layer = DualLayer()\n",
    "dual_layer.fit(X_train_scaled, y_train, epochs=1000)\n",
    "print(dual_layer.score(X_test_scaled, y_test))\n",
    "\n",
    "plt.plot(dual_layer.losses)\n",
    "plt.ylim(0,0.6)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "print(dual_layer.losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca7a38-2ee5-479c-b0b8-ac25b7c8f530",
   "metadata": {},
   "source": [
    "#### 다중 분류 클래스 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b322c0b-0794-43a7-bbc7-f2339480c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 다중 분류 클래스 만들기  : \n",
    "# 다 똑같은데, 다른 점 마지막 활성함수(소프트맥스), 손실함수(크로스 엔트로피)\n",
    "\n",
    "class MultiClassNetwork:\n",
    "    def __init__(self, units = 10): # 전과 동일\n",
    "        self.units = units \n",
    "        self.w1 = None \n",
    "        self.b1 = None \n",
    "        self.w2 = None \n",
    "        self.b2 = None \n",
    "        self.a1 = None \n",
    "        self.losses = []\n",
    "    def forpass(self, x): \n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "        self.a1 = self.sigmoid(z1)             # 활성 함수 이름 변경\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2 \n",
    "        return z2\n",
    "    def backprop(self, x, err): # 전과 동일\n",
    "        m = len(x)\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err) / m\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "    def sigmoid(self, z): # 시그모이드 함수\n",
    "        z = np.clip(z, -100, None)\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "        return a\n",
    "    def softmax(self, z):     #소프트맥스 함수\n",
    "        z = np.clip(z, -100, None)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis = 1).reshape(-1, 1)\n",
    "    def init_weights(self, n_features, n_classes): # 클래스 개수 받음\n",
    "        np.random.seed(0)\n",
    "        self.w1 = np.random.normal(0, 1, (n_features, self.units))\n",
    "        self.b1 = np.zeros(self.units)\n",
    "        self.w2 = np.random.normal(0, 1, (self.units, n_classes)) # 클래스 개수 포함\n",
    "        self.b2 = np.zeros(n_classes) # 클래스 개수 포함\n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)\n",
    "        a = self.softmax(z) # 출력 계층 활성 함수 변경\n",
    "        err = -(y - a)      # 에러값 3개 나옴\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        self.w1 -= w1_grad\n",
    "        self.b1 -= b1_grad\n",
    "        self.w2 -= w2_grad\n",
    "        self.b2 -= b2_grad\n",
    "        return a\n",
    "    def fit(self, x, y, epochs = 100):\n",
    "        m = len(x)\n",
    "        self.init_weights(x.shape[1], y.shape[1]) # 가중치 초기화시 클래스 개수 포함\n",
    "        for I in range(epochs): \n",
    "            print('.', end='') # epochs 1번마다 . 찍음\n",
    "            a = self.training(x, y, m)\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.sum(-y * np.log(a)) # 크로스 엔트로피 손실 함수\n",
    "            self.losses.append(loss / m)\n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)\n",
    "        return np.argmax(z, axis = 1) # 예측한 결과에서 가장 큰 확률 인덱스\n",
    "    def score(self, x, y):\n",
    "        # 정답 인덱스 확인\n",
    "        return np.mean(self.predict(x) == np.argmax(y, axis = 1)) \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80780ef3-028a-46c0-8d36-286e59dbf1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터로 실습 (내일 합시당)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# 데이터 불러오기\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "# 데이터 확인하기\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(X_train[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43812faa-a655-4cee-8a01-deaa5f011d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "nn = NeuralNetwork(kind = 0)\n",
    "nn.fit(X, y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e495f-e402-4931-9222-14aa335f6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiClassNetwork:\n",
    "    def __init__(self, units=10, units2=10): # 은닉층 노드 수 두 개 입력받음\n",
    "        self.units = units \n",
    "        self.units2 = units2 # 새로운 은닉층의 노드 수\n",
    "        self.w1 = None \n",
    "        self.b1 = None \n",
    "        self.w2 = None \n",
    "        self.b2 = None \n",
    "        self.w3 = None  # 새로운 은닉층의 가중치\n",
    "        self.b3 = None  # 새로운 은닉층의 편향\n",
    "        self.a1 = None \n",
    "        self.a2 = None  # 새로운 은닉층의 출력\n",
    "        self.losses = []\n",
    "\n",
    "    def forpass(self, x): \n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "        self.a1 = self.sigmoid(z1) \n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = self.sigmoid(z2)  # 새로운 은닉층의 출력\n",
    "        z3 = np.dot(self.a2, self.w3) + self.b3  # 새로운 은닉층의 출력을 다음 은닉층의 입력으로 사용\n",
    "        return z3\n",
    "\n",
    "    def backprop(self, x, err): \n",
    "        m = len(x)\n",
    "        w3_grad = np.dot(self.a2.T, err) / m  # 새로운 은닉층의 그래디언트 계산\n",
    "        b3_grad = np.sum(err) / m\n",
    "        err_to_hidden2 = np.dot(err, self.w3.T) * self.a2 * (1 - self.a2)  # 새로운 은닉층에서의 오차\n",
    "        w2_grad = np.dot(self.a1.T, err_to_hidden2) / m  # 은닉층2의 그래디언트 계산\n",
    "        b2_grad = np.sum(err_to_hidden2, axis=0) / m\n",
    "        err_to_hidden = np.dot(err_to_hidden2, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad, w3_grad, b3_grad  # 새로운 은닉층의 그래디언트 반환\n",
    "\n",
    "    def sigmoid(self, z): \n",
    "        z = np.clip(z, -100, None)\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "        return a\n",
    "\n",
    "    def softmax(self, z): \n",
    "        z = np.clip(z, -100, None)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n",
    "\n",
    "    def init_weights(self, n_features, n_classes): \n",
    "        np.random.seed(0)\n",
    "        self.w1 = np.random.normal(0, 1, (n_features, self.units))\n",
    "        self.b1 = np.zeros(self.units)\n",
    "        self.w2 = np.random.normal(0, 1, (self.units, self.units2))  # 은닉층 2의 가중치\n",
    "        self.b2 = np.zeros(self.units2)\n",
    "        self.w3 = np.random.normal(0, 1, (self.units2, n_classes))  # 새로운 은닉층의 가중치\n",
    "        self.b3 = np.zeros(n_classes)\n",
    "\n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)\n",
    "        a = self.softmax(z) \n",
    "        err = -(y - a)\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad, w3_grad, b3_grad = self.backprop(x, err)\n",
    "        self.w1 -= w1_grad\n",
    "        self.b1 -= b1_grad\n",
    "        self.w2 -= w2_grad\n",
    "        self.b2 -= b2_grad\n",
    "        self.w3 -= w3_grad  # 새로운 은닉층의 가중치 업데이트\n",
    "        self.b3 -= b3_grad  # 새로운 은닉층의 편향 업데이트\n",
    "        return a\n",
    "\n",
    "    def fit(self, x, y, epochs=100):\n",
    "        m = len(x)\n",
    "        self.init_weights(x.shape[1], y.shape[1])\n",
    "        for i in range(epochs): \n",
    "            print('.', end='') \n",
    "            a = self.training(x, y, m)\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.sum(-y * np.log(a)) \n",
    "            self.losses.append(loss / m)\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)\n",
    "        return np.argmax(z, axis=1)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x) == np.argmax(y, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e2a7b-8bef-4e4a-b819-fa5a27146c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01192e1-a3fa-48d7-aef8-accab7298fb4",
   "metadata": {},
   "source": [
    "#### 신경망의 최적화 : 과적합 제어- 규제(L1,L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf3ce7-fee5-41ae-9e9a-d477fbd98a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1_regular(DualLayer): \n",
    "    def __init__(self, units = 10, l1 = 0):\n",
    "        self.units = units \n",
    "        self.w1 = None \n",
    "        self.b1 = None \n",
    "        self.w2 = None \n",
    "        self.b2 = None \n",
    "        self.a1 = None \n",
    "        self.l1 = l1\n",
    "        self.losses = []\n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)\n",
    "        a = self.activation(z)\n",
    "        err = -(y - a)\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        self.w1 -= w1_grad + self.l1 * np.sign(self.w1) \n",
    "        self.b1 -= b1_grad\n",
    "        self.w2 -= w2_grad + self.l1 * np.sign(self.w2)\n",
    "        self.b2 -= b2_grad \n",
    "        return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b371a-9757-43c9-9092-9fa72c5357e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_layer = DualLayer()\n",
    "dual_layer.fit(X_train_scaled, y_train, epochs = 1000)\n",
    "print(dual_layer.score(X_test_scaled, y_test))\n",
    "l1 = L1_regular(l1 = 0.01)\n",
    "l1.fit(X_train_scaled, y_train, epochs = 1000)\n",
    "print(l1.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44e764-b88d-4604-8e3b-620899b7e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2_regular(DualLayer): \n",
    "    def __init__(self, units = 10, l2 = 0):\n",
    "        self.units = units \n",
    "        self.w1 = None \n",
    "        self.b1 = None \n",
    "        self.w2 = None \n",
    "        self.b2 = None \n",
    "        self.a1 = None \n",
    "        self.l2 = l2\n",
    "        self.losses = []\n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)\n",
    "        a = self.activation(z)\n",
    "        err = -(y - a)\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        self.w1 -= w1_grad + self.l2 * self.w1\n",
    "        self.b1 -= b1_grad\n",
    "        self.w2 -= w2_grad + self.l2 * self.w2\n",
    "        self.b2 -= b2_grad \n",
    "        return a\n",
    "    \n",
    "    def fit(self, x, y, epochs = 100):\n",
    "        y = y.reshape(-1,1)\n",
    "        m = len(x)\n",
    "        self.init_weights(x.shape[1]) \n",
    "        for i in range(epochs):\n",
    "            a = self.training(x, y, m)\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a))) + (self.l2 / 2) * (np.sum(self.w1 ** 2) + np.sum(self.w2 ** 2))\n",
    "            # l1일때\n",
    "#             loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a))) + (self.l1) * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2)))\n",
    "            self.losses.append(loss / m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
